<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>鹏宇甜记</title>
    <url>/2020/06/16/%E9%B9%8F%E5%AE%87%E7%94%9C%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="鹏宇甜记"><a href="#鹏宇甜记" class="headerlink" title="鹏宇甜记"></a>鹏宇甜记</h1><p>本来想叫“鹏宇酸甜苦辣记”，发现我码字的时候嘴角全是笑容，那就叫“鹏宇甜记”。</p>
<h2 id="相识"><a href="#相识" class="headerlink" title="相识"></a>相识</h2><p>一次偶然的机会，我碰到了她，至此，我们的故事就悄然开始了。我喜欢叫她宝贝，她叫我王老师（一开始，我不是很喜欢这个称呼，后来慢慢了解之后我很喜欢这个称呼，她父母都是人民教师，老师这个词就显得特别很亲近了吧，补充一点，宝贝给她妈妈的备注就是杨老师）。</p>
<h2 id="7月"><a href="#7月" class="headerlink" title="7月"></a>7月</h2><p>考研上了个研究生，可惜导师没选好，浪费了我初试第一的分数，我导师没有什么好项目，所以有点空闲的时间我就去打比赛了。偶然，刷北邮人论坛，刚好看到她也在打这个比赛，我当时成绩还不错，就厚着脸皮拉人家一起打比赛，哈哈。（没想到，后续发生如此之多的故事）  </p>
<p>我们加了个好友，按照互联网的自我介绍方法自我介绍了一波（简介、清晰、无趣），后面约了个时间她来我实验室工位（脏、乱），我介绍了一下现在的进展。我俩还没怎么说话，我老师就来找我了（老师的确是事多），第一次见面基本上就是草草结束了。我对宝贝的印象就是“她当时涂了一个正红的口红、很好看，穿的记不住了（貌似是一个碎花裙？反正也很仙）”，不过当时我没有想这么多。</p>
<h2 id="8月"><a href="#8月" class="headerlink" title="8月"></a>8月</h2><p>7月底到8月初，实验室放假了，时常是两周（说多不多，说少不少）。我乘着这个假期回了趟家，这个是惯例，我从小是个留守儿童，外婆带大我的，而且外婆身体不好，每年我都会回家过暑假，陪陪老人（虽然我回家啥也不干，反而添乱，但那里还是我的童年记忆，我也是个比较恋家的人）。  </p>
<p>回家后，不得提一下我的初中同学，这个人学习成绩不好，但是很社会。初中的时候，我和他玩的比较多，每次回家他都疯狂给我打电话，喊我上街玩、吃饭、上网，这些都是他带坏我了，不过我还是有自制力的，不该去的不去。他算是我的狐朋狗友吧，从他身上我也看到了很多社会底层的生活方式。  </p>
<p>那个假期，他问我有没有女朋友，我说没有，然后他就把我微信推给了一个女生，和那女生随便聊了聊（比较日常，后续被这个女生影响蛮大的）。假期结束，我要回学校，刚好去车站路上，大家一起吃了个饭（当时我比较笨，弄的饭局比较尴尬）。回学校后，我就写了个道歉信表示了道歉（这个道歉信给我和宝贝的交往带来了伏笔）。</p>
<h2 id="9月"><a href="#9月" class="headerlink" title="9月"></a>9月</h2><p>我和她的故事开始了。  </p>
<p>9月份开学了，我是研二，她是研一，我开学早，先来了学校，她比较晚，后几天来的。  </p>
<p>我来学校后，开始继续了之前的那个比赛，时不时的我俩之间会沟通一下这个比赛的事，同时也会有一些闲聊。然后，我看到她朋友圈里的照片的时候，我动了一些歪心思，找她聊天的次数就变多了。</p>
<p>9月2日，她回学校了，我可以见到她了，开心。这一天，我自告奋勇的去帮她搬行李，还偷偷的用了实验室的小推车。我知道她实验室在教二，她给我发信息说是在北门，我以为是学校北门，然后我就过去。到了学校北门，没有看见她，后来发消息说是教二北门，这北邮的教二在南门那里，我整个方向搞反了，然后我赶紧的快走+小跑过去了。</p>
<p>看见她了，一个丸子头，这就是青春的气息啊。她此时此刻22岁，正直满脸胶原蛋白的时候，浑身都散发了青春、年少、优雅的气息，一个白色的T，搭配了一条超短裤和运动鞋，青春洋溢。  </p>
<p>我赶紧帮忙，帮她搬行李回宿舍，约莫着搬了半个多小时吧，搬完了，我们的第二次见面就结束了。哈哈，第二次见面算是爱情的开始了。第二天，她点了个外卖奈雪给我喝，包装很好看，至今应该还是在我实验室工位的柜子里，我要留着当纪念了。</p>
<p>后来，我频繁的约她一起自习、吃饭。19年的中秋节晚上，我一如即往的来骚扰她了，她说她一个人刚看完电影，然后现在去食堂吃饭，我就屁颠屁颠的赶紧跑过去找她了，看着她吃饭，哈哈，这是第一次看她吃饭。吃完饭之后，我们有了第一次的北邮田径场散步，这是我们的四大约会地点之一（田径场、漫咖啡、科研楼旁小板凳、实验室），也是最开心、最舒适的地方，空间开阔、视野很好、氛围也不错。 和她随意聊天，她聊到了她去世的奶奶，眼角泛出泪水，奶奶是这个世界上很疼爱她的人，她也很爱她奶奶。中秋佳节，难免会想起那些不在身边，却爱着的人。写到此，不知今年的中秋我还能和她一起在北邮的田径场聊天（今年的中秋刚好和国庆都在同一天，这个时候我们应该都在学校了，结束了异地，不知道还有没有恋，我到时候在去争取争取，哈哈）。</p>
<p>接下来的几天，我们就在漫咖啡里开始了一段我以学习约出来的约会，哈哈，目的不纯。她是学信息工程的，我是学计算机的，我俩算是一个计算机垃圾和通信大佬一起学习计算机知识，我科班出身稍微基础好一点，也能忽悠她一点，哈哈。一次，我电脑上出现了之前提过的那个女生的相关信息，她看到了。我当时惊慌失措，竟然胡扯说是妹妹（我可真是个渣男+直男），她生气了，然后走了。我很卑微，我挽留，她说了几句“与我无瓜”。我想道歉，想挽留，我跑到五道口买奶茶、甜品，想道个歉。回学校后，我在宿舍楼下等了她好久，到了晚上凌晨0.30多吧，她没下来，我把奶茶，甜点都扔在了垃圾桶，然后一个人爬了15楼回宿舍（宿舍11.30就没有了电梯，之后我也经常爬楼梯，每次上去都大汗淋漓，澡都白洗了）。</p>
<p>接下来的两天，我以为我结束了，但是我仔细的想了想，人家女生晚上和你去漫咖啡自习，还气呼呼的说“与我无瓜”，这不就证明了她对我有好感吗，所以我知道我还要机会。我微信给她发发消息，后来她回复我了，然后我俩就慢慢又开始了之前的一起学习。</p>
<p>9.28日，一个值得我一辈子记住的一天（这一天还是后来推算得到的）。晚上，我们在漫咖啡学习了之后，一起在科研楼旁的小板凳上坐下来闲聊一会。已经很晚了，这里还是有很多大爷大妈在这聊天，不知道为啥不回去睡觉去，跟个年轻人一样。终于，大爷大妈们还是走了，我慢慢的靠近她，然后慢慢地，我吻了她，她还吻了吻我的耳垂，我恋爱了。</p>
<h2 id="10月"><a href="#10月" class="headerlink" title="10月"></a>10月</h2><p>10月1日，今天是祖国的70岁生日，举国同庆。我们一起在老食堂吃了个早餐，然后就慢步到了小松林，此时大电视正在播放阅兵。挺有纪念意义的，我们和很多情侣、单身狗一起在小松林看了威风凛凛的阅兵。然后，我们就买了电影票，打算去看中国机长。在去的路上，我们还偶然碰到了同实验室的同学，然后我俩恋爱的消息就师兄师姐就都知道了，哈哈。晚上，也是一个印象深刻的时刻。国庆节，市政府将在天安门有一场烟火表演，趁着在帝都读书的机会，当然不能错过这么好的机缘。大约6点多点时候，我就骑着我的小毛驴带着她先到大钟寺倒地铁，然后慢慢靠近天安门地区，记得是在南锣鼓巷下地铁的，然后一路逛过去，我们也如愿的看到了70岁生日的烟火表演（虽然只有一点点），而且我们还在人群中留下了第一张合拍，超级好看。</p>
<p>10.2-4日，这几天，她有同学过来玩了，我们也就普通在学校抽空一起吃吃午饭，平凡的学校生活。</p>
<p>10.5日，今天是计划好赶着世园会的尾巴去看世园会的，但是当我们去乘坐前往郊区的列车S2时，根本挤不上去，无奈选择放弃。然后就陪她去潘家园取了眼镜，她的眼睛不好，给她带来了很大的困扰，然后我们还去了天坛随意逛了逛。至此，国庆假期基本上就这些了。</p>
<p>10.28，这一天时我们在一起的一个月纪念日—绿茶餐厅。室友推荐我来这里吃饭，这里的环境还不错，晚上6.30，我就骑着小毛驴带她来到了我无数次迷路的西直门。还好，这次我们顺利找到了餐厅，点了点招牌菜，也没排队，就吃上饭了。这家是一个杭帮菜餐厅，主要是环境比较优雅，吃饭的时候，我送给了她一个手链，当作一个月的纪念礼物，这是一个蛇骨手链，可以之后不断的加宝珠，我的想法就是以后的关键纪念日就加上一颗珠子。</p>
<h2 id="11月-12月"><a href="#11月-12月" class="headerlink" title="11月-12月"></a>11月-12月</h2><p>平凡、甜蜜的恋爱，以后再来补充吧。</p>
<h2 id="1月"><a href="#1月" class="headerlink" title="1月"></a>1月</h2><p>1.14日，这是一个改变我们的日子。这一天，我们要回家了，开始我们的异地恋篇章。她回贵州，我回江西。异地恋太难了。这一天发生的一切也在暗示着我。首先，我的书包在过安检的时候忘记拿了，平时如此谨慎的我是不怎么掉东西的；下高铁的时候，我的充电器又掉了；晚上，我朋友的车还被新手司机撞了，处理事故白白耽误两天时间。也许这一天就为我们的缘分埋下了伏笔。</p>
<p>1.23日，新冠肺炎爆发，这意味着我们将继续异地恋无限期，不知何时能回到北京。（白天想、夜里哭、做梦都想回北京）</p>
<h2 id="2月-5月"><a href="#2月-5月" class="headerlink" title="2月-5月"></a>2月-5月</h2><p>我的春招，不如意的半年，感情也在异地恋中慢慢消耗。</p>
<h2 id="6月8日下午5点20分"><a href="#6月8日下午5点20分" class="headerlink" title="6月8日下午5点20分"></a>6月8日下午5点20分</h2><p> 6.8 5.20 多么吉利的数字啊，我们结束了，我在工位上泪水就下来了，赶紧跑到楼梯间，慢慢的抽泣起来了。我知道我情绪一时控制不下来，我走出了网易园区，往出租屋走回去，来回有5-6km，我用这段路程慢慢的平复下来了。</p>
<h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p> 2020.06.08深夜2点，此时胃好像有点难受，可能是今日情绪波动较大，往常深夜饿了也不会难受。哎，明天起来应该就没事了。</p>
<p> Ps: 抽个时间再把其他的也写完吧。</p>
<p>  2020.06.16日深夜1点，补充结束。从与她的第一句聊天记录开始，我所有的聊天记录都有保存，也许下次在来完善的时候，我应该是暮暮老矣，慢慢回味的我的初恋。<br> 最后，我想对你说，你的脾气不好，但是你也别忍着，洒脱点。人，就是要开心点，乐观点，及时行乐，别被许多杂乱的人生百事束缚心灵，保持一个坦然点心态，生活就会比较潇洒。宝贝，我爱你，王老师落笔。</p>
]]></content>
  </entry>
  <entry>
    <title>预训练语言模型</title>
    <url>/2020/04/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Todo</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>Todo</p>
<h2 id="glove"><a href="#glove" class="headerlink" title="glove"></a>glove</h2><p>Todo</p>
<h2 id="elmo"><a href="#elmo" class="headerlink" title="elmo"></a>elmo</h2><p>Todo</p>
<h2 id="gpt"><a href="#gpt" class="headerlink" title="gpt"></a>gpt</h2><p>Todo</p>
<h2 id="gpt2"><a href="#gpt2" class="headerlink" title="gpt2"></a>gpt2</h2><p>Todo</p>
<h2 id="bert"><a href="#bert" class="headerlink" title="bert"></a>bert</h2><p>Todo</p>
<h2 id="roberta"><a href="#roberta" class="headerlink" title="roberta"></a>roberta</h2><h3 id="静态Masking-vs-动态Masking"><a href="#静态Masking-vs-动态Masking" class="headerlink" title="静态Masking vs 动态Masking"></a>静态Masking vs 动态Masking</h3><ul>
<li>静态Masking：Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。</li>
<li>动态Masking：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。</li>
</ul>
<h3 id="with-NSP-vs-without-NSP"><a href="#with-NSP-vs-without-NSP" class="headerlink" title="with NSP vs without NSP"></a>with NSP vs without NSP</h3><p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。</p>
<h3 id="更大的mini-batch"><a href="#更大的mini-batch" class="headerlink" title="更大的mini-batch"></a>更大的mini-batch</h3><p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。</p>
<h3 id="更多的数据，更长时间的训练"><a href="#更多的数据，更长时间的训练" class="headerlink" title="更多的数据，更长时间的训练"></a>更多的数据，更长时间的训练</h3><p>RoBERTa用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h2 id="albert"><a href="#albert" class="headerlink" title="albert"></a>albert</h2><h3 id="对Embedding因式分解"><a href="#对Embedding因式分解" class="headerlink" title="对Embedding因式分解"></a>对Embedding因式分解</h3><p>ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的 embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从$O(V×H)O(V×H)O(V×H)$降低到了$O(V×E+E×H)O(V×E+E×H)O(V×E+E×H) $，当 $E&lt;&lt;H$时参数量减少的很明显。</p>
<h3 id="跨层的参数共享（Cross-layer-parameter-sharing）"><a href="#跨层的参数共享（Cross-layer-parameter-sharing）" class="headerlink" title="跨层的参数共享（Cross-layer parameter sharing）"></a>跨层的参数共享（Cross-layer parameter sharing）</h3><p>全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。</p>
<h3 id="句间连贯（Inter-sentence-coherence-loss）"><a href="#句间连贯（Inter-sentence-coherence-loss）" class="headerlink" title="句间连贯（Inter-sentence coherence loss）"></a>句间连贯（Inter-sentence coherence loss）</h3><p>在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。</p>
<h3 id="移除dropout"><a href="#移除dropout" class="headerlink" title="移除dropout"></a>移除dropout</h3><p>ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</p>
<h2 id="distillBert"><a href="#distillBert" class="headerlink" title="distillBert"></a>distillBert</h2><p>Todo</p>
<h2 id="xlnet"><a href="#xlnet" class="headerlink" title="xlnet"></a>xlnet</h2><p>Todo</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">一文揭开ALBERT的神秘面纱</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>glove</tag>
        <tag>elmo</tag>
        <tag>gpt</tag>
        <tag>gpt2</tag>
        <tag>bert</tag>
        <tag>roberta</tag>
        <tag>distillBert</tag>
        <tag>albert</tag>
        <tag>xlnet</tag>
      </tags>
  </entry>
  <entry>
    <title>focal loss</title>
    <url>/2020/03/31/focal-loss/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2020/03/29/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="速记"><a href="#速记" class="headerlink" title="速记"></a>速记</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">时间复杂度</th>
<th style="text-align:center">稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">选择排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">希尔排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">基数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">桶排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">计数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
</tbody>
</table>
</div>
<p>稳定性记忆方法：高复杂度（ $O(n^2)$ ）选择排序是例外，低复杂度（ $O(nlogn)$ ）归并排序是例外。<br><br>选择排序不稳定举例：(7) 2 5 9 3 4 [7] 1…当我们利用直接选择排序算法进行排序时候,(7)和1调换,(7)就跑到了[7]的后面了,原来的次序改变了,这样就不稳定了.</p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""快速排序"""</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    mid = array[start]  </span><br><span class="line">    low = start  </span><br><span class="line">    high = end </span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        array[low] = array[high]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        array[high] = array[low]</span><br><span class="line"></span><br><span class="line">    array[low] = mid  </span><br><span class="line"></span><br><span class="line">    quick_sort(alist, start, low - <span class="number">1</span>)  </span><br><span class="line">    quick_sort(alist, low + <span class="number">1</span>, end)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">quick_sort(array, <span class="number">0</span>, len(array) - <span class="number">1</span>)          <span class="comment"># 闭区间 [0, len(array) -1]</span></span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(array) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    mid = len(array) // <span class="number">2</span></span><br><span class="line">    left_array = merge_sort(array[:mid])</span><br><span class="line">    right_array = merge_sort(array[mid:])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> merge(left_array, right_array)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array, right_array)</span>:</span></span><br><span class="line">    array = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt;= right_array[j]:</span><br><span class="line">            array.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            array.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array):</span><br><span class="line">        array.append(left_array[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; len(right_array):</span><br><span class="line">        array.append(right_array[j])</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = merge_sort(array)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="topK问题"><a href="#topK问题" class="headerlink" title="topK问题"></a>topK问题</h2><h3 id="暴力解法"><a href="#暴力解法" class="headerlink" title="暴力解法"></a>暴力解法</h3><p>最基本的思路，将N个数进行完全排序，从中选出排在前K的元素即为所求。有了这个思路，我们可以选择相应的排序算法进行处理，目前来看快速排序，堆排序和归并排序都能达到$O(NlogN)$的时间复杂度。</p>
<h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><p>可以采用数据池的思想，选择其中前K个数作为数据池，后面的N-K个数与这K个数进行比较，若小于其中的任何一个数，则进行替换。这种思路的算法复杂度是$O(N*K)$</p>
<h3 id="大根堆"><a href="#大根堆" class="headerlink" title="大根堆"></a>大根堆</h3><p>大根堆维护一个大小为K的数组，目前该大根堆中的元素是排名前K的数，其中根是最大的数。此后，每次从原数组中取一个元素与根进行比较，如小于根的元素，则将根元素替换并进行堆调整（下沉），即保证大根堆中的元素仍然是排名前K的数，且根元素仍然最大；否则不予处理，取下一个数组元素继续该过程。该算法的时间复杂度是O(N*logK)，一般来说企业中都采用该策略处理topK问题，因为该算法不需要一次将原数组中的内容全部加载到内存中，而这正是海量数据处理必然会面临的一个关卡。</p>
<h3 id="快速排序-1"><a href="#快速排序-1" class="headerlink" title="快速排序"></a>快速排序</h3><p>利用快速排序的分划函数找到分划位置K，则其前面的内容即为所求。该算法是一种非常有效的处理方式，时间复杂度是O(N)（证明可以参考算法导论书籍）。对于能一次加载到内存中的数组，该策略非常优秀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topK</span><span class="params">(array, K)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(array) - <span class="number">1</span></span><br><span class="line">    index = quick_sort(array, left, right)</span><br><span class="line">    <span class="keyword">while</span> index+<span class="number">1</span> != K:</span><br><span class="line">        <span class="keyword">if</span> index+<span class="number">1</span> &lt; K:</span><br><span class="line">            index = quick_sort(array, index+<span class="number">1</span>, right)</span><br><span class="line">        <span class="keyword">elif</span> index+<span class="number">1</span> &gt; K:</span><br><span class="line">            index = quick_sort(array, left, index<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> array[:K]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, left, right)</span>:</span></span><br><span class="line">    <span class="comment"># print(left, right)</span></span><br><span class="line">    mid = array[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[right] &lt; mid:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        array[left] = array[right]</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[left] &gt;= mid:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        array[right] = array[left]</span><br><span class="line">    array[left] = mid</span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = topK(array, K=<span class="number">8</span>)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://www.jianshu.com/p/5b8f00d6a9d7" target="_blank" rel="noopener">topK算法问题</a></li>
<li><a href="https://blog.csdn.net/zyq522376829/article/details/47686867?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">海量数据处理 - 10亿个数中找出最大的10000个数（top K问题）</a></li>
</ol>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>快排排序</tag>
        <tag>归并排序</tag>
        <tag>topK</tag>
      </tags>
  </entry>
  <entry>
    <title>蓄水池抽样算法</title>
    <url>/2020/03/17/%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择一个数据？<br></li>
<li>解法：我们以概率1选择第一个数据，以1/2的概率选择第二个数据，以此类推，以1/m的概率选择第m个对象（如果后面某一数据一旦选中，替换掉以前选中的数据）。当所有数据流过时，每个对象具有相同的被选中的概率1/n。<br></li>
<li>证明：<script type="math/tex">P_m = \frac{1}{m} \cdot \frac{m}{m+1} \cdot \frac{m+1}{m+2} \cdot *** \cdot \frac{n-2}{n-1} \cdot \frac{n-1}{n} = \frac{1}{n}</script></li>
<li><p>Python代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n)</span>:</span></span><br><span class="line">    res = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        num = random.randint(<span class="number">1</span>, i):</span><br><span class="line">        <span class="keyword">if</span> i == num:</span><br><span class="line">            res = i</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="蓄水池抽样"><a href="#蓄水池抽样" class="headerlink" title="蓄水池抽样"></a>蓄水池抽样</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择k（k &lt; n）个数据？</li>
<li>解法：先选中前k个数据（当作一个蓄水池），然后以k/(k+1)的概率选择第k+1个数据，以k/(k+2)的概率选择第k+2个数据，以此类推以k/m的概率选择第m个数据，一旦后面有某个数据被选中，则随机替换蓄水池中的一个数据。最终每个数据被选中的概率均为k/n。</li>
<li>证明：<script type="math/tex; mode=display">
\begin{align}
P_m &= \frac{k}{m}  (\frac{m+1-k}{m+1} +\frac{k}{m+1} \frac{k-1}{k})  (\frac{m+2-k}{m+2} +\frac{k}{m+2}\frac{k-1}{k}) ...(\frac{n-k}{n} +\frac{k}{n}\frac{k-1}{k})  \\
 &= \frac{k}{n}
\end{align}</script></li>
<li>Python代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n, k)</span>:</span></span><br><span class="line">    res = [<span class="number">0</span>] * k</span><br><span class="line">    <span class="keyword">for</span> i range(k):</span><br><span class="line">        res[i] = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i range(k, n):</span><br><span class="line">        num = random.randint(<span class="number">0</span>, i)</span><br><span class="line">        <span class="keyword">if</span> num &lt; k:</span><br><span class="line">            res[num]  = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/87bc058b762e" target="_blank" rel="noopener">蓄水池抽样</a></li>
<li><a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">蓄水池抽样算法(Reservoir Sampling)</a></li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>蓄水池抽样</tag>
      </tags>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/2020/03/07/ResNet/</url>
    <content><![CDATA[<p>Todo</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Batch Normalization</title>
    <url>/2020/03/07/Batch-Normalization/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>由于下层的Layer的参数发生改变，导致上层的输入的分布发生改变，最后深度神经网络难以训练。（注意：底层为最下层）</p>
<ul>
<li>Internal Covariate Shift (内部协变量偏移): 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程</li>
<li>Internal Covariate Shift 带来的问题<ul>
<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>
<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</li>
</ul>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><ul>
<li>算法流程图<br><img src="images\BN.png" alt="BN"><br><br>($\epsilon$是为了增加训练稳定性而加入的小的常量数据)</li>
<li><p>测试阶段如何使用Batch Normalization？</p>
<ul>
<li>BN在每一层计算的$\mu$与$\sigma^2$都是基于当前batch中的训练数据，但是这就带来了一个问题：在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时$\mu$与$\sigma^2$的计算一定是有偏估计，这个时候该如何进行计算？</li>
<li>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的$\mu<em>{batch}$与$\sigma^2</em>{batch}$。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：<script type="math/tex; mode=display">\mu_{test} = E(\mu_{batch})</script><script type="math/tex; mode=display">\sigma^2_{test} = \frac{m}{m-1}E(\sigma^2_{batch})</script></li>
<li>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：<script type="math/tex; mode=display">BN(x_{test}) = \gamma \frac{X_{test} - \mu_{test}}{\sqrt{\sigma^2_{test} + \epsilon}} + \beta</script></li>
</ul>
</li>
<li><p>batch_normalization做了normalization后为什么要变回来？即 scale and shift<br><br> 如果只做normalize在某些情况下会出现问题，比如对象是Sigmoid函数的output，而且output是分布在Sigmoid函数的两侧，normalize会强制把output分布在Sigmoid函数的中间的非饱和区域，这样会导致这层网络所学习到的特征分布被normalize破坏。而上面算法的最后一步，scale and shift可以令零均值单位方差的分布（normalize之后的分布）通过调节$\gamma$和$\beta$变成任意更好的分布（对于喂给下一层网络来说）。因为这个$\gamma$和$\beta$是在训练过程中可以学习得到参数。</p>
</li>
</ul>
<h2 id="BN的优势"><a href="#BN的优势" class="headerlink" title="BN的优势"></a>BN的优势</h2><ul>
<li>BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</li>
<li>BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</li>
<li>BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>
<li>BN具有一定的正则化效果<br><br>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</li>
</ul>
<h2 id="BN的缺陷"><a href="#BN的缺陷" class="headerlink" title="BN的缺陷"></a>BN的缺陷</h2><ul>
<li>不适用于mini-batch非常小的训练环境</li>
<li>不适用于RNN，因为它是一个动态的网络结构，同一个batch中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得BN并不能正确的使用。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">Batch Normalization原理与实战</a></li>
<li><a href="https://www.zhihu.com/question/38102762/answer/607815171" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a></li>
<li><a href="https://www.zhihu.com/question/55917730/answer/154269264" target="_blank" rel="noopener">请问batch_normalization做了normalization后为什么要变回来？</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失、梯度爆炸、解决方法</title>
    <url>/2020/03/06/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><h2 id="梯度消失解决方法"><a href="#梯度消失解决方法" class="headerlink" title="梯度消失解决方法"></a>梯度消失解决方法</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>TextCNN&amp;TextGCN</title>
    <url>/2020/03/04/TextCNN/</url>
    <content><![CDATA[<h2 id="TextCNN原理"><a href="#TextCNN原理" class="headerlink" title="TextCNN原理"></a>TextCNN原理</h2><ul>
<li>框架图 <img src="images/TextCNN.png" alt="TextCNN"></li>
<li>$x_i: $ the k-dimensional word vector corresponding to the i-th word in the sentence.</li>
<li>向量拼接 <script type="math/tex; mode=display">x_{1:n} = concat(x_i, x_2, ...,x_n)</script></li>
<li>卷积操作 <script type="math/tex; mode=display">c_i = f (w \cdot x_{i:i+h-1} + b)</script></li>
<li>卷积结果 <script type="math/tex; mode=display">c = [c_1, c_2, c_3, ..., c_{n-h+1}]</script></li>
<li>max-over-time pooling <script type="math/tex; mode=display">\hat c = max(x)</script></li>
<li>这是一个卷积核的结果，实际网络中有256个卷积核和两个通道(static embedding 和 dynamic embedding)</li>
</ul>
<h2 id="TextGCN原理"><a href="#TextGCN原理" class="headerlink" title="TextGCN原理"></a>TextGCN原理</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TextCNN</tag>
        <tag>TextGCN</tag>
      </tags>
  </entry>
  <entry>
    <title>TF-IDF及TextRank</title>
    <url>/2020/03/04/TF-IDF%E5%8F%8ATextRank/</url>
    <content><![CDATA[<h2 id="1-TF-IDF"><a href="#1-TF-IDF" class="headerlink" title="1. TF-IDF"></a>1. TF-IDF</h2><ul>
<li>TF<ul>
<li>TF是词频(TF，Term Frequency): 词频（TF）表示词条（关键字）在文本中出现的频率。<script type="math/tex; mode=display">词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}</script></li>
</ul>
</li>
<li>IDF<ul>
<li>IDF是逆向文件频率 (IDF，Inverse Document Frequency): 某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<script type="math/tex; mode=display">逆文档频率(IDF) = log \frac{语料库的总文档数}{包含该词的文档数+1}</script></li>
</ul>
</li>
<li>TF-IDF<ul>
<li>TF-IDF实际上是 $TF * IDF$ 。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。<script type="math/tex; mode=display">TFIDF = TF \cdot IDF</script></li>
</ul>
</li>
</ul>
<h2 id="2-TextRank"><a href="#2-TextRank" class="headerlink" title="2. TextRank"></a>2. TextRank</h2><ul>
<li>基于词语词之间的共现性构建无向图。<br><br>参考<a href="https://my.oschina.net/u/3800567/blog/2870640" target="_blank" rel="noopener">jieba源码分析之关键字提取(TF-IDF/TextRank)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>TextRank</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2020/03/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-激活函数的理解"><a href="#1-激活函数的理解" class="headerlink" title="1.激活函数的理解"></a>1.激活函数的理解</h2><p>解决线形模型 y = wx + b 的弊端，线形分类模型的分界地方都是平面或者超平面，无法解决具有非线形模型特征的数据。如果没有激励函数，在这种情况下你每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，相当于没有隐藏层，网络的学习能力有限。<br><br><img src="images/非线型数据.png" alt="非线型数据"><br><br><a href="https://zhuanlan.zhihu.com/p/25279356" target="_blank" rel="noopener">形象的解释神经网络激活函数的作用是什么？</a><br><br><a href="https://blog.csdn.net/GreatXiang888/article/details/99296607" target="_blank" rel="noopener">常见激活函数，及其优缺点 - 面试篇</a></p>
<h2 id="2-Sigmoid"><a href="#2-Sigmoid" class="headerlink" title="2. Sigmoid"></a>2. Sigmoid</h2><ul>
<li>数学形式<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+ e^{-x}}</script><img src="images\sigmoid.png" alt="sigmoid"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\sigma ^{'}(x)  &=  (1+e^{-x}) ^ {-2}  \cdot e^{-x} \\
          &= \frac{e^{-x}}{(1+e^{-x})^2} \\
          &= \frac{1+e^{-x} -1}{(1+e^{-x})^2} \\
          &= \frac{1}{ 1+ e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
          &= \sigma(x) - \sigma ^ 2(x) \\
          &= \sigma(x) (1-\sigma(x))
\end{align}</script><img src="images\sigmoid导数.png" alt="sigmoid导数"></li>
<li>特点 <ul>
<li>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。 </li>
</ul>
</li>
<li>缺点<ul>
<li>如果我们初始化神经网络的权值为 $[0,1]$ 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 $(1,+∞)$ 区间内的值，则会出现梯度爆炸情况。梯度消失更容易产生。</li>
<li>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li>
<li>Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，这个特性会导致为在后面神经网络的高层处理中收到不是零中心的数据。这将导致梯度下降时的晃动，因为如果数据到了神经元永远时正数时，反向传播时权值w就会全为正数或者负数。</li>
</ul>
</li>
</ul>
<h2 id="3-tanh"><a href="#3-tanh" class="headerlink" title="3. tanh"></a>3. tanh</h2><ul>
<li>数学形式<script type="math/tex; mode=display">tanh(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{-x}}</script><img src="images\tanh.png" alt="tanh"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\tanh ^{'}(x)  &=  1 - tanh^2(x)
\end{align}</script><img src="images\tanh导数.png" alt="tanh导数"></li>
<li>特点 <ul>
<li>解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</li>
</ul>
</li>
</ul>
<h2 id="4-Relu"><a href="#4-Relu" class="headerlink" title="4. Relu"></a>4. Relu</h2><ul>
<li>数学形式<script type="math/tex; mode=display">Relu(x) = max(0, x)</script><img src="images\Relu.png" alt="Relu"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{equation}
Relu(x) = \left \{
\begin{aligned}
1 &  & {x>0} \\
0 &  & x<0 \\
None & & x=0 
\end{aligned}
\right.
\end{equation}</script></li>
<li>优点<ul>
<li>解决了梯度消失问题</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid</li>
</ul>
</li>
<li>缺点<ul>
<li>输出不是zero-centered</li>
<li>原点不可导</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见。例如w初始化全部为一些负数。 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
<li>leaky relu函数，$f(x)=max(αx,x)$ , 比如取α=0.01\alpha=0.01α=0.01，可以改善relu中x&lt;0部分的dead问题。</li>
</ul>
</li>
</ul>
<h2 id="5-如何选择合适的激活函数"><a href="#5-如何选择合适的激活函数" class="headerlink" title="5. 如何选择合适的激活函数"></a>5. 如何选择合适的激活函数</h2><ul>
<li>首选 ReLU，速度快，但是要注意学习速率的调整，</li>
<li>如果 ReLU 效果欠佳,尝试使用 Leaky ReLU 变种。</li>
<li>可以尝试使用 tanh。</li>
<li>Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。</li>
<li>在浅层神经网络中，选择使用哪种激励函数影响不大。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>手推LR</title>
    <url>/2020/03/01/%E6%89%8B%E6%8E%A8LR/</url>
    <content><![CDATA[<h2 id="1-假说"><a href="#1-假说" class="headerlink" title="1. 假说"></a>1. 假说</h2><p>假设样本的标签为0和1， h(x) 为取得预测为标签1的概率。</p>
<script type="math/tex; mode=display">h(x) = \frac{1}{1+e^{-w^Tx}}</script><h2 id="2-后验概率"><a href="#2-后验概率" class="headerlink" title="2. 后验概率"></a>2. 后验概率</h2><script type="math/tex; mode=display">p(y=1|x) = h(x) = \frac{1}{1+e^{-w^Tx}}</script><script type="math/tex; mode=display">p(y=0|x) = 1- h(x) = 1- \frac{1}{1+e^{-w^Tx}} = \frac{e^{-w^Tx}}{1+e^{-w^Tx}}</script><h2 id="3-似然函数"><a href="#3-似然函数" class="headerlink" title="3. 似然函数"></a>3. 似然函数</h2><p>N 为数据样本数</p>
<script type="math/tex; mode=display">L(w) = \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i}  = \prod_{i=1}^N h(x_i)^{y_i}(1-h(x_i))^{1-y_i}</script><h2 id="4-对数似然"><a href="#4-对数似然" class="headerlink" title="4. 对数似然"></a>4. 对数似然</h2><script type="math/tex; mode=display">
\begin{align}
    log(L(w)) &= log( \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i} ) \\
    &=  \sum_{i=1}^N  (y_i log(h(x_i))  + (1-y_i)log(1-h(x_i)) ) \\
    &= \sum_{i=1}^N \{ y_i\{log(h(x_i)) - log(1-h(x_i))\} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{ y_i log\frac{h(x_i)}{1-h(x_i)} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) + log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(\frac{1+e^{-w^Tx}}{ e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}
\end{align}</script><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5. 损失函数"></a>5. 损失函数</h2><p>似然函数乘以 -1/N</p>
<script type="math/tex; mode=display">J(w) = -\frac{1}{N} log(L(w)) = -\frac{1}{N} \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}</script><h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6. 梯度下降"></a>6. 梯度下降</h2><p>Loss 如下：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{ \partial J(w) }{\partial w} &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} \frac{\partial h(x_i)}{\partial w} - \frac{1-y_i}{1-h(x_i)} \frac{\partial h(x_i)}{\partial w}  \} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \}\frac{\partial h(x_i)}{\partial w}  \\
    &=  -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \} h(x_i)(1-h(x_i)) \frac{\partial w^Tx_i}{\partial w} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i (1-h(x_i)) -(1-y_i)h(x_i) \} x_i \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i - h(x_i)\} x_i
\end{align}</script><p>参数更新如下：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= w-\alpha \frac{\partial J(w)}{ \partial w}\\
&= w- \frac{1}{N}\sum_{i=1}^N \{  h(x_i) -y_i\} x_i
\end{align}</script><h2 id="7-一些问题"><a href="#7-一些问题" class="headerlink" title="7. 一些问题"></a>7. 一些问题</h2><ul>
<li><p>LR为什么是线性模型？<br><br>Logistic Regression从几率的概念构建线性回归模型。一个事件发生的几率（odds）为该事件发生的概率与不发生概率的比值，几率的取值范围为$[0,+\infty)$，其对数的取值范围为实数域，所以，可以将对数几率作为因变量构建线性回归模型: $log\frac{p}{1-p} = w^Tx$,  由此可得 $p = \frac{1}{1+e^{-w^Tx}}$。这便是Logistic Regression采用sigmoid函数的原因，sigmoid函数将自变量的线性组合映射到（0,1），用以表述分类的概率特性。从sigmoid函数看出，当$w^Tx &gt;0 $ 时，y=1，否则 y=0。$w^Tx=0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）, 所以说逻辑回归本质上是一个线性模型。</p>
</li>
<li><p>为什么逻辑回归比线性回归要好？<br><br>逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。</p>
</li>
<li><p>sigmoid函数</p>
<script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script><script type="math/tex; mode=display">\sigma^{'}(z) = \frac{e^{-z}}{(1+e^{-z})^2} = \sigma(z) (1- \sigma(z))</script><ul>
<li>优点：<ul>
<li>Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。</li>
<li>求导简单。</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于其软饱和性，容易产生梯度消失，导致训练出现问题。</li>
</ul>
</li>
</ul>
</li>
<li>LR 如何解决多分类问题？<br><br> 如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即 y=i 的时候意味着 y 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）, 模型通过 softmax 函数来对概率建模。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/ltlitao717/article/details/75453480" target="_blank" rel="noopener">机器学习杂货铺-手推LR</a></li>
<li><a href="https://blog.csdn.net/weixin_44915167/article/details/89377022?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">机器学习面试题之LR</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合及其解决方法</title>
    <url>/2020/03/01/%E8%BF%87%E6%8B%9F%E5%90%88%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-过拟合概念"><a href="#1-过拟合概念" class="headerlink" title="1. 过拟合概念"></a>1. 过拟合概念</h2><p>在深度学习或机器学习过程中，在训练集上表现过于优越，在验证集及测试集中表现不佳，模型泛化能力差。<br><a href="https://blog.csdn.net/jingbo18/article/details/80609006" target="_blank" rel="noopener">过拟合及常见处理办法整理</a></p>
<h2 id="2-常见原因"><a href="#2-常见原因" class="headerlink" title="2. 常见原因"></a>2. 常见原因</h2><p>原因主要是数据样本少及噪声多</p>
<ul>
<li>数据样本少</li>
<li>数据噪声多</li>
<li>模型复杂度高</li>
<li>迭代次数多</li>
</ul>
<h2 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h2><ul>
<li>获取更多的数据<ul>
<li>数据源获取更多的数据</li>
<li>数据增强</li>
</ul>
</li>
<li>更改模型结构<ul>
<li>换简单模型</li>
<li>L1 &amp; L2 范式</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
</li>
</ul>
<h2 id="4-数据增强"><a href="#4-数据增强" class="headerlink" title="4. 数据增强"></a>4. 数据增强</h2><p>自然语言处理技术中常用的数据增强方法</p>
<ul>
<li>同义词替换</li>
<li>随机插入</li>
<li>随机交换</li>
<li>随机删除</li>
</ul>
<h2 id="5-L1-amp-L2-范式"><a href="#5-L1-amp-L2-范式" class="headerlink" title="5. L1 &amp; L2 范式"></a>5. L1 &amp; L2 范式</h2><ul>
<li>范式定义<script type="math/tex; mode=display">\left (  \sum_{i}\left  | x_i \right | ^p   \right)^\frac{1}{p}</script></li>
<li>加入范式的目标函数<script type="math/tex; mode=display">J^2\left(w, b\right) = J\left(w, b\right) + \frac{\lambda}{2m}\Omega (w)</script></li>
<li>L1范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  = \sum_{i} \left | w_i  \right |</script></li>
<li>L2范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  =  \sum_{i} \left | w_i  \right | ^2</script></li>
<li>结论<br><br>L1 正则化用作特征选择，L2 正则化用作防止过拟合<br><br>参考介绍—-<a href="https://www.jianshu.com/p/569efedf6985" target="_blank" rel="noopener">机器学习中的正则化</a></li>
</ul>
<h2 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6. Dropout"></a>6. Dropout</h2><p>思想：以指数级别创建不同的网络结果，然后各个网络结果加权平均，解决过拟合<br><br>训练阶段，对每一个神经元的输出以keep_prob保留，1-keep_prob置为0；<br><br>预测阶段，对每一个神经元的输出乘以keep_prob。<br><br>缺点: 训练时间变长2-3倍。<br><br>参考介绍—-<a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
        <tag>L1范式</tag>
        <tag>L2范式</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN、LSTM、GRU对比</title>
    <url>/2020/02/29/RNN%E3%80%81LSTM%E3%80%81GRU%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="RNN框架图"><a href="#RNN框架图" class="headerlink" title="RNN框架图"></a>RNN框架图</h3><h3 id="梯度消失及爆炸"><a href="#梯度消失及爆炸" class="headerlink" title="梯度消失及爆炸"></a>梯度消失及爆炸</h3><p>Todo</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>从RNN网络出发开始介绍LSTM网络，记录其架构图及公式。</p>
<h3 id="LSTM的框架图"><a href="#LSTM的框架图" class="headerlink" title="LSTM的框架图"></a>LSTM的框架图</h3><p><img src="/images/LSTM.png" alt="LSTM"></p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><script type="math/tex; mode=display">f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)</script><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><script type="math/tex; mode=display">i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)</script><script type="math/tex; mode=display">\tilde{C_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)</script><script type="math/tex; mode=display">C_t=f_t * C_{t-1} + i_t * \tilde{C_t}</script><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><script type="math/tex; mode=display">o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)</script><script type="math/tex; mode=display">h_t=o_t * tanh(C_t)</script><h3 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h3><ul>
<li>首先需要明确的是，RNN 中的梯度消失/梯度爆炸和普通的 MLP 或者深层 CNN 中梯度消失/梯度爆炸的含义不一样。MLP/CNN 中不同的层有不同的参数，各是各的梯度；而 RNN 中同样的权重在各个时间步共享，最终的梯度 g = 各个时间步的梯度 g_t 的和。</li>
<li>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</li>
<li>LSTM 中梯度的传播有很多条路径，cell 这条路径上只有逐元素相乘和相加的操作，梯度流最稳定；但是其他路径上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘。</li>
<li>但是在其他路径上，LSTM 的梯度流和普通 RNN 没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 = 各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度 + 消失梯度 = 正常梯度）。因此 LSTM 通过改善一条路径上的梯度问题拯救了总体的远距离梯度。</li>
<li>同样，因为总的远距离梯度 = 各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度 = 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h3 id="GRU框架图"><a href="#GRU框架图" class="headerlink" title="GRU框架图"></a>GRU框架图</h3><p><img src="/images/LSTM.png" alt="GRU"></p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><script type="math/tex; mode=display">z_t = \sigma (W_z \cdot [h_{t-1}, x_t] + b_z)</script><script type="math/tex; mode=display">r_t = \sigma (W_r \cdot [h_{t-1}, x_t] + b_r)</script><script type="math/tex; mode=display">\tilde{h_t} = W \cdot [r_t * h_{t-1}, x_t] + b</script><script type="math/tex; mode=display">h_t = (1-z_t) * h_{t-1}  + z_t * \tilde{h_t}</script><h3 id="与LSTM区别"><a href="#与LSTM区别" class="headerlink" title="与LSTM区别"></a>与LSTM区别</h3><ul>
<li>GRU和LSTM的性能在很多任务上不分伯仲。</li>
<li>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。</li>
<li>从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将 hidden state 传给下一个单元，而LSTM则用 memory cell 把 hidden state 包装起来。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="https://www.zhihu.com/question/34878706/answer/665429718" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://blog.csdn.net/u012223913/article/details/77724621" target="_blank" rel="noopener">LSTM 和GRU的区别</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>自然语言处理</tag>
        <tag>RNN</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
</search>
