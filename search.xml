<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>梯度下降法、牛顿法、拟牛顿法</title>
    <url>/2020/07/12/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>求解无约束最优化问题的极小值</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>通过不断的迭代，进行目标函数的极小化，直到收敛</p>
<h2 id="泰勒一阶展开"><a href="#泰勒一阶展开" class="headerlink" title="泰勒一阶展开"></a>泰勒一阶展开</h2><script type="math/tex; mode=display">f(x) = f(x_0) + (x-x_0) f'(x_0)</script><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>极小化 $f(x)$ ，则 $x-x_0 = -f’(x_0) * \lambda$ ，其中 $\lambda &gt; 0 $</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>$ x = x_0  - \lambda * f’(x_0)$ ，其中学习率 $\lambda &gt; 0 $</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>数学问题中，求解极小值</li>
<li>机器学习中，求解目标函数最优解</li>
</ul>
<h1 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>牛顿法，又称牛顿迭代法，主要是迭代的方法用于求解方程的根。后应用于无约束问题的最优化。</p>
<h2 id="解方程"><a href="#解方程" class="headerlink" title="解方程"></a>解方程</h2><p>利用泰勒公式一阶展开，$f(x) = f(x<em>0) + (x-x_0) f’(x_0)$<br>求解方程 $f(x) = 0$， 则  $f(x_0) + (x-x_0) f’(x_0) = 0$<br>推出，$x = x_0  - \frac{f(x_0)}{f’(x_0)}$<br>算法流程：<br>不断迭代 $x</em>{k+1} = x<em>k - \frac{f(x_k)}{f’(x_k)}$，直至 $f(x</em>{k+1}) = 0$ </p>
<p>举例如下：<br>求解$\sqrt2$ ， 保留 5位小数<br>思路：设计 $y = f(x)$， 将$x = \sqrt2$ 代入，$y = 0$， 故 $f(x) = x^2 -2$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mySqrt</span><span class="params">(self, x: int)</span> -&gt; int:</span></span><br><span class="line">        num_pre = x</span><br><span class="line">        num_cur = (num_pre * num_pre + x) / (<span class="number">2</span> * num_pre)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> abs(num_pre-num_cur) &gt; <span class="number">0.0001</span>:</span><br><span class="line">            num_pre = num_cur</span><br><span class="line">            num_cur = (num_pre * num_pre + x) / (<span class="number">2</span> * num_pre)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> num_cur</span><br></pre></td></tr></table></figure>
<h2 id="求解无约束最优化问题"><a href="#求解无约束最优化问题" class="headerlink" title="求解无约束最优化问题"></a>求解无约束最优化问题</h2><p>利用泰勒公式二阶展开</p>
<script type="math/tex; mode=display">f(x) = f(x_0) + (x-x_0) f'(x_0) + \frac{1}{2} (x-x_0)^2 f''(x_0)</script><p>函数$f(x)$ 在 $x_0$有极值的必要条件是 $f’(x_0) = 0$</p>
<script type="math/tex; mode=display">0 = f'(x) = f'(x_0) + (x-x_0) f''(x_0)</script><script type="math/tex; mode=display">x = x_0 - \frac{ f'(x_0)}{ f''(x_0)}</script><h2 id="最优化问题应用至高纬"><a href="#最优化问题应用至高纬" class="headerlink" title="最优化问题应用至高纬"></a>最优化问题应用至高纬</h2><p>引入Hessian矩阵 </p>
<script type="math/tex; mode=display">H(x) = [\frac{\partial^2f}{\partial x_i \partial x_j }]_{n*n}</script><script type="math/tex; mode=display">x_{k+1} = x_k - \frac{g_k}{H_k} =  x_k - H_k^{-1} g_k</script><p>其中，$g_k$ 是一阶导（梯度），$H_k$ 是Hessian矩阵 </p>
<h1 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h1><p>避免每次都要计算Hessian矩阵，采用正定矩阵近似方法实现</p>
<h1 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h1><h2 id="速记"><a href="#速记" class="headerlink" title="速记"></a>速记</h2><ul>
<li>梯度下降时，迭代 $ x = x_0  - \lambda * f’(x_0)$ 直至收敛，其中学习率 $\lambda &gt; 0 $</li>
<li>牛顿法解方程时，迭代 $ x = x_0 - \frac{f(x_0)}{f’(x_0)} $，直至收敛</li>
<li>牛顿法最优化时，迭代 $ x = x_0 - \frac{ f’(x_0)}{ f’’(x_0)}$ 直至收敛</li>
</ul>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><ul>
<li>牛顿法是二阶收敛，梯度下降法是一阶收敛，牛顿法收敛速度快</li>
<li>牛顿法容易陷入局部最优</li>
<li>牛顿法要同时求出梯度和Hessian矩阵，计算量大且不好计算</li>
<li>当输入向量维度$N$较大时，Hessian矩阵的大小时 $N*N$，需要内存和显存非常大</li>
</ul>
]]></content>
      <categories>
        <category>最优化</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>牛顿法</tag>
        <tag>拟牛顿法</tag>
      </tags>
  </entry>
  <entry>
    <title>牛顿法</title>
    <url>/2020/07/12/%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>最长上升子序列及二维最长上升子序列</title>
    <url>/2020/07/11/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97%E5%8F%8A%E4%BA%8C%E7%BB%B4%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h1 id="最长上升子序列"><a href="#最长上升子序列" class="headerlink" title="最长上升子序列"></a>最长上升子序列</h1><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h2 id="动态规划-二分查找"><a href="#动态规划-二分查找" class="headerlink" title="动态规划+二分查找"></a>动态规划+二分查找</h2><h1 id="俄罗斯套娃娃信封"><a href="#俄罗斯套娃娃信封" class="headerlink" title="俄罗斯套娃娃信封"></a>俄罗斯套娃娃信封</h1>]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>最长上升子序列</tag>
      </tags>
  </entry>
  <entry>
    <title>二分法常见题型</title>
    <url>/2020/07/11/%E4%BA%8C%E5%88%86%E6%B3%95%E5%B8%B8%E8%A7%81%E9%A2%98%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="排序数组中查找"><a href="#排序数组中查找" class="headerlink" title="排序数组中查找"></a>排序数组中查找</h1><h1 id="隐含的最大最小值，其中寻找答案"><a href="#隐含的最大最小值，其中寻找答案" class="headerlink" title="隐含的最大最小值，其中寻找答案"></a>隐含的最大最小值，其中寻找答案</h1>]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>手撕代码</tag>
        <tag>二分法</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀和、二维前缀和、差分</title>
    <url>/2020/07/10/%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%81%E4%BA%8C%E7%BB%B4%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%81%E5%B7%AE%E5%88%86/</url>
    <content><![CDATA[<h2 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h2><h2 id="二维前缀和"><a href="#二维前缀和" class="headerlink" title="二维前缀和"></a>二维前缀和</h2><h2 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h2><p><a href="https://www.cnblogs.com/-Ackerman/p/11162651.html" target="_blank" rel="noopener">https://www.cnblogs.com/-Ackerman/p/11162651.html</a></p>
]]></content>
      <categories>
        <category>手撕代码</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>前缀和</tag>
        <tag>二维前缀和</tag>
        <tag>差分</tag>
      </tags>
  </entry>
  <entry>
    <title>条件随机场（Conditional Random Field, CRF）</title>
    <url>/2020/07/10/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%EF%BC%88Conditional-Random-Field-CRF%EF%BC%89/</url>
    <content><![CDATA[<h1 id="0-引言"><a href="#0-引言" class="headerlink" title="0 引言"></a>0 引言</h1><p>条件随机场（Conditional Random Field，CRF）是给定一组输入随机变量条件下另一组输出变量的条件概率分布模型，其特点是输出随机变量构成了马尔可夫随机场。</p>
<h1 id="1-概率无向图模型"><a href="#1-概率无向图模型" class="headerlink" title="1 概率无向图模型"></a>1 概率无向图模型</h1><pre><code>引言：概率无向图模型，又称为马尔可夫随机场，是一个可以由无向图
表示的联合概率分布。
</code></pre><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>设有联合概率分布 $P(Y)$ , 由无向图 $G=(V,E)$ 表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。 如果联合概率分布 $P(Y)$   满足成对、局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型，或马尔可夫随机场（Markov Random Field）。</p>
<ul>
<li>成对马尔可夫性：</li>
<li>局部马尔可夫性：</li>
<li>全局马尔可夫性：</li>
</ul>
<h2 id="1-2-概率无线图模型的因子分解"><a href="#1-2-概率无线图模型的因子分解" class="headerlink" title="1.2 概率无线图模型的因子分解"></a>1.2 概率无线图模型的因子分解</h2><h3 id="1-2-1-团与最大团"><a href="#1-2-1-团与最大团" class="headerlink" title="1.2.1 团与最大团"></a>1.2.1 团与最大团</h3><h3 id="1-2-2-因子分解"><a href="#1-2-2-因子分解" class="headerlink" title="1.2.2 因子分解"></a>1.2.2 因子分解</h3><h2 id="2-条件随机场的定义与形式"><a href="#2-条件随机场的定义与形式" class="headerlink" title="2 条件随机场的定义与形式"></a>2 条件随机场的定义与形式</h2><h2 id="2-1-条件随机场的定义"><a href="#2-1-条件随机场的定义" class="headerlink" title="2.1 条件随机场的定义"></a>2.1 条件随机场的定义</h2><h3 id="2-1-1-整体架构"><a href="#2-1-1-整体架构" class="headerlink" title="2.1.1 整体架构"></a>2.1.1 整体架构</h3><ul>
<li>条件随机场是给定随机变量 $X$ 条件下，随机变量 $Y$ 的马尔可夫随机场。</li>
<li>线性链条件随机场可用于标注问题。这时，在条件概率模型 $P(Y|X)$ 中，$Y$ 是输出变量，表示标记序列，$X$ 是输入变量，表示需要标注的观测序列，也把标记序列称为状态序列。</li>
<li>学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型 $P\hat (Y|X)$ 。</li>
<li>预测时，对于给定的输入序列 $x$，求出条件概率 $P\hat(y|x)$ 最大的输出序列 $\hat{y} $ </li>
</ul>
<h3 id="2-1-2-条件随机场的定义"><a href="#2-1-2-条件随机场的定义" class="headerlink" title="2.1.2 条件随机场的定义"></a>2.1.2 条件随机场的定义</h3><h3 id="2-1-3-线性链条件随机场的定义"><a href="#2-1-3-线性链条件随机场的定义" class="headerlink" title="2.1.3 线性链条件随机场的定义"></a>2.1.3 线性链条件随机场的定义</h3><h2 id="2-2-条件随机场的参数化形式"><a href="#2-2-条件随机场的参数化形式" class="headerlink" title="2.2 条件随机场的参数化形式"></a>2.2 条件随机场的参数化形式</h2><h2 id="2-3-条件随机场的简化形式"><a href="#2-3-条件随机场的简化形式" class="headerlink" title="2.3 条件随机场的简化形式"></a>2.3 条件随机场的简化形式</h2><h2 id="2-4-条件随机场的矩阵形式"><a href="#2-4-条件随机场的矩阵形式" class="headerlink" title="2.4 条件随机场的矩阵形式"></a>2.4 条件随机场的矩阵形式</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>条件随机场</tag>
        <tag>CRF</tag>
        <tag>Conditional Random Field</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title>隐马尔可夫模型（Hidden Markov Model, HMM）</title>
    <url>/2020/07/08/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%88Hidden-Markov-Model-HMM%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-隐马尔可夫模型的基本概念"><a href="#1-隐马尔可夫模型的基本概念" class="headerlink" title="1. 隐马尔可夫模型的基本概念"></a>1. 隐马尔可夫模型的基本概念</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测从而产生观测随机序列的过程。   </p>
<ul>
<li>马尔可夫链：隐藏的马尔可夫链随机生成的状态的序列</li>
<li>每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列</li>
</ul>
<h2 id="1-2-三要素"><a href="#1-2-三要素" class="headerlink" title="1.2 三要素"></a>1.2 三要素</h2><p>隐马尔可夫模型由初始状态概率向量  $\pi$ 、状态转移概率矩阵 $A$ 和 观测概率矩阵 $B$ 决定。 $ \pi $ 和 $A$ 决定状态序列，$B$ 决定观测序列。</p>
<script type="math/tex; mode=display">\lambda = (A, B, \pi)</script><h2 id="1-3-两个基本假设"><a href="#1-3-两个基本假设" class="headerlink" title="1.3 两个基本假设"></a>1.3 两个基本假设</h2><ul>
<li>齐次马尔可夫性假设：假设隐藏的马尔可夫链在任意时刻 $t$ 的状态只依赖于其前一时刻的状态，与其他时刻的状态集观测无关，也与时刻 $t$ 无关</li>
<li>观测独立性假设：假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关</li>
</ul>
<h2 id="1-4-隐马尔可夫模型的3个基本问题"><a href="#1-4-隐马尔可夫模型的3个基本问题" class="headerlink" title="1.4 隐马尔可夫模型的3个基本问题"></a>1.4 隐马尔可夫模型的3个基本问题</h2><ul>
<li>概率计算问题。 给定模型 $\lambda=(A,B,\pi)$ 和观测序列$O=(o_1, o_2, … , o_T)$ ，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率  $P(O|\lambda)$</li>
<li>学习问题。已知观测序列 $O=(o_1, o_2, … , o_T)$ ，估计模型 $ \lambda = (A, B, \pi)$ 参数，使得在该模型下观测序列概率 $P(O|\lambda)$ 最大。即用最大似然估计的方法估计参数。</li>
<li>预测问题，也称为解码问题。已知模型 $\lambda=(A,B,\pi) $ 和观测序列$O=(o_1, o_2, … , o_T)$ ，求对给定观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1, i_2, …, i_T)$ 。即给定观测序列，求最有可能的对应的状态序列。</li>
</ul>
<h1 id="2-概率计算算法"><a href="#2-概率计算算法" class="headerlink" title="2. 概率计算算法"></a>2. 概率计算算法</h1><h2 id="2-1-直接计算法"><a href="#2-1-直接计算法" class="headerlink" title="2.1 直接计算法"></a>2.1 直接计算法</h2><p>给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1, o_2, … , o_T)$ ，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率  $P(O|\lambda)$</p>
<p>通过列举所以可能的长度为 $T$ 的状态序列 $I=(i_1, i_2, …, i_T)$， 求各个状态序列 $I$ 与观测序列 $O=(o_1, o_2, … , o_T)$ 的联合概率 $ P(O, I | \lambda)$，然后对所有可能的状态序列求和，得到  $P(O|\lambda)$</p>
<ul>
<li>时间复杂度 $O(TN^T)$</li>
</ul>
<h2 id="2-2-前向算法"><a href="#2-2-前向算法" class="headerlink" title="2.2 前向算法"></a>2.2 前向算法</h2><ul>
<li><p>前向概率：给定隐马尔可夫模型 $\lambda $，定义到时刻 $t$ 部分观测序列为 $o_1, o_2, … , o_t $ 且状态为 $q_i$ 的概率为前向概率，记作</p>
<script type="math/tex; mode=display">\alpha_t(i) = P( o_1, o_2, ... , o_t , i_t = q_i | \lambda)</script></li>
<li><p>结合动态规划递推求解 $ \alpha_t(i) $ 即观测序列概率 $P(O|\lambda)$</p>
</li>
<li>时间复杂度 $O(TN^2)$</li>
</ul>
<h2 id="2-3-后向算法"><a href="#2-3-后向算法" class="headerlink" title="2.3 后向算法"></a>2.3 后向算法</h2><ul>
<li><p>后向概率：给定隐马尔可夫模型 $\lambda $，定义在时刻 $t$ 状态为  $q<em>i$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列为 $o</em>{t+1}, o_{t+2}, … , o_T $ 的概率为后向概率，记作</p>
<script type="math/tex; mode=display">\beta_t(i) = P( o_{t+1}, o_{t+2}, ... , o_T, i_t = q_i | \lambda)</script></li>
<li><p>结合动态规划递推求解 $ \beta_t(i) $ 即观测序列概率 $P(O|\lambda)$</p>
</li>
<li>时间复杂度 $O(TN^2)$</li>
</ul>
<h1 id="3-学习算法"><a href="#3-学习算法" class="headerlink" title="3. 学习算法"></a>3. 学习算法</h1><h2 id="3-1-监督学习方法"><a href="#3-1-监督学习方法" class="headerlink" title="3.1 监督学习方法"></a>3.1 监督学习方法</h2><p>状态数据和观测序列都存在的训练数据，利用极大似然估计发来估计隐马尔可夫模型的参数</p>
<ul>
<li>转移概率</li>
<li>观测概率</li>
<li>初始状态概率</li>
</ul>
<h2 id="3-2-Baum-Welch算法"><a href="#3-2-Baum-Welch算法" class="headerlink" title="3.2 Baum-Welch算法"></a>3.2 Baum-Welch算法</h2><p>Todo</p>
<h1 id="4-预测算法"><a href="#4-预测算法" class="headerlink" title="4. 预测算法"></a>4. 预测算法</h1><h2 id="4-1-近似算法"><a href="#4-1-近似算法" class="headerlink" title="4.1 近似算法"></a>4.1 近似算法</h2><p>贪心思想，在每个时刻 $t$ 选择该时刻最有可能出现的状态，优点是计算简单，缺点是不能保证预测的状态序列整体是最优可能的状态序列。</p>
<h2 id="4-2-维特比算法"><a href="#4-2-维特比算法" class="headerlink" title="4.2 维特比算法"></a>4.2 维特比算法</h2><p>用动态规划求概率最大路径（最优路径），这时一条路径对应着一个状态序列。</p>
<p>下一步规划：补充EM算法，CRF </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title>并查集的python实现</title>
    <url>/2020/06/16/%E5%B9%B6%E6%9F%A5%E9%9B%86%E7%9A%84python%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h1 id="1-并查集概念"><a href="#1-并查集概念" class="headerlink" title="1.并查集概念"></a>1.并查集概念</h1><h1 id="2-用处"><a href="#2-用处" class="headerlink" title="2. 用处"></a>2. 用处</h1><h1 id="3-优缺点"><a href="#3-优缺点" class="headerlink" title="3. 优缺点"></a>3. 优缺点</h1><h1 id="4-实现方法"><a href="#4-实现方法" class="headerlink" title="4. 实现方法"></a>4. 实现方法</h1><h2 id="4-1-思想"><a href="#4-1-思想" class="headerlink" title="4.1 思想"></a>4.1 思想</h2><h2 id="4-2-API"><a href="#4-2-API" class="headerlink" title="4.2 API"></a>4.2 API</h2><h2 id="4-3-具体实现"><a href="#4-3-具体实现" class="headerlink" title="4.3 具体实现"></a>4.3 具体实现</h2><h2 id="4-4-Todo"><a href="#4-4-Todo" class="headerlink" title="4.4 Todo"></a>4.4 Todo</h2>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练语言模型</title>
    <url>/2020/04/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Todo</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>Todo</p>
<h2 id="glove"><a href="#glove" class="headerlink" title="glove"></a>glove</h2><p>Todo</p>
<h2 id="elmo"><a href="#elmo" class="headerlink" title="elmo"></a>elmo</h2><p>Todo</p>
<h2 id="gpt"><a href="#gpt" class="headerlink" title="gpt"></a>gpt</h2><p>Todo</p>
<h2 id="gpt2"><a href="#gpt2" class="headerlink" title="gpt2"></a>gpt2</h2><p>Todo</p>
<h2 id="bert"><a href="#bert" class="headerlink" title="bert"></a>bert</h2><p>Todo</p>
<h2 id="roberta"><a href="#roberta" class="headerlink" title="roberta"></a>roberta</h2><h3 id="静态Masking-vs-动态Masking"><a href="#静态Masking-vs-动态Masking" class="headerlink" title="静态Masking vs 动态Masking"></a>静态Masking vs 动态Masking</h3><ul>
<li>静态Masking：Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。</li>
<li>动态Masking：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。</li>
</ul>
<h3 id="with-NSP-vs-without-NSP"><a href="#with-NSP-vs-without-NSP" class="headerlink" title="with NSP vs without NSP"></a>with NSP vs without NSP</h3><p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。</p>
<h3 id="更大的mini-batch"><a href="#更大的mini-batch" class="headerlink" title="更大的mini-batch"></a>更大的mini-batch</h3><p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。</p>
<h3 id="更多的数据，更长时间的训练"><a href="#更多的数据，更长时间的训练" class="headerlink" title="更多的数据，更长时间的训练"></a>更多的数据，更长时间的训练</h3><p>RoBERTa用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h2 id="albert"><a href="#albert" class="headerlink" title="albert"></a>albert</h2><h3 id="对Embedding因式分解"><a href="#对Embedding因式分解" class="headerlink" title="对Embedding因式分解"></a>对Embedding因式分解</h3><p>ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的 embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从$O(V×H)O(V×H)O(V×H)$降低到了$O(V×E+E×H)O(V×E+E×H)O(V×E+E×H) $，当 $E&lt;&lt;H$时参数量减少的很明显。</p>
<h3 id="跨层的参数共享（Cross-layer-parameter-sharing）"><a href="#跨层的参数共享（Cross-layer-parameter-sharing）" class="headerlink" title="跨层的参数共享（Cross-layer parameter sharing）"></a>跨层的参数共享（Cross-layer parameter sharing）</h3><p>全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。</p>
<h3 id="句间连贯（Inter-sentence-coherence-loss）"><a href="#句间连贯（Inter-sentence-coherence-loss）" class="headerlink" title="句间连贯（Inter-sentence coherence loss）"></a>句间连贯（Inter-sentence coherence loss）</h3><p>在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。</p>
<h3 id="移除dropout"><a href="#移除dropout" class="headerlink" title="移除dropout"></a>移除dropout</h3><p>ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</p>
<h2 id="distillBert"><a href="#distillBert" class="headerlink" title="distillBert"></a>distillBert</h2><p>Todo</p>
<h2 id="xlnet"><a href="#xlnet" class="headerlink" title="xlnet"></a>xlnet</h2><p>Todo</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">一文揭开ALBERT的神秘面纱</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>glove</tag>
        <tag>elmo</tag>
        <tag>gpt</tag>
        <tag>gpt2</tag>
        <tag>bert</tag>
        <tag>roberta</tag>
        <tag>distillBert</tag>
        <tag>albert</tag>
        <tag>xlnet</tag>
      </tags>
  </entry>
  <entry>
    <title>focal loss</title>
    <url>/2020/03/31/focal-loss/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2020/03/29/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="速记"><a href="#速记" class="headerlink" title="速记"></a>速记</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">时间复杂度</th>
<th style="text-align:center">稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">选择排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">希尔排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">基数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">桶排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">计数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
</tbody>
</table>
</div>
<p>稳定性记忆方法：高复杂度（ $O(n^2)$ ）选择排序是例外，低复杂度（ $O(nlogn)$ ）归并排序是例外。<br><br>选择排序不稳定举例：(7) 2 5 9 3 4 [7] 1…当我们利用直接选择排序算法进行排序时候,(7)和1调换,(7)就跑到了[7]的后面了,原来的次序改变了,这样就不稳定了.</p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""快速排序"""</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    mid = array[start]  </span><br><span class="line">    low = start  </span><br><span class="line">    high = end </span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        array[low] = array[high]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        array[high] = array[low]</span><br><span class="line"></span><br><span class="line">    array[low] = mid  </span><br><span class="line"></span><br><span class="line">    quick_sort(alist, start, low - <span class="number">1</span>)  </span><br><span class="line">    quick_sort(alist, low + <span class="number">1</span>, end)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">quick_sort(array, <span class="number">0</span>, len(array) - <span class="number">1</span>)          <span class="comment"># 闭区间 [0, len(array) -1]</span></span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(array) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    mid = len(array) // <span class="number">2</span></span><br><span class="line">    left_array = merge_sort(array[:mid])</span><br><span class="line">    right_array = merge_sort(array[mid:])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> merge(left_array, right_array)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array, right_array)</span>:</span></span><br><span class="line">    array = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt;= right_array[j]:</span><br><span class="line">            array.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            array.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array):</span><br><span class="line">        array.append(left_array[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; len(right_array):</span><br><span class="line">        array.append(right_array[j])</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = merge_sort(array)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="topK问题"><a href="#topK问题" class="headerlink" title="topK问题"></a>topK问题</h2><h3 id="暴力解法"><a href="#暴力解法" class="headerlink" title="暴力解法"></a>暴力解法</h3><p>最基本的思路，将N个数进行完全排序，从中选出排在前K的元素即为所求。有了这个思路，我们可以选择相应的排序算法进行处理，目前来看快速排序，堆排序和归并排序都能达到$O(NlogN)$的时间复杂度。</p>
<h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><p>可以采用数据池的思想，选择其中前K个数作为数据池，后面的N-K个数与这K个数进行比较，若小于其中的任何一个数，则进行替换。这种思路的算法复杂度是$O(N*K)$</p>
<h3 id="大根堆"><a href="#大根堆" class="headerlink" title="大根堆"></a>大根堆</h3><p>大根堆维护一个大小为K的数组，目前该大根堆中的元素是排名前K的数，其中根是最大的数。此后，每次从原数组中取一个元素与根进行比较，如小于根的元素，则将根元素替换并进行堆调整（下沉），即保证大根堆中的元素仍然是排名前K的数，且根元素仍然最大；否则不予处理，取下一个数组元素继续该过程。该算法的时间复杂度是O(N*logK)，一般来说企业中都采用该策略处理topK问题，因为该算法不需要一次将原数组中的内容全部加载到内存中，而这正是海量数据处理必然会面临的一个关卡。</p>
<h3 id="快速排序-1"><a href="#快速排序-1" class="headerlink" title="快速排序"></a>快速排序</h3><p>利用快速排序的分划函数找到分划位置K，则其前面的内容即为所求。该算法是一种非常有效的处理方式，时间复杂度是O(N)（证明可以参考算法导论书籍）。对于能一次加载到内存中的数组，该策略非常优秀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topK</span><span class="params">(array, K)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(array) - <span class="number">1</span></span><br><span class="line">    index = quick_sort(array, left, right)</span><br><span class="line">    <span class="keyword">while</span> index+<span class="number">1</span> != K:</span><br><span class="line">        <span class="keyword">if</span> index+<span class="number">1</span> &lt; K:</span><br><span class="line">            index = quick_sort(array, index+<span class="number">1</span>, right)</span><br><span class="line">        <span class="keyword">elif</span> index+<span class="number">1</span> &gt; K:</span><br><span class="line">            index = quick_sort(array, left, index<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> array[:K]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, left, right)</span>:</span></span><br><span class="line">    <span class="comment"># print(left, right)</span></span><br><span class="line">    mid = array[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[right] &lt; mid:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        array[left] = array[right]</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[left] &gt;= mid:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        array[right] = array[left]</span><br><span class="line">    array[left] = mid</span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = topK(array, K=<span class="number">8</span>)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://www.jianshu.com/p/5b8f00d6a9d7" target="_blank" rel="noopener">topK算法问题</a></li>
<li><a href="https://blog.csdn.net/zyq522376829/article/details/47686867?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">海量数据处理 - 10亿个数中找出最大的10000个数（top K问题）</a></li>
</ol>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>快排排序</tag>
        <tag>归并排序</tag>
        <tag>topK</tag>
      </tags>
  </entry>
  <entry>
    <title>蓄水池抽样算法</title>
    <url>/2020/03/17/%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择一个数据？<br></li>
<li>解法：我们以概率1选择第一个数据，以1/2的概率选择第二个数据，以此类推，以1/m的概率选择第m个对象（如果后面某一数据一旦选中，替换掉以前选中的数据）。当所有数据流过时，每个对象具有相同的被选中的概率1/n。<br></li>
<li>证明：<script type="math/tex">P_m = \frac{1}{m} \cdot \frac{m}{m+1} \cdot \frac{m+1}{m+2} \cdot *** \cdot \frac{n-2}{n-1} \cdot \frac{n-1}{n} = \frac{1}{n}</script></li>
<li><p>Python代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n)</span>:</span></span><br><span class="line">    res = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        num = random.randint(<span class="number">1</span>, i):</span><br><span class="line">        <span class="keyword">if</span> i == num:</span><br><span class="line">            res = i</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="蓄水池抽样"><a href="#蓄水池抽样" class="headerlink" title="蓄水池抽样"></a>蓄水池抽样</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择k（k &lt; n）个数据？</li>
<li>解法：先选中前k个数据（当作一个蓄水池），然后以k/(k+1)的概率选择第k+1个数据，以k/(k+2)的概率选择第k+2个数据，以此类推以k/m的概率选择第m个数据，一旦后面有某个数据被选中，则随机替换蓄水池中的一个数据。最终每个数据被选中的概率均为k/n。</li>
<li>证明：<script type="math/tex; mode=display">
\begin{align}
P_m &= \frac{k}{m}  (\frac{m+1-k}{m+1} +\frac{k}{m+1} \frac{k-1}{k})  (\frac{m+2-k}{m+2} +\frac{k}{m+2}\frac{k-1}{k}) ...(\frac{n-k}{n} +\frac{k}{n}\frac{k-1}{k})  \\
 &= \frac{k}{n}
\end{align}</script></li>
<li>Python代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n, k)</span>:</span></span><br><span class="line">    res = [<span class="number">0</span>] * k</span><br><span class="line">    <span class="keyword">for</span> i range(k):</span><br><span class="line">        res[i] = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i range(k, n):</span><br><span class="line">        num = random.randint(<span class="number">0</span>, i)</span><br><span class="line">        <span class="keyword">if</span> num &lt; k:</span><br><span class="line">            res[num]  = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/87bc058b762e" target="_blank" rel="noopener">蓄水池抽样</a></li>
<li><a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">蓄水池抽样算法(Reservoir Sampling)</a></li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>蓄水池抽样</tag>
      </tags>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/2020/03/07/ResNet/</url>
    <content><![CDATA[<p>Todo</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Batch Normalization</title>
    <url>/2020/03/07/Batch-Normalization/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>由于下层的Layer的参数发生改变，导致上层的输入的分布发生改变，最后深度神经网络难以训练。（注意：底层为最下层）</p>
<ul>
<li>Internal Covariate Shift (内部协变量偏移): 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程</li>
<li>Internal Covariate Shift 带来的问题<ul>
<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>
<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</li>
</ul>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><ul>
<li>算法流程图<br><img src="images\BN.png" alt="BN"><br><br>($\epsilon$是为了增加训练稳定性而加入的小的常量数据)</li>
<li><p>测试阶段如何使用Batch Normalization？</p>
<ul>
<li>BN在每一层计算的$\mu$与$\sigma^2$都是基于当前batch中的训练数据，但是这就带来了一个问题：在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时$\mu$与$\sigma^2$的计算一定是有偏估计，这个时候该如何进行计算？</li>
<li>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的$\mu<em>{batch}$与$\sigma^2</em>{batch}$。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：<script type="math/tex; mode=display">\mu_{test} = E(\mu_{batch})</script><script type="math/tex; mode=display">\sigma^2_{test} = \frac{m}{m-1}E(\sigma^2_{batch})</script></li>
<li>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：<script type="math/tex; mode=display">BN(x_{test}) = \gamma \frac{X_{test} - \mu_{test}}{\sqrt{\sigma^2_{test} + \epsilon}} + \beta</script></li>
</ul>
</li>
<li><p>batch_normalization做了normalization后为什么要变回来？即 scale and shift<br><br> 如果只做normalize在某些情况下会出现问题，比如对象是Sigmoid函数的output，而且output是分布在Sigmoid函数的两侧，normalize会强制把output分布在Sigmoid函数的中间的非饱和区域，这样会导致这层网络所学习到的特征分布被normalize破坏。而上面算法的最后一步，scale and shift可以令零均值单位方差的分布（normalize之后的分布）通过调节$\gamma$和$\beta$变成任意更好的分布（对于喂给下一层网络来说）。因为这个$\gamma$和$\beta$是在训练过程中可以学习得到参数。</p>
</li>
</ul>
<h2 id="BN的优势"><a href="#BN的优势" class="headerlink" title="BN的优势"></a>BN的优势</h2><ul>
<li>BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</li>
<li>BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</li>
<li>BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>
<li>BN具有一定的正则化效果<br><br>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</li>
</ul>
<h2 id="BN的缺陷"><a href="#BN的缺陷" class="headerlink" title="BN的缺陷"></a>BN的缺陷</h2><ul>
<li>不适用于mini-batch非常小的训练环境</li>
<li>不适用于RNN，因为它是一个动态的网络结构，同一个batch中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得BN并不能正确的使用。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">Batch Normalization原理与实战</a></li>
<li><a href="https://www.zhihu.com/question/38102762/answer/607815171" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a></li>
<li><a href="https://www.zhihu.com/question/55917730/answer/154269264" target="_blank" rel="noopener">请问batch_normalization做了normalization后为什么要变回来？</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失、梯度爆炸、解决方法</title>
    <url>/2020/03/06/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><h2 id="梯度消失解决方法"><a href="#梯度消失解决方法" class="headerlink" title="梯度消失解决方法"></a>梯度消失解决方法</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>TextCNN&amp;TextGCN</title>
    <url>/2020/03/04/TextCNN/</url>
    <content><![CDATA[<h2 id="TextCNN原理"><a href="#TextCNN原理" class="headerlink" title="TextCNN原理"></a>TextCNN原理</h2><ul>
<li>框架图 <img src="images/TextCNN.png" alt="TextCNN"></li>
<li>$x_i: $ the k-dimensional word vector corresponding to the i-th word in the sentence.</li>
<li>向量拼接 <script type="math/tex; mode=display">x_{1:n} = concat(x_i, x_2, ...,x_n)</script></li>
<li>卷积操作 <script type="math/tex; mode=display">c_i = f (w \cdot x_{i:i+h-1} + b)</script></li>
<li>卷积结果 <script type="math/tex; mode=display">c = [c_1, c_2, c_3, ..., c_{n-h+1}]</script></li>
<li>max-over-time pooling <script type="math/tex; mode=display">\hat c = max(x)</script></li>
<li>这是一个卷积核的结果，实际网络中有256个卷积核和两个通道(static embedding 和 dynamic embedding)</li>
</ul>
<h2 id="TextGCN原理"><a href="#TextGCN原理" class="headerlink" title="TextGCN原理"></a>TextGCN原理</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TextCNN</tag>
        <tag>TextGCN</tag>
      </tags>
  </entry>
  <entry>
    <title>TF-IDF及TextRank</title>
    <url>/2020/03/04/TF-IDF%E5%8F%8ATextRank/</url>
    <content><![CDATA[<h2 id="1-TF-IDF"><a href="#1-TF-IDF" class="headerlink" title="1. TF-IDF"></a>1. TF-IDF</h2><ul>
<li>TF<ul>
<li>TF是词频(TF，Term Frequency): 词频（TF）表示词条（关键字）在文本中出现的频率。<script type="math/tex; mode=display">词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}</script></li>
</ul>
</li>
<li>IDF<ul>
<li>IDF是逆向文件频率 (IDF，Inverse Document Frequency): 某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<script type="math/tex; mode=display">逆文档频率(IDF) = log \frac{语料库的总文档数}{包含该词的文档数+1}</script></li>
</ul>
</li>
<li>TF-IDF<ul>
<li>TF-IDF实际上是 $TF * IDF$ 。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。<script type="math/tex; mode=display">TFIDF = TF \cdot IDF</script></li>
</ul>
</li>
</ul>
<h2 id="2-TextRank"><a href="#2-TextRank" class="headerlink" title="2. TextRank"></a>2. TextRank</h2><ul>
<li>基于词语词之间的共现性构建无向图。<br><br>参考<a href="https://my.oschina.net/u/3800567/blog/2870640" target="_blank" rel="noopener">jieba源码分析之关键字提取(TF-IDF/TextRank)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>TextRank</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2020/03/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-激活函数的理解"><a href="#1-激活函数的理解" class="headerlink" title="1.激活函数的理解"></a>1.激活函数的理解</h2><p>解决线形模型 y = wx + b 的弊端，线形分类模型的分界地方都是平面或者超平面，无法解决具有非线形模型特征的数据。如果没有激励函数，在这种情况下你每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，相当于没有隐藏层，网络的学习能力有限。<br><br><img src="images/非线型数据.png" alt="非线型数据"><br><br><a href="https://zhuanlan.zhihu.com/p/25279356" target="_blank" rel="noopener">形象的解释神经网络激活函数的作用是什么？</a><br><br><a href="https://blog.csdn.net/GreatXiang888/article/details/99296607" target="_blank" rel="noopener">常见激活函数，及其优缺点 - 面试篇</a></p>
<h2 id="2-Sigmoid"><a href="#2-Sigmoid" class="headerlink" title="2. Sigmoid"></a>2. Sigmoid</h2><ul>
<li>数学形式<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+ e^{-x}}</script><img src="images\sigmoid.png" alt="sigmoid"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\sigma ^{'}(x)  &=  (1+e^{-x}) ^ {-2}  \cdot e^{-x} \\
          &= \frac{e^{-x}}{(1+e^{-x})^2} \\
          &= \frac{1+e^{-x} -1}{(1+e^{-x})^2} \\
          &= \frac{1}{ 1+ e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
          &= \sigma(x) - \sigma ^ 2(x) \\
          &= \sigma(x) (1-\sigma(x))
\end{align}</script><img src="images\sigmoid导数.png" alt="sigmoid导数"></li>
<li>特点 <ul>
<li>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。 </li>
</ul>
</li>
<li>缺点<ul>
<li>如果我们初始化神经网络的权值为 $[0,1]$ 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 $(1,+∞)$ 区间内的值，则会出现梯度爆炸情况。梯度消失更容易产生。</li>
<li>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li>
<li>Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，这个特性会导致为在后面神经网络的高层处理中收到不是零中心的数据。这将导致梯度下降时的晃动，因为如果数据到了神经元永远时正数时，反向传播时权值w就会全为正数或者负数。</li>
</ul>
</li>
</ul>
<h2 id="3-tanh"><a href="#3-tanh" class="headerlink" title="3. tanh"></a>3. tanh</h2><ul>
<li>数学形式<script type="math/tex; mode=display">tanh(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{-x}}</script><img src="images\tanh.png" alt="tanh"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\tanh ^{'}(x)  &=  1 - tanh^2(x)
\end{align}</script><img src="images\tanh导数.png" alt="tanh导数"></li>
<li>特点 <ul>
<li>解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</li>
</ul>
</li>
</ul>
<h2 id="4-Relu"><a href="#4-Relu" class="headerlink" title="4. Relu"></a>4. Relu</h2><ul>
<li>数学形式<script type="math/tex; mode=display">Relu(x) = max(0, x)</script><img src="images\Relu.png" alt="Relu"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{equation}
Relu(x) = \left \{
\begin{aligned}
1 &  & {x>0} \\
0 &  & x<0 \\
None & & x=0 
\end{aligned}
\right.
\end{equation}</script></li>
<li>优点<ul>
<li>解决了梯度消失问题</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid</li>
</ul>
</li>
<li>缺点<ul>
<li>输出不是zero-centered</li>
<li>原点不可导</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见。例如w初始化全部为一些负数。 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
<li>leaky relu函数，$f(x)=max(αx,x)$ , 比如取α=0.01\alpha=0.01α=0.01，可以改善relu中x&lt;0部分的dead问题。</li>
</ul>
</li>
</ul>
<h2 id="5-如何选择合适的激活函数"><a href="#5-如何选择合适的激活函数" class="headerlink" title="5. 如何选择合适的激活函数"></a>5. 如何选择合适的激活函数</h2><ul>
<li>首选 ReLU，速度快，但是要注意学习速率的调整，</li>
<li>如果 ReLU 效果欠佳,尝试使用 Leaky ReLU 变种。</li>
<li>可以尝试使用 tanh。</li>
<li>Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。</li>
<li>在浅层神经网络中，选择使用哪种激励函数影响不大。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合及其解决方法</title>
    <url>/2020/03/01/%E8%BF%87%E6%8B%9F%E5%90%88%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-过拟合概念"><a href="#1-过拟合概念" class="headerlink" title="1. 过拟合概念"></a>1. 过拟合概念</h2><p>在深度学习或机器学习过程中，在训练集上表现过于优越，在验证集及测试集中表现不佳，模型泛化能力差。<br><a href="https://blog.csdn.net/jingbo18/article/details/80609006" target="_blank" rel="noopener">过拟合及常见处理办法整理</a></p>
<h2 id="2-常见原因"><a href="#2-常见原因" class="headerlink" title="2. 常见原因"></a>2. 常见原因</h2><p>原因主要是数据样本少及噪声多</p>
<ul>
<li>数据样本少</li>
<li>数据噪声多</li>
<li>模型复杂度高</li>
<li>迭代次数多</li>
</ul>
<h2 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h2><ul>
<li>获取更多的数据<ul>
<li>数据源获取更多的数据</li>
<li>数据增强</li>
</ul>
</li>
<li>更改模型结构<ul>
<li>换简单模型</li>
<li>L1 &amp; L2 范式</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
</li>
</ul>
<h2 id="4-数据增强"><a href="#4-数据增强" class="headerlink" title="4. 数据增强"></a>4. 数据增强</h2><p>自然语言处理技术中常用的数据增强方法</p>
<ul>
<li>同义词替换</li>
<li>随机插入</li>
<li>随机交换</li>
<li>随机删除</li>
</ul>
<h2 id="5-L1-amp-L2-范式"><a href="#5-L1-amp-L2-范式" class="headerlink" title="5. L1 &amp; L2 范式"></a>5. L1 &amp; L2 范式</h2><ul>
<li>范式定义<script type="math/tex; mode=display">\left (  \sum_{i}\left  | x_i \right | ^p   \right)^\frac{1}{p}</script></li>
<li>加入范式的目标函数<script type="math/tex; mode=display">J^2\left(w, b\right) = J\left(w, b\right) + \frac{\lambda}{2m}\Omega (w)</script></li>
<li>L1范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  = \sum_{i} \left | w_i  \right |</script></li>
<li>L2范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  =  \sum_{i} \left | w_i  \right | ^2</script></li>
<li>结论<br><br>L1 正则化用作特征选择，L2 正则化用作防止过拟合<br><br>参考介绍—-<a href="https://www.jianshu.com/p/569efedf6985" target="_blank" rel="noopener">机器学习中的正则化</a></li>
</ul>
<h2 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6. Dropout"></a>6. Dropout</h2><p>思想：以指数级别创建不同的网络结果，然后各个网络结果加权平均，解决过拟合<br><br>训练阶段，对每一个神经元的输出以keep_prob保留，1-keep_prob置为0；<br><br>预测阶段，对每一个神经元的输出乘以keep_prob。<br><br>缺点: 训练时间变长2-3倍。<br><br>参考介绍—-<a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
        <tag>L1范式</tag>
        <tag>L2范式</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN、LSTM、GRU对比</title>
    <url>/2020/02/29/RNN%E3%80%81LSTM%E3%80%81GRU%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="RNN框架图"><a href="#RNN框架图" class="headerlink" title="RNN框架图"></a>RNN框架图</h3><h3 id="梯度消失及爆炸"><a href="#梯度消失及爆炸" class="headerlink" title="梯度消失及爆炸"></a>梯度消失及爆炸</h3><p>Todo</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>从RNN网络出发开始介绍LSTM网络，记录其架构图及公式。</p>
<h3 id="LSTM的框架图"><a href="#LSTM的框架图" class="headerlink" title="LSTM的框架图"></a>LSTM的框架图</h3><p><img src="/images/LSTM.png" alt="LSTM"></p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><script type="math/tex; mode=display">f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)</script><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><script type="math/tex; mode=display">i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)</script><script type="math/tex; mode=display">\tilde{C_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)</script><script type="math/tex; mode=display">C_t=f_t * C_{t-1} + i_t * \tilde{C_t}</script><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><script type="math/tex; mode=display">o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)</script><script type="math/tex; mode=display">h_t=o_t * tanh(C_t)</script><h3 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h3><ul>
<li>首先需要明确的是，RNN 中的梯度消失/梯度爆炸和普通的 MLP 或者深层 CNN 中梯度消失/梯度爆炸的含义不一样。MLP/CNN 中不同的层有不同的参数，各是各的梯度；而 RNN 中同样的权重在各个时间步共享，最终的梯度 g = 各个时间步的梯度 g_t 的和。</li>
<li>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</li>
<li>LSTM 中梯度的传播有很多条路径，cell 这条路径上只有逐元素相乘和相加的操作，梯度流最稳定；但是其他路径上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘。</li>
<li>但是在其他路径上，LSTM 的梯度流和普通 RNN 没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 = 各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度 + 消失梯度 = 正常梯度）。因此 LSTM 通过改善一条路径上的梯度问题拯救了总体的远距离梯度。</li>
<li>同样，因为总的远距离梯度 = 各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度 = 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h3 id="GRU框架图"><a href="#GRU框架图" class="headerlink" title="GRU框架图"></a>GRU框架图</h3><p><img src="/images/LSTM.png" alt="GRU"></p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><script type="math/tex; mode=display">z_t = \sigma (W_z \cdot [h_{t-1}, x_t] + b_z)</script><script type="math/tex; mode=display">r_t = \sigma (W_r \cdot [h_{t-1}, x_t] + b_r)</script><script type="math/tex; mode=display">\tilde{h_t} = W \cdot [r_t * h_{t-1}, x_t] + b</script><script type="math/tex; mode=display">h_t = (1-z_t) * h_{t-1}  + z_t * \tilde{h_t}</script><h3 id="与LSTM区别"><a href="#与LSTM区别" class="headerlink" title="与LSTM区别"></a>与LSTM区别</h3><ul>
<li>GRU和LSTM的性能在很多任务上不分伯仲。</li>
<li>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。</li>
<li>从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将 hidden state 传给下一个单元，而LSTM则用 memory cell 把 hidden state 包装起来。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="https://www.zhihu.com/question/34878706/answer/665429718" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://blog.csdn.net/u012223913/article/details/77724621" target="_blank" rel="noopener">LSTM 和GRU的区别</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>自然语言处理</tag>
        <tag>RNN</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
</search>
