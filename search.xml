<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ResNet</title>
    <url>/2020/03/07/ResNet/</url>
    <content><![CDATA[<p>Todo</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Batch Normalization</title>
    <url>/2020/03/07/Batch-Normalization/</url>
    <content><![CDATA[<p>Todo</p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失、梯度爆炸、解决方法</title>
    <url>/2020/03/06/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><h2 id="梯度消失解决方法"><a href="#梯度消失解决方法" class="headerlink" title="梯度消失解决方法"></a>梯度消失解决方法</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>TextCNN&amp;TextGCN</title>
    <url>/2020/03/04/TextCNN/</url>
    <content><![CDATA[<h2 id="TextCNN原理"><a href="#TextCNN原理" class="headerlink" title="TextCNN原理"></a>TextCNN原理</h2><ul>
<li>框架图 <img src="images/TextCNN.png" alt="TextCNN"></li>
<li>$x_i: $ the k-dimensional word vector corresponding to the i-th word in the sentence.</li>
<li>向量拼接 <script type="math/tex; mode=display">x_{1:n} = concat(x_i, x_2, ...,x_n)</script></li>
<li>卷积操作 <script type="math/tex; mode=display">c_i = f (w \cdot x_{i:i+h-1} + b)</script></li>
<li>卷积结果 <script type="math/tex; mode=display">c = [c_1, c_2, c_3, ..., c_{n-h+1}]</script></li>
<li>max-over-time pooling <script type="math/tex; mode=display">\hat c = max(x)</script></li>
<li>这是一个卷积核的结果，实际网络中有256个卷积核和两个通道(static embedding 和 dynamic embedding)</li>
</ul>
<h2 id="TextGCN原理"><a href="#TextGCN原理" class="headerlink" title="TextGCN原理"></a>TextGCN原理</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TextCNN</tag>
        <tag>TextGCN</tag>
      </tags>
  </entry>
  <entry>
    <title>TF-IDF及TextRank</title>
    <url>/2020/03/04/TF-IDF%E5%8F%8ATextRank/</url>
    <content><![CDATA[<h2 id="1-TF-IDF"><a href="#1-TF-IDF" class="headerlink" title="1. TF-IDF"></a>1. TF-IDF</h2><ul>
<li>TF<ul>
<li>TF是词频(TF，Term Frequency): 词频（TF）表示词条（关键字）在文本中出现的频率。<script type="math/tex; mode=display">词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}</script></li>
</ul>
</li>
<li>IDF<ul>
<li>IDF是逆向文件频率 (IDF，Inverse Document Frequency): 某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<script type="math/tex; mode=display">逆文档频率(IDF) = log \frac{语料库的总文档数}{包含该词的文档数+1}</script></li>
</ul>
</li>
<li>TF-IDF<ul>
<li>TF-IDF实际上是 $TF * IDF$ 。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。<script type="math/tex; mode=display">TFIDF = TF \cdot IDF</script></li>
</ul>
</li>
</ul>
<h2 id="2-TextRank"><a href="#2-TextRank" class="headerlink" title="2. TextRank"></a>2. TextRank</h2><ul>
<li>基于词语词之间的共现性构建无向图。<br><br>参考<a href="https://my.oschina.net/u/3800567/blog/2870640" target="_blank" rel="noopener">jieba源码分析之关键字提取(TF-IDF/TextRank)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>TextRank</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2020/03/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-激活函数的理解"><a href="#1-激活函数的理解" class="headerlink" title="1.激活函数的理解"></a>1.激活函数的理解</h2><p>解决线形模型 y = wx + b 的弊端，线形分类模型的分界地方都是平面或者超平面，无法解决具有非线形模型特征的数据。如果没有激励函数，在这种情况下你每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，相当于没有隐藏层，网络的学习能力有限。<br><br><img src="images/非线型数据.png" alt="非线型数据"><br><br><a href="https://zhuanlan.zhihu.com/p/25279356" target="_blank" rel="noopener">形象的解释神经网络激活函数的作用是什么？</a><br><br><a href="https://blog.csdn.net/GreatXiang888/article/details/99296607" target="_blank" rel="noopener">常见激活函数，及其优缺点 - 面试篇</a></p>
<h2 id="2-Sigmoid"><a href="#2-Sigmoid" class="headerlink" title="2. Sigmoid"></a>2. Sigmoid</h2><ul>
<li>数学形式<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+ e^{-x}}</script><img src="images\sigmoid.png" alt="sigmoid"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\sigma ^{'}(x)  &=  (1+e^{-x}) ^ {-2}  \cdot e^{-x} \\
          &= \frac{e^{-x}}{(1+e^{-x})^2} \\
          &= \frac{1+e^{-x} -1}{(1+e^{-x})^2} \\
          &= \frac{1}{ 1+ e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
          &= \sigma(x) - \sigma ^ 2(x) \\
          &= \sigma(x) (1-\sigma(x))
\end{align}</script><img src="images\sigmoid导数.png" alt="sigmoid导数"></li>
<li>特点 <ul>
<li>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。 </li>
</ul>
</li>
<li>缺点<ul>
<li>如果我们初始化神经网络的权值为 $[0,1]$ 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 $(1,+∞)$ 区间内的值，则会出现梯度爆炸情况。梯度消失更容易产生。</li>
<li>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li>
<li>Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，这个特性会导致为在后面神经网络的高层处理中收到不是零中心的数据。这将导致梯度下降时的晃动，因为如果数据到了神经元永远时正数时，反向传播时权值w就会全为正数或者负数。</li>
</ul>
</li>
</ul>
<h2 id="3-tanh"><a href="#3-tanh" class="headerlink" title="3. tanh"></a>3. tanh</h2><ul>
<li>数学形式<script type="math/tex; mode=display">tanh(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{-x}}</script><img src="images\tanh.png" alt="tanh"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\tanh ^{'}(x)  &=  1 - tanh^2(x)
\end{align}</script><img src="images\tanh导数.png" alt="tanh导数"></li>
<li>特点 <ul>
<li>解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</li>
</ul>
</li>
</ul>
<h2 id="4-Relu"><a href="#4-Relu" class="headerlink" title="4. Relu"></a>4. Relu</h2><ul>
<li>数学形式<script type="math/tex; mode=display">Relu(x) = max(0, x)</script><img src="images\Relu.png" alt="Relu"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{equation}
Relu(x) = \left \{
\begin{aligned}
1 &  & {x>0} \\
0 &  & x<0 \\
None & & x=0 
\end{aligned}
\right.
\end{equation}</script></li>
<li>优点<ul>
<li>解决了梯度消失问题</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid</li>
</ul>
</li>
<li>缺点<ul>
<li>输出不是zero-centered</li>
<li>原点不可导</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见。例如w初始化全部为一些负数。 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
<li>leaky relu函数，$f(x)=max(αx,x)$ , 比如取α=0.01\alpha=0.01α=0.01，可以改善relu中x&lt;0部分的dead问题。</li>
</ul>
</li>
</ul>
<h2 id="5-如何选择合适的激活函数"><a href="#5-如何选择合适的激活函数" class="headerlink" title="5. 如何选择合适的激活函数"></a>5. 如何选择合适的激活函数</h2><ul>
<li>首选 ReLU，速度快，但是要注意学习速率的调整，</li>
<li>如果 ReLU 效果欠佳,尝试使用 Leaky ReLU 变种。</li>
<li>可以尝试使用 tanh。</li>
<li>Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。</li>
<li>在浅层神经网络中，选择使用哪种激励函数影响不大。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>手推LR</title>
    <url>/2020/03/01/%E6%89%8B%E6%8E%A8LR/</url>
    <content><![CDATA[<h2 id="1-假说"><a href="#1-假说" class="headerlink" title="1. 假说"></a>1. 假说</h2><p>假设样本的标签为0和1， h(x) 为取得预测为标签1的概率。</p>
<script type="math/tex; mode=display">h(x) = \frac{1}{1+e^{-w^Tx}}</script><h2 id="2-后验概率"><a href="#2-后验概率" class="headerlink" title="2. 后验概率"></a>2. 后验概率</h2><script type="math/tex; mode=display">p(y=1|x) = h(x) = \frac{1}{1+e^{-w^Tx}}</script><script type="math/tex; mode=display">p(y=0|x) = 1- h(x) = 1- \frac{1}{1+e^{-w^Tx}} = \frac{e^{-w^Tx}}{1+e^{-w^Tx}}</script><h2 id="3-似然函数"><a href="#3-似然函数" class="headerlink" title="3. 似然函数"></a>3. 似然函数</h2><p>N 为数据样本数</p>
<script type="math/tex; mode=display">L(w) = \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i}  = \prod_{i=1}^N h(x_i)^{y_i}(1-h(x_i))^{1-y_i}</script><h2 id="4-对数似然"><a href="#4-对数似然" class="headerlink" title="4. 对数似然"></a>4. 对数似然</h2><script type="math/tex; mode=display">
\begin{align}
    log(L(w)) &= log( \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i} ) \\
    &=  \sum_{i=1}^N  (y_i log(h(x_i))  + (1-y_i)log(1-h(x_i)) ) \\
    &= \sum_{i=1}^N \{ y_i\{log(h(x_i)) - log(1-h(x_i))\} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{ y_i log\frac{h(x_i)}{1-h(x_i)} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) + log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(\frac{1+e^{-w^Tx}}{ e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}
\end{align}</script><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5. 损失函数"></a>5. 损失函数</h2><p>似然函数乘以 -1/N</p>
<script type="math/tex; mode=display">J(w) = -\frac{1}{N} log(L(w)) = -\frac{1}{N} \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}</script><h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6. 梯度下降"></a>6. 梯度下降</h2><p>Loss 如下：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{ \partial J(w) }{\partial w} &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} \frac{\partial h(x_i)}{\partial w} - \frac{1-y_i}{1-h(x_i)} \frac{\partial h(x_i)}{\partial w}  \} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \}\frac{\partial h(x_i)}{\partial w}  \\
    &=  -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \} h(x_i)(1-h(x_i)) \frac{\partial w^Tx_i}{\partial w} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i (1-h(x_i)) -(1-y_i)h(x_i) \} x_i \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i - h(x_i)\} x_i
\end{align}</script><p>参数更新如下：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= w-\alpha \frac{\partial J(w)}{ \partial w}\\
&= w- \frac{1}{N}\sum_{i=1}^N \{  h(x_i) -y_i\} x_i
\end{align}</script><h2 id="7-一些问题"><a href="#7-一些问题" class="headerlink" title="7. 一些问题"></a>7. 一些问题</h2><ul>
<li><p>LR为什么是线性模型？<br><br>Logistic Regression从几率的概念构建线性回归模型。一个事件发生的几率（odds）为该事件发生的概率与不发生概率的比值，几率的取值范围为$[0,+\infty)$，其对数的取值范围为实数域，所以，可以将对数几率作为因变量构建线性回归模型: $log\frac{p}{1-p} = w^Tx$,  由此可得 $p = \frac{1}{1+e^{-w^Tx}}$。这便是Logistic Regression采用sigmoid函数的原因，sigmoid函数将自变量的线性组合映射到（0,1），用以表述分类的概率特性。从sigmoid函数看出，当$w^Tx &gt;0 $ 时，y=1，否则 y=0。$w^Tx=0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）, 所以说逻辑回归本质上是一个线性模型。</p>
</li>
<li><p>为什么逻辑回归比线性回归要好？<br><br>逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。</p>
</li>
<li><p>sigmoid函数</p>
<script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script><script type="math/tex; mode=display">\sigma^{'}(z) = \frac{e^{-z}}{(1+e^{-z})^2} = \sigma(z) (1- \sigma(z))</script><ul>
<li>优点：<ul>
<li>Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。</li>
<li>求导简单。</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于其软饱和性，容易产生梯度消失，导致训练出现问题。</li>
</ul>
</li>
</ul>
</li>
<li>LR 如何解决多分类问题？<br><br> 如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即 y=i 的时候意味着 y 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）, 模型通过 softmax 函数来对概率建模。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/ltlitao717/article/details/75453480" target="_blank" rel="noopener">机器学习杂货铺-手推LR</a></li>
<li><a href="https://blog.csdn.net/weixin_44915167/article/details/89377022?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">机器学习面试题之LR</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合及其解决方法</title>
    <url>/2020/03/01/%E8%BF%87%E6%8B%9F%E5%90%88%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-过拟合概念"><a href="#1-过拟合概念" class="headerlink" title="1. 过拟合概念"></a>1. 过拟合概念</h2><p>在深度学习或机器学习过程中，在训练集上表现过于优越，在验证集及测试集中表现不佳，模型泛化能力差。<br><a href="https://blog.csdn.net/jingbo18/article/details/80609006" target="_blank" rel="noopener">过拟合及常见处理办法整理</a></p>
<h2 id="2-常见原因"><a href="#2-常见原因" class="headerlink" title="2. 常见原因"></a>2. 常见原因</h2><p>原因主要是数据样本少及噪声多</p>
<ul>
<li>数据样本少</li>
<li>数据噪声多</li>
<li>模型复杂度高</li>
<li>迭代次数多</li>
</ul>
<h2 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h2><ul>
<li>获取更多的数据<ul>
<li>数据源获取更多的数据</li>
<li>数据增强</li>
</ul>
</li>
<li>更改模型结构<ul>
<li>换简单模型</li>
<li>L1 &amp; L2 范式</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
</li>
</ul>
<h2 id="4-数据增强"><a href="#4-数据增强" class="headerlink" title="4. 数据增强"></a>4. 数据增强</h2><p>自然语言处理技术中常用的数据增强方法</p>
<ul>
<li>同义词替换</li>
<li>随机插入</li>
<li>随机交换</li>
<li>随机删除</li>
</ul>
<h2 id="5-L1-amp-L2-范式"><a href="#5-L1-amp-L2-范式" class="headerlink" title="5. L1 &amp; L2 范式"></a>5. L1 &amp; L2 范式</h2><ul>
<li>范式定义<script type="math/tex; mode=display">\left (  \sum_{i}\left  | x_i \right | ^p   \right)^\frac{1}{p}</script></li>
<li>加入范式的目标函数<script type="math/tex; mode=display">J^2\left(w, b\right) = J\left(w, b\right) + \frac{\lambda}{2m}\Omega (w)</script></li>
<li>L1范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  = \sum_{i} \left | w_i  \right |</script></li>
<li>L2范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  =  \sum_{i} \left | w_i  \right | ^2</script></li>
<li>结论<br><br>L1 正则化用作特征选择，L2 正则化用作防止过拟合<br><br>参考介绍—-<a href="https://www.jianshu.com/p/569efedf6985" target="_blank" rel="noopener">机器学习中的正则化</a></li>
</ul>
<h2 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6. Dropout"></a>6. Dropout</h2><p>训练阶段，对每一个神经元的输出以keep_prob保留，1-keep_prob置为0；<br><br>预测阶段，对每一个神经元的输出乘以keep_prob。<br><br>参考介绍—-<a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
        <tag>L1范式</tag>
        <tag>L2范式</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM 常见问题</title>
    <url>/2020/02/29/LSTM-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="1-LSTM-原理"><a href="#1-LSTM-原理" class="headerlink" title="1 LSTM 原理"></a>1 LSTM 原理</h2><p>从RNN网络出发开始介绍LSTM网络<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">理解LSTM网络</a>，记录其架构图及公式。</p>
<h3 id="1-1-LSTM的框架图"><a href="#1-1-LSTM的框架图" class="headerlink" title="1.1 LSTM的框架图"></a>1.1 LSTM的框架图</h3><p><img src="http://q6goq372a.bkt.clouddn.com/LSTM.png" alt="LSTM"></p>
<h3 id="1-2-遗忘门"><a href="#1-2-遗忘门" class="headerlink" title="1.2 遗忘门"></a>1.2 遗忘门</h3><script type="math/tex; mode=display">f_t=\delta(W_f\cdot[h_{t-1},x_t]+b_f)</script><h3 id="1-3-输入门"><a href="#1-3-输入门" class="headerlink" title="1.3 输入门"></a>1.3 输入门</h3><script type="math/tex; mode=display">i_t=\delta(W_i\cdot[h_{t-1},x_t]+b_i)</script><script type="math/tex; mode=display">\tilde{C_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)</script><script type="math/tex; mode=display">C_t=f_t * h_{t-1} + i_t * \tilde{C_t}</script><h3 id="1-4-输出门"><a href="#1-4-输出门" class="headerlink" title="1.4 输出门"></a>1.4 输出门</h3><script type="math/tex; mode=display">o_t=\delta(W_o\cdot[h_{t-1},x_t]+b_o)</script><script type="math/tex; mode=display">h_t=o_t * tanh(C_t)</script>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
</search>
