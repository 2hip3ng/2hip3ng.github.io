<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>手推LR</title>
    <url>/2020/03/01/%E6%89%8B%E6%8E%A8LR/</url>
    <content><![CDATA[<h2 id="1-假说"><a href="#1-假说" class="headerlink" title="1. 假说"></a>1. 假说</h2><p>假设样本的标签为0和1， h(x) 为取得预测为标签1的概率。</p>
<script type="math/tex; mode=display">h(x) = \frac{1}{1+e^{-w^Tx}}</script><h2 id="2-后验概率"><a href="#2-后验概率" class="headerlink" title="2. 后验概率"></a>2. 后验概率</h2><script type="math/tex; mode=display">p(y=1|x) = h(x) = \frac{1}{1+e^{-w^Tx}}</script><script type="math/tex; mode=display">p(y=0|x) = 1- h(x) = 1- \frac{1}{1+e^{-w^Tx}} = \frac{e^{-w^Tx}}{1+e^{-w^Tx}}</script><h2 id="3-似然函数"><a href="#3-似然函数" class="headerlink" title="3. 似然函数"></a>3. 似然函数</h2><p>N 为数据样本数</p>
<script type="math/tex; mode=display">L(w) = \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i}  = \prod_{i=1}^N h(x_i)^{y_i}(1-h(x_i))^{1-y_i}</script><h2 id="4-对数似然"><a href="#4-对数似然" class="headerlink" title="4. 对数似然"></a>4. 对数似然</h2><script type="math/tex; mode=display">
\begin{align}
    log(L(w)) &= log( \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i} ) \\
    &=  \sum_{i=1}^N  (y_i log(h(x_i))  + (1-y_i)log(1-h(x_i)) ) \\
    &= \sum_{i=1}^N \{ y_i\{log(h(x_i)) - log(1-h(x_i))\} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{ y_i log\frac{h(x_i)}{1-h(x_i)} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) + log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(\frac{1+e^{-w^Tx}}{ e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}
\end{align}</script><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5. 损失函数"></a>5. 损失函数</h2><p>似然函数乘以 -1/N</p>
<script type="math/tex; mode=display">J(w) = -\frac{1}{N} log(L(w)) = -\frac{1}{N} \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}</script><h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6. 梯度下降"></a>6. 梯度下降</h2><p>Loss 如下：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{ \partial J(w) }{\partial w} &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} \frac{\partial h(x_i)}{\partial w} - \frac{1-y_i}{1-h(x_i)} \frac{\partial h(x_i)}{\partial w}  \} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \}\frac{\partial h(x_i)}{\partial w}  \\
    &=  -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \} h(x_i)(1-h(x_i)) \frac{\partial w^Tx_i}{\partial w} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i (1-h(x_i)) -(1-y_i)h(x_i) \} x_i \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i - h(x_i)\} x_i
\end{align}</script><p>参数更新如下：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= w-\alpha \frac{\partial J(w)}{ \partial w}\\
&= w- \frac{1}{N}\sum_{i=1}^N \{  h(x_i) -y_i\} x_i
\end{align}</script><h2 id="7-一些问题"><a href="#7-一些问题" class="headerlink" title="7. 一些问题"></a>7. 一些问题</h2><ul>
<li><p>LR为什么是线性模型？<br><br>Logistic Regression从几率的概念构建线性回归模型。一个事件发生的几率（odds）为该事件发生的概率与不发生概率的比值，几率的取值范围为$[0,+\infty)$，其对数的取值范围为实数域，所以，可以将对数几率作为因变量构建线性回归模型: $log\frac{p}{1-p} = w^Tx$,  由此可得 $p = \frac{1}{1+e^{-w^Tx}}$。这便是Logistic Regression采用sigmoid函数的原因，sigmoid函数将自变量的线性组合映射到（0,1），用以表述分类的概率特性。从sigmoid函数看出，当$w^Tx &gt;0 $ 时，y=1，否则 y=0。$w^Tx=0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）, 所以说逻辑回归本质上是一个线性模型。</p>
</li>
<li><p>为什么逻辑回归比线性回归要好？<br><br>逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。</p>
</li>
<li><p>sigmoid函数</p>
<script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script><script type="math/tex; mode=display">\sigma^{'}(z) = \frac{e^{-z}}{(1+e^{-z})^2} = \sigma(z) (1- \sigma(z))</script><ul>
<li>优点：<ul>
<li>Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。</li>
<li>求导简单。</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于其软饱和性，容易产生梯度消失，导致训练出现问题。</li>
</ul>
</li>
</ul>
</li>
<li>LR 如何解决多分类问题？<br><br> 如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即 y=i 的时候意味着 y 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）, 模型通过 softmax 函数来对概率建模。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/ltlitao717/article/details/75453480" target="_blank" rel="noopener">机器学习杂货铺-手推LR</a></li>
<li><a href="https://blog.csdn.net/weixin_44915167/article/details/89377022?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">机器学习面试题之LR</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合及其解决方法</title>
    <url>/2020/03/01/%E8%BF%87%E6%8B%9F%E5%90%88%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-过拟合概念"><a href="#1-过拟合概念" class="headerlink" title="1. 过拟合概念"></a>1. 过拟合概念</h2><p>在深度学习或机器学习过程中，在训练集上表现过于优越，在验证集及测试集中表现不佳，模型泛化能力差。<br><a href="https://blog.csdn.net/jingbo18/article/details/80609006" target="_blank" rel="noopener">过拟合及常见处理办法整理</a></p>
<h2 id="2-常见原因"><a href="#2-常见原因" class="headerlink" title="2. 常见原因"></a>2. 常见原因</h2><p>原因主要是数据样本少及噪声多</p>
<ul>
<li>数据样本少</li>
<li>数据噪声多</li>
<li>模型复杂度高</li>
<li>迭代次数多</li>
</ul>
<h2 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h2><ul>
<li>获取更多的数据<ul>
<li>数据源获取更多的数据</li>
<li>数据增强</li>
</ul>
</li>
<li>更改模型结构<ul>
<li>换简单模型</li>
<li>L1 &amp; L2 范式</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
</li>
</ul>
<h2 id="4-数据增强"><a href="#4-数据增强" class="headerlink" title="4. 数据增强"></a>4. 数据增强</h2><p>自然语言处理技术中常用的数据增强方法</p>
<ul>
<li>同义词替换</li>
<li>随机插入</li>
<li>随机交换</li>
<li>随机删除</li>
</ul>
<h2 id="5-L1-amp-L2-范式"><a href="#5-L1-amp-L2-范式" class="headerlink" title="5. L1 &amp; L2 范式"></a>5. L1 &amp; L2 范式</h2><ul>
<li>范式定义<script type="math/tex; mode=display">\left (  \sum_{i}\left  | x_i \right | ^p   \right)^\frac{1}{p}</script></li>
<li>加入范式的目标函数<script type="math/tex; mode=display">J^2\left(w, b\right) = J\left(w, b\right) + \frac{\lambda}{2m}\Omega (w)</script></li>
<li>L1范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  = \sum_{i} \left | w_i  \right |</script></li>
<li>L2范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  =  \sum_{i} \left | w_i  \right | ^2</script></li>
<li>结论<br><br>L1 正则化用作特征选择，L2 正则化用作防止过拟合<br><br>参考介绍—-<a href="https://www.jianshu.com/p/569efedf6985" target="_blank" rel="noopener">机器学习中的正则化</a></li>
</ul>
<h2 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6. Dropout"></a>6. Dropout</h2><p>训练阶段，对每一个神经元的输出以keep_prob保留，1-keep_prob置为0；<br><br>预测阶段，对每一个神经元的输出乘以keep_prob。<br><br>参考介绍—-<a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
        <tag>L1范式</tag>
        <tag>L2范式</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM 常见问题</title>
    <url>/2020/02/29/LSTM-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="1-LSTM-原理"><a href="#1-LSTM-原理" class="headerlink" title="1 LSTM 原理"></a>1 LSTM 原理</h2><p>从RNN网络出发开始介绍LSTM网络<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">理解LSTM网络</a>，记录其架构图及公式。</p>
<h3 id="1-1-LSTM的框架图"><a href="#1-1-LSTM的框架图" class="headerlink" title="1.1 LSTM的框架图"></a>1.1 LSTM的框架图</h3><p><img src="http://q6goq372a.bkt.clouddn.com/LSTM.png" alt="LSTM"></p>
<h3 id="1-2-遗忘门"><a href="#1-2-遗忘门" class="headerlink" title="1.2 遗忘门"></a>1.2 遗忘门</h3><script type="math/tex; mode=display">f_t=\delta(W_f\cdot[h_{t-1},x_t]+b_f)</script><h3 id="1-3-输入门"><a href="#1-3-输入门" class="headerlink" title="1.3 输入门"></a>1.3 输入门</h3><script type="math/tex; mode=display">i_t=\delta(W_i\cdot[h_{t-1},x_t]+b_i)</script><script type="math/tex; mode=display">\tilde{C_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)</script><script type="math/tex; mode=display">C_t=f_t * h_{t-1} + i_t * \tilde{C_t}</script><h3 id="1-4-输出门"><a href="#1-4-输出门" class="headerlink" title="1.4 输出门"></a>1.4 输出门</h3><script type="math/tex; mode=display">o_t=\delta(W_o\cdot[h_{t-1},x_t]+b_o)</script><script type="math/tex; mode=display">h_t=o_t * tanh(C_t)</script>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>blog_test</title>
    <url>/2020/02/29/blog-test/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
