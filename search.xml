<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>旋转数组最小数字</title>
    <url>/2020/08/07/%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<h1 id="1-旋转数组最小数字"><a href="#1-旋转数组最小数字" class="headerlink" title="1. 旋转数组最小数字"></a>1. 旋转数组最小数字</h1><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。  </p>
<p>示例1：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入：[3,4,5,1,2]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure>
<p>示例2：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入：[2,2,2,0,1]</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure>
<h1 id="2-实现"><a href="#2-实现" class="headerlink" title="2. 实现"></a>2. 实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minArray</span><span class="params">(self, numbers: List[int])</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = len(numbers)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = (left+right)//<span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> numbers[mid] &gt; numbers[right]:</span><br><span class="line">                left = mid+<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> numbers[mid] == numbers[right]:</span><br><span class="line">                <span class="comment"># 除了这里就是一个普通的二分，记忆这个技巧</span></span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="comment"># print(left,right)</span></span><br><span class="line">        <span class="keyword">return</span> numbers[left]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>二分法</tag>
        <tag>旋转数组</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention 机制</title>
    <url>/2020/08/05/Attention-%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h1 id="1-Attention-计算方法"><a href="#1-Attention-计算方法" class="headerlink" title="1.  Attention 计算方法"></a>1.  Attention 计算方法</h1><h2 id="1-1-点乘"><a href="#1-1-点乘" class="headerlink" title="1.1 点乘"></a>1.1 点乘</h2><p>$score = a^T * b$</p>
<h2 id="1-2-权值网络映射"><a href="#1-2-权值网络映射" class="headerlink" title="1.2 权值网络映射"></a>1.2 权值网络映射</h2><p>$score = a^T W b$ </p>
<h2 id="1-3-拼接映射"><a href="#1-3-拼接映射" class="headerlink" title="1.3 拼接映射"></a>1.3 拼接映射</h2><p>$score = v^T tanh(W[a, b])$</p>
<h1 id="2-Attention-种类"><a href="#2-Attention-种类" class="headerlink" title="2 Attention 种类"></a>2 Attention 种类</h1><h2 id="2-1-Soft-Attention-And-Hard-Attention"><a href="#2-1-Soft-Attention-And-Hard-Attention" class="headerlink" title="2.1 Soft Attention And Hard Attention"></a>2.1 Soft Attention And Hard Attention</h2><ul>
<li>soft attention： 加权平均</li>
<li>hard attention： 取最大的一个或者抽样</li>
</ul>
<h2 id="2-2-Global-Attention-And-Local-Attention"><a href="#2-2-Global-Attention-And-Local-Attention" class="headerlink" title="2.2 Global Attention And Local Attention"></a>2.2 Global Attention And Local Attention</h2><ul>
<li>Global Attention： 全局</li>
<li>Local Attention：预测中心点</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>优化器算法</title>
    <url>/2020/08/05/%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="1-BGD（Batch-Gradient-Descent）"><a href="#1-BGD（Batch-Gradient-Descent）" class="headerlink" title="1. BGD（Batch Gradient Descent）"></a>1. BGD（Batch Gradient Descent）</h1><p>思想：使用整个数据训练集计算损失函数</p>
<p>缺点：计算过程很慢</p>
<h1 id="2-SGD（Stochastic-Gradient-Descent）"><a href="#2-SGD（Stochastic-Gradient-Descent）" class="headerlink" title="2. SGD（Stochastic Gradient Descent）"></a>2. SGD（Stochastic Gradient Descent）</h1><p>和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 <strong>SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。</strong></p>
<p>优点：收敛快</p>
<p>缺点：引入噪声，存在局部最优</p>
<h1 id="3-MBGD（Mini-Batch-Gradient-Descent）"><a href="#3-MBGD（Mini-Batch-Gradient-Descent）" class="headerlink" title="3. MBGD（Mini-Batch Gradient Descent）"></a>3. MBGD（Mini-Batch Gradient Descent）</h1><p>MBGD 每一次利用一小批样本，即 n 个样本进行计算</p>
<p>以上三个算法的缺点：</p>
<ul>
<li>Learning Rate 如果选择的太小，收敛速度会很慢，如果太大，Loss Function 就会在极小值处不停地震荡甚至偏离。</li>
<li>SGD对所有参数更新时应用同样的 Learning Rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。Learning Rate会随着更新的次数逐渐变小。</li>
</ul>
<h1 id="4-Momentum"><a href="#4-Momentum" class="headerlink" title="4. Momentum"></a>4. Momentum</h1><script type="math/tex; mode=display">m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t</script><p>其中$\beta_1 取0.9$</p>
<p>一阶动量：各个时刻梯度方向的指数移动平均值，约最近 $\frac{1}{1-\beta}$个时刻的梯度向量的平均值。</p>
<p>优点：<strong>使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</strong></p>
<h1 id="5-Adagrad（Adaptive-gradient-algorithm）"><a href="#5-Adagrad（Adaptive-gradient-algorithm）" class="headerlink" title="5. Adagrad（Adaptive gradient algorithm）"></a>5. Adagrad（Adaptive gradient algorithm）</h1><script type="math/tex; mode=display">\theta = \theta - \frac{\eta}{\sqrt{\sum_{\tau=1}^{t}g_{\tau}^2 + \epsilon} * g_t}</script><p>二阶动量$v<em>t = \sum</em>{\tau=1}^{t}g_\tau^2$ ：所有梯度值的平方和。</p>
<p>思想：</p>
<ul>
<li>对经常更新的参数，学习率慢一些；</li>
<li>对偶尔更新的参数，学习率快一些。</li>
</ul>
<p>优点：自适应，不需要人调节</p>
<p>缺点：学习率越来越小，趋于0</p>
<h1 id="6-Adam"><a href="#6-Adam" class="headerlink" title="6. Adam"></a>6. Adam</h1><p>结合Momentum及自适应</p>
<p>具有一阶动量及二阶动量</p>
<h1 id="7-AdamW"><a href="#7-AdamW" class="headerlink" title="7. AdamW"></a>7. AdamW</h1><p>加入了Weight Decay</p>
<h1 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h1><p>1 <a href="https://zhuanlan.zhihu.com/p/55150256" target="_blank" rel="noopener">优化算法Optimizer比较和总结</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BGD</tag>
        <tag>SGD</tag>
        <tag>Momentum</tag>
        <tag>Adagrad</tag>
        <tag>RMSProp</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title>字典树</title>
    <url>/2020/08/05/%E5%AD%97%E5%85%B8%E6%A0%91/</url>
    <content><![CDATA[<h1 id="1-字典树（Trie）"><a href="#1-字典树（Trie）" class="headerlink" title="1 字典树（Trie）"></a>1 字典树（Trie）</h1><h1 id="2-实现方法"><a href="#2-实现方法" class="headerlink" title="2 实现方法"></a>2 实现方法</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize your data structure here.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line">        <span class="comment"># 特殊标记</span></span><br><span class="line">        self.word_end = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, word: str)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Inserts a word into the trie.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> char <span class="keyword">in</span> cur_node:</span><br><span class="line">                cur_node[char] = &#123;&#125;</span><br><span class="line">            cur_node = cur_node[char]</span><br><span class="line">        cur_node[self.word_end] = <span class="literal">True</span></span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, word: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns if the word is in the trie.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> cur_node:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            cur_node = cur_node[char]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.word_end <span class="keyword">not</span> <span class="keyword">in</span> cur_node:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startsWith</span><span class="params">(self, prefix: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Returns if there is any word in the trie that starts with the given prefix.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> prefix:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> char <span class="keyword">in</span> cur_node:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            cur_node = cur_node[char]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your Trie object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = Trie()</span></span><br><span class="line"><span class="comment"># obj.insert(word)</span></span><br><span class="line"><span class="comment"># param_2 = obj.search(word)</span></span><br><span class="line"><span class="comment"># param_3 = obj.startsWith(prefix)</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
        <category>数据结构</category>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>Trie</tag>
      </tags>
  </entry>
  <entry>
    <title>哈夫曼树、哈夫曼编码</title>
    <url>/2020/07/28/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91%E3%80%81%E5%93%88%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/wardseptember/article/details/80717653" target="_blank" rel="noopener">哈夫曼树及哈夫曼编码详解【完整版代码】</a></p>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>哈夫曼树</tag>
        <tag>哈夫曼编码</tag>
      </tags>
  </entry>
  <entry>
    <title>概率</title>
    <url>/2020/07/25/%E5%87%A0%E4%BD%95%E6%A6%82%E7%8E%87/</url>
    <content><![CDATA[<h1 id="1-几何概率"><a href="#1-几何概率" class="headerlink" title="1. 几何概率"></a>1. 几何概率</h1><h2 id="1-1-一根木棒，截成三截，组成三角形的概率是多少。"><a href="#1-1-一根木棒，截成三截，组成三角形的概率是多少。" class="headerlink" title="1.1 一根木棒，截成三截，组成三角形的概率是多少。"></a>1.1 一根木棒，截成三截，组成三角形的概率是多少。</h2><p>答案：0.25</p>
<p>画图求概率</p>
<p>$x+y&gt;1-x-y$     =&gt;   $2x+2y &gt; 1$</p>
<p>$x+1-x-y&gt;y$     =&gt;    $y &lt; 1/2$</p>
<p>$x-y<1-x-y$     =>    $x &lt; 1/2$</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>几何概率</tag>
      </tags>
  </entry>
  <entry>
    <title>滑动窗口</title>
    <url>/2020/07/23/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/</url>
    <content><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>滑动窗口用来解决一些查找满足一定条件的连续区间的性质（长度等）的问题。由于区间连续，因此当区间发生变化时，可以通过旧有的计算结果对搜索空间进行剪枝，这样便减少了重复计算，降低了时间复杂度。</p>
<h1 id="2-两种形式"><a href="#2-两种形式" class="headerlink" title="2. 两种形式"></a>2. 两种形式</h1><ul>
<li>非固定长度窗口—快慢指针；如满足某一条件的子数组的最大最小长度。</li>
<li>固定长度窗口—双端队列；如满足固定长度、某一条件的子数组中的最大最小值。</li>
</ul>
<h1 id="3-非固定长度窗口"><a href="#3-非固定长度窗口" class="headerlink" title="3. 非固定长度窗口"></a>3. 非固定长度窗口</h1><h2 id="3-1-解题框架"><a href="#3-1-解题框架" class="headerlink" title="3.1 解题框架"></a>3.1 解题框架</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left = <span class="number">0</span></span><br><span class="line">right = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">window = []</span><br><span class="line"><span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">	window.append(s[right])</span><br><span class="line">  right += <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> (windon needs shrink):</span><br><span class="line">    window.pop(<span class="number">0</span>)</span><br><span class="line">    left += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="3-2-长度最小的子数组"><a href="#3-2-长度最小的子数组" class="headerlink" title="3.2 长度最小的子数组"></a>3.2 长度最小的子数组</h2><p><a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">leetcode:209 长度最小的子数组</a></p>
<p>给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：s &#x3D; 7, nums &#x3D; [2,3,1,2,4,3]</span><br><span class="line">输出：2</span><br><span class="line">解释：子数组 [4,3] 是该条件下的长度最小的子数组。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minSubArrayLen</span><span class="params">(self, s: int, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        summation = <span class="number">0</span></span><br><span class="line">        res = len(nums) + <span class="number">1</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">while</span> right &lt; len(nums):</span><br><span class="line">            summation += nums[right]</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> summation &gt;= s:</span><br><span class="line">                res = min(res, right-left)</span><br><span class="line">                summation -= nums[left]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> res <span class="keyword">if</span> res != len(nums)+<span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="3-2-无重复字符的最长子串"><a href="#3-2-无重复字符的最长子串" class="headerlink" title="3.2 无重复字符的最长子串"></a>3.2 无重复字符的最长子串</h2><h4 id="Leetcode-3-无重复字符的最长子串"><a href="#Leetcode-3-无重复字符的最长子串" class="headerlink" title="Leetcode:3 无重复字符的最长子串"></a><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">Leetcode:3 无重复字符的最长子串</a></h4><p>给定一个字符串，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: &quot;abcabcbb&quot;</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: &quot;bbbbb&quot;</span><br><span class="line">输出: 1</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: &quot;pwwkew&quot;</span><br><span class="line">输出: 3</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。</span><br><span class="line">     请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        char2count = &#123;&#125;</span><br><span class="line">        <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">            char = s[right]</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> char2count:</span><br><span class="line">                char2count[char] = <span class="number">0</span></span><br><span class="line">            char2count[char] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> char2count[char] &gt; <span class="number">1</span>:</span><br><span class="line">                new_char = s[left]</span><br><span class="line">                char2count[new_char] -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            res = max(res, right-left)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="3-3-最大连续1的个数-III"><a href="#3-3-最大连续1的个数-III" class="headerlink" title="3.3 最大连续1的个数 III"></a>3.3 最大连续1的个数 III</h2><h4 id="Leetcode-1004-最大连续1的个数-III"><a href="#Leetcode-1004-最大连续1的个数-III" class="headerlink" title="Leetcode 1004. 最大连续1的个数 III"></a><a href="https://leetcode-cn.com/problems/max-consecutive-ones-iii/" target="_blank" rel="noopener">Leetcode 1004. 最大连续1的个数 III</a></h4><p>给定一个由若干 <code>0</code> 和 <code>1</code> 组成的数组 <code>A</code>，我们最多可以将 <code>K</code> 个值从 0 变成 1 。</p>
<p>返回仅包含 1 的最长（连续）子数组的长度。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入：A = [1,1,1,0,0,0,1,1,1,1,0], K = 2</span><br><span class="line">输出：6</span><br><span class="line">解释： </span><br><span class="line">[1,1,1,0,0,<span class="strong">**1**</span>,1,1,1,1,<span class="strong">**1**</span>]</span><br><span class="line">粗体数字从 0 翻转到 1，最长的子数组长度为 6。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestOnes</span><span class="params">(self, A: List[int], K: int)</span> -&gt; int:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> right &lt; len(A):</span><br><span class="line">            <span class="keyword">if</span> A[right] == <span class="number">0</span>:</span><br><span class="line">                K -= <span class="number">1</span></span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> K &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> A[left] == <span class="number">0</span>:</span><br><span class="line">                    K += <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            res = max(res, right-left)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="3-4-最小覆盖子串"><a href="#3-4-最小覆盖子串" class="headerlink" title="3.4 最小覆盖子串"></a>3.4 最小覆盖子串</h2><h4 id="Leetcode-76-最小覆盖子串"><a href="#Leetcode-76-最小覆盖子串" class="headerlink" title="Leetcode: 76. 最小覆盖子串"></a><a href="https://leetcode-cn.com/problems/minimum-window-substring/" target="_blank" rel="noopener">Leetcode: 76. 最小覆盖子串</a></h4><p>给你一个字符串 S、一个字符串 T，请在字符串 S 里面找出：包含 T 所有字符的最小子串。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: S = "ADOBECODEBANC", T = "ABC"</span><br><span class="line">输出: "BANC"</span><br></pre></td></tr></table></figure>
<p><strong>说明：</strong></p>
<ul>
<li>如果 S 中不存这样的子串，则返回空字符串 <code>&quot;&quot;</code>。</li>
<li>如果 S 中存在这样的子串，我们保证它是唯一的答案。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minWindow</span><span class="params">(self, s: str, t: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(needs, windows)</span>:</span></span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> needs.items():</span><br><span class="line">                <span class="keyword">if</span> windows[key] &lt; value:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        needs = defaultdict(int)</span><br><span class="line">        windows = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> t:</span><br><span class="line">            needs[char] += <span class="number">1</span></span><br><span class="line">            windows[char] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        res = len(s) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">            char = s[right]</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> windows:</span><br><span class="line">                windows[char] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> helper(needs, windows):</span><br><span class="line">                <span class="keyword">if</span> right-left &lt; res:</span><br><span class="line">                    start = left</span><br><span class="line">                    res = right-left</span><br><span class="line">                </span><br><span class="line">                char = s[left]</span><br><span class="line">                <span class="keyword">if</span> char <span class="keyword">in</span> windows:</span><br><span class="line">                    windows[char] -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span> <span class="keyword">if</span> res == len(s) + <span class="number">1</span> <span class="keyword">else</span> s[start:start+res]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findAnagrams</span><span class="params">(self, s: str, p: str)</span> -&gt; List[int]:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(needs, windows)</span>:</span></span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> needs.items():</span><br><span class="line">                <span class="keyword">if</span> windows[key] != value:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        needs = defaultdict(int)</span><br><span class="line">        windows = defaultdict(int)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> p:</span><br><span class="line">            needs[char] += <span class="number">1</span></span><br><span class="line">            windows[char] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span> </span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        res = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">            char = s[right]</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> windows:</span><br><span class="line">                windows[char] += <span class="number">1</span></span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> right - left == len(p):</span><br><span class="line">                <span class="keyword">if</span> check(needs, windows):</span><br><span class="line">                    res.append(left)</span><br><span class="line">                char = s[left]</span><br><span class="line">                <span class="keyword">if</span> char <span class="keyword">in</span> windows:</span><br><span class="line">                    windows[char] -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="3-5-至多包含两个不同字符的最长子串"><a href="#3-5-至多包含两个不同字符的最长子串" class="headerlink" title="3.5 至多包含两个不同字符的最长子串"></a>3.5 至多包含两个不同字符的最长子串</h2><p>给定一个字符串 s ，找出 至多 包含两个不同字符的最长子串 t 。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: "eceba"</span><br><span class="line">输出: 3</span><br><span class="line">解释: t 是 "ece"，长度为3。</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: "ccaabbb"</span><br><span class="line">输出: 5</span><br><span class="line">解释: t 是 "aabbb"，长度为5。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        char2count = &#123;&#125;</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        res = <span class="number">0</span> </span><br><span class="line">      </span><br><span class="line">        <span class="keyword">while</span> right &lt; len(s):</span><br><span class="line">            char = s[right]</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> char2count:</span><br><span class="line">                char2count[char] = <span class="number">0</span></span><br><span class="line">            char2count[char] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> char2count[char] == <span class="number">1</span>:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> count &gt; <span class="number">2</span>:</span><br><span class="line">                char = s[left]</span><br><span class="line">                char2count[char] -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> char2count[char] == <span class="number">0</span>:</span><br><span class="line">                    count -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            res = max(res, right-left)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="3-6-字符串的排列"><a href="#3-6-字符串的排列" class="headerlink" title="3.6  字符串的排列"></a>3.6  字符串的排列</h2><h4 id="Leetcode-567-字符串的排列"><a href="#Leetcode-567-字符串的排列" class="headerlink" title="Leetcode: 567. 字符串的排列"></a><a href="https://leetcode-cn.com/problems/permutation-in-string/" target="_blank" rel="noopener">Leetcode: 567. 字符串的排列</a></h4><p>给定两个字符串 <strong>s1</strong> 和 <strong>s2</strong>，写一个函数来判断 <strong>s2</strong> 是否包含 <strong>s1</strong> 的排列。</p>
<p>换句话说，第一个字符串的排列之一是第二个字符串的子串。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: s1 = "ab" s2 = "eidbaooo"</span><br><span class="line">输出: True</span><br><span class="line">解释: s2 包含 s1 的排列之一 ("ba").</span><br></pre></td></tr></table></figure>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: s1= "ab" s2 = "eidboaoo"</span><br><span class="line">输出: False</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checkInclusion</span><span class="params">(self, s1: str, s2: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(needs, windows)</span>:</span></span><br><span class="line">            <span class="keyword">for</span> key,value <span class="keyword">in</span> needs.items():</span><br><span class="line">                <span class="keyword">if</span> value != windows[key]:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        needs = defaultdict(int)</span><br><span class="line">        windows = defaultdict(int)     </span><br><span class="line">        res = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s1:</span><br><span class="line">            needs[c] += <span class="number">1</span></span><br><span class="line">            windows[c] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> right &lt; len(s2):</span><br><span class="line">            char = s2[right]</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">            windows[char] += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">while</span> right-left == len(s1):</span><br><span class="line">                <span class="keyword">if</span> check(needs, windows):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                char = s2[left]</span><br><span class="line">                windows[char] -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="3-7-串联所有单词的子串"><a href="#3-7-串联所有单词的子串" class="headerlink" title="3.7 串联所有单词的子串"></a>3.7 串联所有单词的子串</h2><h4 id="Leetcode-30-串联所有单词的子串"><a href="#Leetcode-30-串联所有单词的子串" class="headerlink" title="Leetcode 30. 串联所有单词的子串"></a><a href="https://leetcode-cn.com/problems/substring-with-concatenation-of-all-words/" target="_blank" rel="noopener">Leetcode 30. 串联所有单词的子串</a></h4><p>给定一个字符串 s 和一些长度相同的单词 words。找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。</p>
<p>注意子串要与 words 中的单词完全匹配，中间不能有其他字符，但不需要考虑 words 中单词串联的顺序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">  s &#x3D; &quot;barfoothefoobarman&quot;,</span><br><span class="line">  words &#x3D; [&quot;foo&quot;,&quot;bar&quot;]</span><br><span class="line">输出：[0,9]</span><br><span class="line">解释：</span><br><span class="line">从索引 0 和 9 开始的子串分别是 &quot;barfoo&quot; 和 &quot;foobar&quot; 。</span><br><span class="line">输出的顺序不重要, [9,0] 也是有效答案。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">  s &#x3D; &quot;wordgoodgoodgoodbestword&quot;,</span><br><span class="line">  words &#x3D; [&quot;word&quot;,&quot;good&quot;,&quot;best&quot;,&quot;word&quot;]</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSubstring</span><span class="params">(self, s: str, words: List[str])</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> words <span class="keyword">or</span> <span class="keyword">not</span> s: <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        one_word = len(words[<span class="number">0</span>])</span><br><span class="line">        word_num = len(words)</span><br><span class="line"></span><br><span class="line">        n = len(s)</span><br><span class="line">        words = Counter(words)</span><br><span class="line">        res = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, one_word):</span><br><span class="line">            cur_count = <span class="number">0</span></span><br><span class="line">            left = i</span><br><span class="line">            right = i</span><br><span class="line">            cur_Counter = Counter()</span><br><span class="line">            <span class="keyword">while</span> right + one_word &lt;= n:</span><br><span class="line">                w = s[right:right + one_word]</span><br><span class="line">                right += one_word</span><br><span class="line">                <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> words:</span><br><span class="line">                    left = right</span><br><span class="line">                    cur_Counter.clear()</span><br><span class="line">                    cur_count = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    cur_Counter[w] += <span class="number">1</span></span><br><span class="line">                    cur_count += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> cur_Counter[w] &gt; words[w]:</span><br><span class="line">                        left_w = s[left:left+one_word]</span><br><span class="line">                        left += one_word</span><br><span class="line">                        cur_Counter[left_w] -= <span class="number">1</span></span><br><span class="line">                        cur_count -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> cur_count == word_num:</span><br><span class="line">                        res.append(left)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="3-8-最小区间"><a href="#3-8-最小区间" class="headerlink" title="3.8 最小区间"></a>3.8 最小区间</h2><p>你有 k 个升序排列的整数数组。找到一个<strong>最小</strong>区间，使得 k 个列表中的每个列表至少有一个数包含在其中。</p>
<p>我们定义如果 b-a &lt; d-c 或者在 b-a == d-c 时 a &lt; c，则区间 [a,b] 比 [c,d] 小。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入:[[4,10,15,24,26], [0,9,12,20], [5,18,22,30]]</span><br><span class="line">输出: [20,24]</span><br><span class="line">解释:</span><br><span class="line">列表 1：[4, 10, 15, 24, 26]，24 在区间 [20,24] 中。</span><br><span class="line">列表 2：[0, 9, 12, 20]，20 在区间 [20,24] 中。</span><br><span class="line">列表 3：[5, 18, 22, 30]，22 在区间 [20,24] 中。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    def smallestRange(self, nums: List[List[int]]): -&gt; List[int]</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">        v  = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> nums[i]:</span><br><span class="line">                v.append((num, i))</span><br><span class="line">        v.sort()</span><br><span class="line"></span><br><span class="line">        left, right, n = <span class="number">0</span>, <span class="number">0</span>, len(v)</span><br><span class="line">        index2count = collections.defaultdict(int)</span><br><span class="line">        k = len(nums)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        res = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">        diff = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> right &lt; n:</span><br><span class="line">            index = v[right][<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> index2count[index] == <span class="number">0</span>:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            index2count[index] += <span class="number">1</span></span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> count == k:</span><br><span class="line">                <span class="keyword">if</span> v[right][<span class="number">0</span>] - v[left][<span class="number">0</span>] &lt; diff:</span><br><span class="line">                    diff = v[right][<span class="number">0</span>] - v[left][<span class="number">0</span>]</span><br><span class="line">                    res = [v[left][<span class="number">0</span>], v[right][<span class="number">0</span>]]</span><br><span class="line">                d[v[left][<span class="number">1</span>]] -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> d[v[left][<span class="number">1</span>]] == <span class="number">0</span>:</span><br><span class="line">                    count -= <span class="number">1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="4-固定窗口"><a href="#4-固定窗口" class="headerlink" title="4. 固定窗口"></a>4. 固定窗口</h2><h2 id="4-1-滑动窗口最大值"><a href="#4-1-滑动窗口最大值" class="headerlink" title="4.1 滑动窗口最大值"></a>4.1 滑动窗口最大值</h2><p><a href="https://leetcode-cn.com/problems/sliding-window-maximum/" target="_blank" rel="noopener">239. 滑动窗口最大值</a></p>
<p>给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。</p>
<p>返回滑动窗口中的最大值。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3</span><br><span class="line">输出: [3,3,5,5,6,7] </span><br><span class="line">解释: </span><br><span class="line"></span><br><span class="line">  滑动窗口的位置                最大值</span><br><span class="line">---------------               -----</span><br><span class="line">[1  3  -1] -3  5  3  6  7       3</span><br><span class="line"> 1 [3  -1  -3] 5  3  6  7       3</span><br><span class="line"> 1  3 [-1  -3  5] 3  6  7       5</span><br><span class="line"> 1  3  -1 [-3  5  3] 6  7       5</span><br><span class="line"> 1  3  -1  -3 [5  3  6] 7       6</span><br><span class="line"> 1  3  -1  -3  5 [3  6  7]      7</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">类似于单调队列的思想</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxSlidingWindow</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; List[int]:</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line">        queue = deque()  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># init queue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">            <span class="keyword">while</span> queue <span class="keyword">and</span> nums[queue[<span class="number">-1</span>]] &lt;= nums[i]: queue.pop()</span><br><span class="line">            queue.append(i)</span><br><span class="line">        </span><br><span class="line">        res = [nums[queue[<span class="number">0</span>]]]</span><br><span class="line">        <span class="comment"># slide the window</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k, len(nums)):</span><br><span class="line">            <span class="comment"># remove the start of the window</span></span><br><span class="line">            <span class="keyword">while</span> queue <span class="keyword">and</span> i - queue[<span class="number">0</span>] + <span class="number">1</span> &gt; k:</span><br><span class="line">                queue.popleft()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> queue <span class="keyword">and</span> nums[queue[<span class="number">-1</span>]] &lt;= nums[i]: queue.pop()</span><br><span class="line">            queue.append(i)</span><br><span class="line">            </span><br><span class="line">            res.append(nums[queue[<span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>滑动窗口</tag>
      </tags>
  </entry>
  <entry>
    <title>面试大杂烩</title>
    <url>/2020/07/21/%E9%9D%A2%E8%AF%95%E5%A4%A7%E6%9D%82%E7%83%A9/</url>
    <content><![CDATA[<p>偏差：描述模型输出结果的期望与样本真实结果的差距。<br>方差：描述模型对于给定值的输出稳定性。</p>
<p>如果模型不能很好地拟合训练数据，则模型具有高偏差（也称欠拟合 - Underfitting）；<br>反之，如果模型能够很好地拟合训练数据，但在测试数据上有较大的误差，则模型具有高方差（也称过拟合 - Overfitting）。</p>
<p><a href="https://zhuanlan.zhihu.com/p/57208761" target="_blank" rel="noopener">[机器学习]偏差和方差的理解</a></p>
<p>True Positive Rate（真阳率）、False Positive（伪阳率）</p>
<ul>
<li>TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。 </li>
<li>FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。</li>
</ul>
<p><a href="https://www.zhihu.com/question/39840928" target="_blank" rel="noopener">如何理解机器学习和统计中的AUC？</a></p>
]]></content>
  </entry>
  <entry>
    <title>朴素贝叶斯</title>
    <url>/2020/07/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h1 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h1><p>朴素贝叶斯法（Naive Bayes）：基于贝叶斯定理与特征条件独立的分类方法。</p>
<ul>
<li>对于给定的训练数据集，基于特征条件独立假设学习输入输出的联合概率分布；</li>
<li>基于联合概率分布，对给定的输入 $x$ ， 利用贝叶斯定理求出后验概率最大输出 $ y $ 。</li>
</ul>
<p> <br/></p>
<ul>
<li>条件概率公式  <script type="math/tex; mode=display">P(A|B) =  \frac{P(A, B)}{P(B)}</script><script type="math/tex; mode=display">P(B|A) =  \frac{P(A, B)}{P(A)}</script></li>
<li>贝叶斯定理   <script type="math/tex; mode=display">P(A|B) =  P(B|A)  * \frac{P(A)}{P(B)}</script></li>
</ul>
<h1 id="2-基本方法"><a href="#2-基本方法" class="headerlink" title="2 基本方法"></a>2 基本方法</h1><h2 id="联合概率分布-P-X-Y"><a href="#联合概率分布-P-X-Y" class="headerlink" title="联合概率分布 $P(X, Y)$"></a>联合概率分布 $P(X, Y)$</h2><ul>
<li>先验概率分布   <script type="math/tex; mode=display">P(Y = c_k)， k=1,2,...,K</script></li>
<li>条件概率分布<script type="math/tex; mode=display">P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k), k=1,2,...,K</script></li>
<li>条件概率分布加入强的条件独立假设<script type="math/tex; mode=display">
\begin{align}
P(X=x|Y=c_k)  &= P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)} | Y=c_k) \\
&= \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y=c_k) \\
\end{align}</script></li>
<li>朴素贝叶斯法实际上学习到生成数据的机制，属于生成模型。条件独立假设等于是说分类的特征在类确定的条件下都是条件独立的，这一假设使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。</li>
<li>预测时，计算后验概率分布，将后验概率最大的类作为 $x$ 的类输出<script type="math/tex; mode=display">
\begin{align} P(Y=c_k | X = x) &= \frac{ P (X=x |Y=c_k) * \frac{P(Y=c_k)}{P(X=x)}  }{ \sum_k  P (X=x | Y=c_k) * \frac{P(Y=c_k)}{P(X=x)} } \\
                                   &= \frac{ P (X=x | Y=c_k) * P(Y=c_k) }{ \sum_k  P (X=x | Y=c_k) * P(Y=c_k) }  \\
\end{align}</script></li>
</ul>
<ul>
<li>拉普拉斯平滑</li>
</ul>
<h1 id="3-代码示例"><a href="#3-代码示例" class="headerlink" title="3 代码示例"></a>3 代码示例</h1>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降法、牛顿法、拟牛顿法</title>
    <url>/2020/07/12/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[<h1 id="1-梯度下降法"><a href="#1-梯度下降法" class="headerlink" title="1 梯度下降法"></a>1 梯度下降法</h1><h2 id="1-1-目的"><a href="#1-1-目的" class="headerlink" title="1.1 目的"></a>1.1 目的</h2><p>求解无约束最优化问题的极小值</p>
<h2 id="1-2-方法"><a href="#1-2-方法" class="headerlink" title="1.2 方法"></a>1.2 方法</h2><p>通过不断的迭代，进行目标函数的极小化，直到收敛</p>
<h2 id="1-3-泰勒一阶展开"><a href="#1-3-泰勒一阶展开" class="headerlink" title="1.3 泰勒一阶展开"></a>1.3 泰勒一阶展开</h2><script type="math/tex; mode=display">f(x) = f(x_0) + (x-x_0) f'(x_0)</script><h2 id="1-4-思想"><a href="#1-4-思想" class="headerlink" title="1.4 思想"></a>1.4 思想</h2><p>极小化 $f(x)$ ，则 $x-x_0 = -f’(x_0) * \lambda$ ，其中 $\lambda &gt; 0 $</p>
<h2 id="1-5-结论"><a href="#1-5-结论" class="headerlink" title="1.5 结论"></a>1.5 结论</h2><p>$ x = x_0  - \lambda * f’(x_0)$ ，其中学习率 $\lambda &gt; 0 $</p>
<h2 id="1-6-应用"><a href="#1-6-应用" class="headerlink" title="1.6 应用"></a>1.6 应用</h2><ul>
<li>数学问题中，求解极小值</li>
<li>机器学习中，求解目标函数最优解</li>
</ul>
<h1 id="2-牛顿法"><a href="#2-牛顿法" class="headerlink" title="2 牛顿法"></a>2 牛顿法</h1><h2 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h2><p>牛顿法，又称牛顿迭代法，主要是迭代的方法用于求解方程的根。后应用于无约束问题的最优化。</p>
<h2 id="2-2-解方程"><a href="#2-2-解方程" class="headerlink" title="2.2 解方程"></a>2.2 解方程</h2><p>利用泰勒公式一阶展开，$f(x) = f(x_0) + (x-x_0) f’(x_0)$   </p>
<p>求解方程 $f(x) = 0$， 则  $f(x_0) + (x-x_0) f’(x_0) = 0$    </p>
<p>推出，$x = x_0  - \frac{f(x_0)}{f’(x_0)}$    </p>
<p>算法流程：  </p>
<p>不断迭代 $x_{k+1} = x_k - \frac{f(x_k)}{f’(x_k)}$，</p>
<p>直至 $f(x_{k+1}) = 0$ </p>
<p>举例如下：  </p>
<p>求解$\sqrt2$ ， 保留 5位小数<br>思路：设计 $y = f(x)$， 将$x = \sqrt2$ 代入，$y = 0$， 故 $f(x) = x^2 -2$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mySqrt</span><span class="params">(self, x: int)</span> -&gt; int:</span></span><br><span class="line">        num_pre = x</span><br><span class="line">        num_cur = (num_pre * num_pre + x) / (<span class="number">2</span> * num_pre)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> abs(num_pre-num_cur) &gt; <span class="number">0.0001</span>:</span><br><span class="line">            num_pre = num_cur</span><br><span class="line">            num_cur = (num_pre * num_pre + x) / (<span class="number">2</span> * num_pre)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> num_cur</span><br></pre></td></tr></table></figure>
<h2 id="2-3-求解无约束最优化问题"><a href="#2-3-求解无约束最优化问题" class="headerlink" title="2.3 求解无约束最优化问题"></a>2.3 求解无约束最优化问题</h2><p>利用泰勒公式二阶展开</p>
<script type="math/tex; mode=display">f(x) = f(x_0) + (x-x_0) f'(x_0) + \frac{1}{2} (x-x_0)^2 f''(x_0)</script><p>函数$f(x)$ 在 $x_0$有极值的必要条件是 $f’(x_0) = 0$</p>
<script type="math/tex; mode=display">0 = f'(x) = f'(x_0) + (x-x_0) f''(x_0)</script><script type="math/tex; mode=display">x = x_0 - \frac{ f'(x_0)}{ f''(x_0)}</script><h2 id="2-4-最优化问题应用至高维"><a href="#2-4-最优化问题应用至高维" class="headerlink" title="2.4 最优化问题应用至高维"></a>2.4 最优化问题应用至高维</h2><p>引入Hessian矩阵 </p>
<script type="math/tex; mode=display">H(x) = [\frac{\partial^2f}{\partial x_i \partial x_j }]_{n*n}</script><script type="math/tex; mode=display">x_{k+1} = x_k - \frac{g_k}{H_k} =  x_k - H_k^{-1} g_k</script><p>其中，$g_k$ 是一阶导（梯度），$H_k$ 是Hessian矩阵 </p>
<h1 id="3-拟牛顿法"><a href="#3-拟牛顿法" class="headerlink" title="3 拟牛顿法"></a>3 拟牛顿法</h1><p>避免每次都要计算Hessian矩阵，采用正定矩阵近似方法实现</p>
<h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h1><h2 id="4-1-速记"><a href="#4-1-速记" class="headerlink" title="4.1 速记"></a>4.1 速记</h2><ul>
<li>梯度下降时，迭代 $ x = x_0  - \lambda * f’(x_0)$ 直至收敛，其中学习率 $\lambda &gt; 0 $</li>
<li>牛顿法解方程时，迭代 $ x = x_0 - \frac{f(x_0)}{f’(x_0)} $，直至收敛</li>
<li>牛顿法最优化时，迭代 $ x = x_0 - \frac{ f’(x_0)}{ f’’(x_0)}$ 直至收敛</li>
</ul>
<h2 id="4-2-对比"><a href="#4-2-对比" class="headerlink" title="4.2 对比"></a>4.2 对比</h2><ul>
<li>梯度下降法靠近极小值，收敛速度慢；通过步长控制，极值点震荡</li>
<li>牛顿法是二阶收敛，梯度下降法是一阶收敛，牛顿法收敛速度快</li>
<li>牛顿法容易陷入局部最优</li>
<li>牛顿法要同时求出梯度和Hessian矩阵，计算量大且不好计算；当输入向量维度$N$较大时，Hessian矩阵的大小时 $N*N$，需要内存和显存非常大</li>
</ul>
]]></content>
      <categories>
        <category>最优化</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>牛顿法</tag>
        <tag>拟牛顿法</tag>
      </tags>
  </entry>
  <entry>
    <title>最长上升子序列</title>
    <url>/2020/07/11/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h1 id="1-最长上升子序列"><a href="#1-最长上升子序列" class="headerlink" title="1. 最长上升子序列"></a>1. 最长上升子序列</h1><p>Leetcode 300:  <a href="https://leetcode-cn.com/problems/longest-increasing-subsequence/" target="_blank" rel="noopener">最长上升子序列</a></p>
<p>给定一个无序的整数数组，找到其中最长上升子序列的长度。</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: [10,9,2,5,3,7,101,18]</span><br><span class="line">输出: 4 </span><br><span class="line">解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。</span><br></pre></td></tr></table></figure>
<h2 id="1-1-动态规划解法"><a href="#1-1-动态规划解法" class="headerlink" title="1.1 动态规划解法"></a>1.1 动态规划解法</h2><ul>
<li>dp[i] :  以i为结尾的最长上升子序列</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        dp = [<span class="number">1</span>] * len(nums)</span><br><span class="line">        res = <span class="number">1</span> </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                <span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line">                    dp[i] = max(dp[i], dp[j]+<span class="number">1</span>)</span><br><span class="line">                res = max(res, dp[i])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="1-2-二分查找"><a href="#1-2-二分查找" class="headerlink" title="1.2 二分查找"></a>1.2 二分查找</h2><p>基于贪心的思想，并结合二分查找解题。</p>
<p>新建数组 res，用于保存最长上升子序列。对原序列进行遍历，将每位元素二分插入 cell 中。</p>
<ul>
<li>如果 res 中元素都比它小，将它插到最后</li>
<li>否则，用它覆盖掉比它大的元素中最小元素</li>
</ul>
<p>总之，思想就是让 res 中存储比较小的元素。这样，res 未必是真实的最长上升子序列，但长度是对的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">from</span> bisect <span class="keyword">import</span> bisect_left</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            index = bisect_left(res, num)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> index == len(res):</span><br><span class="line">                res.append(num)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[index] = num</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">return</span> len(res)</span><br></pre></td></tr></table></figure>
<h1 id="2-俄罗斯套娃娃信封问题"><a href="#2-俄罗斯套娃娃信封问题" class="headerlink" title="2. 俄罗斯套娃娃信封问题"></a>2. 俄罗斯套娃娃信封问题</h1><p>Leetcode 354 <a href="https://leetcode-cn.com/problems/russian-doll-envelopes/" target="_blank" rel="noopener">俄罗斯套娃娃信封问题</a></p>
<p>给定一些标记了宽度和高度的信封，宽度和高度以整数对形式 (w, h) 出现。当另一个信封的宽度和高度都比这个信封大的时候，这个信封就可以放进另一个信封里，如同俄罗斯套娃一样。</p>
<p>请计算最多能有多少个信封能组成一组“俄罗斯套娃”信封（即可以把一个信封放到另一个信封里面）。</p>
<p>说明:<br>不允许旋转信封。</p>
<p>示例:</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">输入: envelopes = [[5,4],[6,4],[6,7],[2,3]]</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 最多信封的个数为 3, 组合为: [2,3] =&gt; [5,4] =&gt; [6,7]。</span><br></pre></td></tr></table></figure>
<p>思路：</p>
<ul>
<li>按照宽度升序、高度降序给信封排序</li>
<li>此时宽度已经是升序状态，可以将问题理解为乱序的高度的最长上升子序列问题。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxEnvelopes</span><span class="params">(self, envelopes: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        envelopes.sort(key=<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], -x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">from</span> bisect <span class="keyword">import</span> bisect_left</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> _, num <span class="keyword">in</span> envelopes:</span><br><span class="line">            index = bisect_left(res, num)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> index == len(res):</span><br><span class="line">                res.append(num)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[index] = num</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> len(res)</span><br></pre></td></tr></table></figure>
<h1 id="3-俄罗斯套娃娃信封问题发散"><a href="#3-俄罗斯套娃娃信封问题发散" class="headerlink" title="3 俄罗斯套娃娃信封问题发散"></a>3 俄罗斯套娃娃信封问题发散</h1><p>假如存在两封信封的宽度相同，且同样宽度也可以塞进去（即大信封的宽度和高度都不小于小信封的高度和宽度），怎么处理？</p>
<p>思路：</p>
<ul>
<li>按照宽度升序、高度升序给信封排序</li>
<li>此时宽度已经是升序状态，可以将问题理解为乱序的高度的最长上升子序列问题。</li>
<li>注意，这个题意下不需要给高度做降序操作</li>
</ul>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>最长上升子序列</tag>
        <tag>俄罗斯套娃娃</tag>
      </tags>
  </entry>
  <entry>
    <title>二分法常见题型</title>
    <url>/2020/07/11/%E4%BA%8C%E5%88%86%E6%B3%95%E5%B8%B8%E8%A7%81%E9%A2%98%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="排序数组中查找"><a href="#排序数组中查找" class="headerlink" title="排序数组中查找"></a>排序数组中查找</h1><h1 id="隐含的最大最小值，其中寻找答案"><a href="#隐含的最大最小值，其中寻找答案" class="headerlink" title="隐含的最大最小值，其中寻找答案"></a>隐含的最大最小值，其中寻找答案</h1>]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>手撕代码</tag>
        <tag>二分法</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀和、二维前缀和、差分</title>
    <url>/2020/07/10/%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%81%E4%BA%8C%E7%BB%B4%E5%89%8D%E7%BC%80%E5%92%8C%E3%80%81%E5%B7%AE%E5%88%86/</url>
    <content><![CDATA[<h2 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h2><h2 id="二维前缀和"><a href="#二维前缀和" class="headerlink" title="二维前缀和"></a>二维前缀和</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/-Ackerman/p/11162651.html" target="_blank" rel="noopener">前缀和</a></p>
]]></content>
      <categories>
        <category>手撕代码</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>前缀和</tag>
        <tag>二维前缀和</tag>
        <tag>差分</tag>
      </tags>
  </entry>
  <entry>
    <title>条件随机场（Conditional Random Field, CRF）</title>
    <url>/2020/07/10/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%EF%BC%88Conditional-Random-Field-CRF%EF%BC%89/</url>
    <content><![CDATA[<h1 id="0-引言"><a href="#0-引言" class="headerlink" title="0 引言"></a>0 引言</h1><p>条件随机场（Conditional Random Field，CRF）是给定一组输入随机变量条件下另一组输出变量的条件概率分布模型，其特点是输出随机变量构成了马尔可夫随机场。</p>
<h1 id="1-概率无向图模型"><a href="#1-概率无向图模型" class="headerlink" title="1 概率无向图模型"></a>1 概率无向图模型</h1><pre><code>引言：概率无向图模型，又称为马尔可夫随机场，是一个可以由无向图
表示的联合概率分布。
</code></pre><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>设有联合概率分布 $P(Y)$ , 由无向图 $G=(V,E)$ 表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。 如果联合概率分布 $P(Y)$   满足成对、局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型，或马尔可夫随机场（Markov Random Field）。</p>
<ul>
<li>成对马尔可夫性：</li>
<li>局部马尔可夫性：</li>
<li>全局马尔可夫性：</li>
</ul>
<h2 id="1-2-概率无线图模型的因子分解"><a href="#1-2-概率无线图模型的因子分解" class="headerlink" title="1.2 概率无线图模型的因子分解"></a>1.2 概率无线图模型的因子分解</h2><h3 id="1-2-1-团与最大团"><a href="#1-2-1-团与最大团" class="headerlink" title="1.2.1 团与最大团"></a>1.2.1 团与最大团</h3><h3 id="1-2-2-因子分解"><a href="#1-2-2-因子分解" class="headerlink" title="1.2.2 因子分解"></a>1.2.2 因子分解</h3><h2 id="2-条件随机场的定义与形式"><a href="#2-条件随机场的定义与形式" class="headerlink" title="2 条件随机场的定义与形式"></a>2 条件随机场的定义与形式</h2><h2 id="2-1-条件随机场的定义"><a href="#2-1-条件随机场的定义" class="headerlink" title="2.1 条件随机场的定义"></a>2.1 条件随机场的定义</h2><h3 id="2-1-1-整体架构"><a href="#2-1-1-整体架构" class="headerlink" title="2.1.1 整体架构"></a>2.1.1 整体架构</h3><ul>
<li>条件随机场是给定随机变量 $X$ 条件下，随机变量 $Y$ 的马尔可夫随机场。</li>
<li>线性链条件随机场可用于标注问题。这时，在条件概率模型 $P(Y|X)$ 中，$Y$ 是输出变量，表示标记序列，$X$ 是输入变量，表示需要标注的观测序列，也把标记序列称为状态序列。</li>
<li>学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型 $P\hat (Y|X)$ 。</li>
<li>预测时，对于给定的输入序列 $x$，求出条件概率 $P\hat(y|x)$ 最大的输出序列 $\hat{y} $ </li>
</ul>
<h3 id="2-1-2-条件随机场的定义"><a href="#2-1-2-条件随机场的定义" class="headerlink" title="2.1.2 条件随机场的定义"></a>2.1.2 条件随机场的定义</h3><h3 id="2-1-3-线性链条件随机场的定义"><a href="#2-1-3-线性链条件随机场的定义" class="headerlink" title="2.1.3 线性链条件随机场的定义"></a>2.1.3 线性链条件随机场的定义</h3><h2 id="2-2-条件随机场的参数化形式"><a href="#2-2-条件随机场的参数化形式" class="headerlink" title="2.2 条件随机场的参数化形式"></a>2.2 条件随机场的参数化形式</h2><h2 id="2-3-条件随机场的简化形式"><a href="#2-3-条件随机场的简化形式" class="headerlink" title="2.3 条件随机场的简化形式"></a>2.3 条件随机场的简化形式</h2><h2 id="2-4-条件随机场的矩阵形式"><a href="#2-4-条件随机场的矩阵形式" class="headerlink" title="2.4 条件随机场的矩阵形式"></a>2.4 条件随机场的矩阵形式</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>条件随机场</tag>
        <tag>CRF</tag>
        <tag>Conditional Random Field</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title>隐马尔可夫模型（Hidden Markov Model, HMM）</title>
    <url>/2020/07/08/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%88Hidden-Markov-Model-HMM%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-隐马尔可夫模型的基本概念"><a href="#1-隐马尔可夫模型的基本概念" class="headerlink" title="1. 隐马尔可夫模型的基本概念"></a>1. 隐马尔可夫模型的基本概念</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><p>隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测从而产生观测随机序列的过程。   </p>
<ul>
<li>马尔可夫链：隐藏的马尔可夫链随机生成的状态的序列</li>
<li>每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列</li>
</ul>
<h2 id="1-2-三要素"><a href="#1-2-三要素" class="headerlink" title="1.2 三要素"></a>1.2 三要素</h2><p>隐马尔可夫模型由初始状态概率向量  $\pi$ 、状态转移概率矩阵 $A$ 和 观测概率矩阵 $B$ 决定。 $ \pi $ 和 $A$ 决定状态序列，$B$ 决定观测序列。</p>
<script type="math/tex; mode=display">\lambda = (A, B, \pi)</script><h2 id="1-3-两个基本假设"><a href="#1-3-两个基本假设" class="headerlink" title="1.3 两个基本假设"></a>1.3 两个基本假设</h2><ul>
<li>齐次马尔可夫性假设：假设隐藏的马尔可夫链在任意时刻 $t$ 的状态只依赖于其前一时刻的状态，与其他时刻的状态集观测无关，也与时刻 $t$ 无关</li>
<li>观测独立性假设：假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关</li>
</ul>
<h2 id="1-4-隐马尔可夫模型的3个基本问题"><a href="#1-4-隐马尔可夫模型的3个基本问题" class="headerlink" title="1.4 隐马尔可夫模型的3个基本问题"></a>1.4 隐马尔可夫模型的3个基本问题</h2><ul>
<li>概率计算问题。 给定模型 $\lambda=(A,B,\pi)$ 和观测序列$O=(o_1, o_2, … , o_T)$ ，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率  $P(O|\lambda)$</li>
<li>学习问题。已知观测序列 $O=(o_1, o_2, … , o_T)$ ，估计模型 $ \lambda = (A, B, \pi)$ 参数，使得在该模型下观测序列概率 $P(O|\lambda)$ 最大。即用最大似然估计的方法估计参数。</li>
<li>预测问题，也称为解码问题。已知模型 $\lambda=(A,B,\pi) $ 和观测序列$O=(o_1, o_2, … , o_T)$ ，求对给定观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1, i_2, …, i_T)$ 。即给定观测序列，求最有可能的对应的状态序列。</li>
</ul>
<h1 id="2-概率计算算法"><a href="#2-概率计算算法" class="headerlink" title="2. 概率计算算法"></a>2. 概率计算算法</h1><h2 id="2-1-直接计算法"><a href="#2-1-直接计算法" class="headerlink" title="2.1 直接计算法"></a>2.1 直接计算法</h2><p>给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1, o_2, … , o_T)$ ，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率  $P(O|\lambda)$</p>
<p>通过列举所以可能的长度为 $T$ 的状态序列 $I=(i_1, i_2, …, i_T)$， 求各个状态序列 $I$ 与观测序列 $O=(o_1, o_2, … , o_T)$ 的联合概率 $ P(O, I | \lambda)$，然后对所有可能的状态序列求和，得到  $P(O|\lambda)$</p>
<ul>
<li>时间复杂度 $O(TN^T)$</li>
</ul>
<h2 id="2-2-前向算法"><a href="#2-2-前向算法" class="headerlink" title="2.2 前向算法"></a>2.2 前向算法</h2><ul>
<li><p>前向概率：给定隐马尔可夫模型 $\lambda $，定义到时刻 $t$ 部分观测序列为 $o_1, o_2, … , o_t $ 且状态为 $q_i$ 的概率为前向概率，记作</p>
<script type="math/tex; mode=display">\alpha_t(i) = P( o_1, o_2, ... , o_t , i_t = q_i | \lambda)</script></li>
<li><p>结合动态规划递推求解 $ \alpha_t(i) $ 即观测序列概率 $P(O|\lambda)$</p>
</li>
<li>时间复杂度 $O(TN^2)$</li>
</ul>
<h2 id="2-3-后向算法"><a href="#2-3-后向算法" class="headerlink" title="2.3 后向算法"></a>2.3 后向算法</h2><ul>
<li><p>后向概率：给定隐马尔可夫模型 $\lambda $，定义在时刻 $t$ 状态为  $q<em>i$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列为 $o</em>{t+1}, o_{t+2}, … , o_T $ 的概率为后向概率，记作</p>
<script type="math/tex; mode=display">\beta_t(i) = P( o_{t+1}, o_{t+2}, ... , o_T, i_t = q_i | \lambda)</script></li>
<li><p>结合动态规划递推求解 $ \beta_t(i) $ 即观测序列概率 $P(O|\lambda)$</p>
</li>
<li>时间复杂度 $O(TN^2)$</li>
</ul>
<h1 id="3-学习算法"><a href="#3-学习算法" class="headerlink" title="3. 学习算法"></a>3. 学习算法</h1><h2 id="3-1-监督学习方法"><a href="#3-1-监督学习方法" class="headerlink" title="3.1 监督学习方法"></a>3.1 监督学习方法</h2><p>状态数据和观测序列都存在的训练数据，利用极大似然估计发来估计隐马尔可夫模型的参数</p>
<ul>
<li>转移概率</li>
<li>观测概率</li>
<li>初始状态概率</li>
</ul>
<h2 id="3-2-Baum-Welch算法"><a href="#3-2-Baum-Welch算法" class="headerlink" title="3.2 Baum-Welch算法"></a>3.2 Baum-Welch算法</h2><p>Todo</p>
<h1 id="4-预测算法"><a href="#4-预测算法" class="headerlink" title="4. 预测算法"></a>4. 预测算法</h1><h2 id="4-1-近似算法"><a href="#4-1-近似算法" class="headerlink" title="4.1 近似算法"></a>4.1 近似算法</h2><p>贪心思想，在每个时刻 $t$ 选择该时刻最有可能出现的状态，优点是计算简单，缺点是不能保证预测的状态序列整体是最优可能的状态序列。</p>
<h2 id="4-2-维特比算法"><a href="#4-2-维特比算法" class="headerlink" title="4.2 维特比算法"></a>4.2 维特比算法</h2><p>用动态规划求概率最大路径（最优路径），这时一条路径对应着一个状态序列。</p>
<p>下一步规划：补充EM算法，CRF </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title>并查集</title>
    <url>/2020/06/16/%E5%B9%B6%E6%9F%A5%E9%9B%86/</url>
    <content><![CDATA[<h1 id="1-并查集概念"><a href="#1-并查集概念" class="headerlink" title="1.并查集概念"></a>1.并查集概念</h1><p>并查集，在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要反复查找一个元素在哪个集合中。</p>
<h1 id="2-实现方法"><a href="#2-实现方法" class="headerlink" title="2. 实现方法"></a>2. 实现方法</h1><h2 id="2-1-API"><a href="#2-1-API" class="headerlink" title="2.1 API"></a>2.1 API</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UF</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.parents = []</span><br><span class="line">        self.ranks = []</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_connected</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-具体实现"><a href="#2-2-具体实现" class="headerlink" title="2.2 具体实现"></a>2.2 具体实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UF</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.parents = []</span><br><span class="line">        self.ranks = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 尾递归</span></span><br><span class="line">        <span class="keyword">if</span> x != self.parents[x]:</span><br><span class="line">            self.parents[x] = self.find(self.parents[x])</span><br><span class="line">        <span class="keyword">return</span> self.parents[x]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_connected</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.find(x) == self.find(y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">union</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        parent_x = self.find(x)</span><br><span class="line">        parent_y = self.find(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入ranks 防止树过高</span></span><br><span class="line">        rank_x = self.ranks[parent_x]</span><br><span class="line">        rank_y = self.ranks[parent_y]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rank_x == rank_y:</span><br><span class="line">            self.parents[parent_y] = parent_x</span><br><span class="line">            self.ranks[parent_x] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> rank_x &gt; rank_y:</span><br><span class="line">            self.parents[parent_y] = parent_x</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.parents[parent_x] = parent_y</span><br></pre></td></tr></table></figure>
<h2 id="2-3-Todo"><a href="#2-3-Todo" class="headerlink" title="2.3 Todo"></a>2.3 Todo</h2>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title>预训练语言模型</title>
    <url>/2020/04/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="0-引言"><a href="#0-引言" class="headerlink" title="0. 引言"></a>0. 引言</h1><p>Todo</p>
<h1 id="1-word2vec"><a href="#1-word2vec" class="headerlink" title="1. word2vec"></a>1. word2vec</h1><p>Todo</p>
<h1 id="2-glove"><a href="#2-glove" class="headerlink" title="2. glove"></a>2. glove</h1><p>Todo</p>
<h1 id="3-elmo"><a href="#3-elmo" class="headerlink" title="3. elmo"></a>3. elmo</h1><p>Todo</p>
<h1 id="4-gpt"><a href="#4-gpt" class="headerlink" title="4. gpt"></a>4. gpt</h1><p>Todo</p>
<h1 id="5-gpt2"><a href="#5-gpt2" class="headerlink" title="5. gpt2"></a>5. gpt2</h1><p>Todo</p>
<h1 id="6-bert"><a href="#6-bert" class="headerlink" title="6. bert"></a>6. bert</h1><p>Todo</p>
<h1 id="7-roberta"><a href="#7-roberta" class="headerlink" title="7. roberta"></a>7. roberta</h1><h2 id="7-1-静态Masking-vs-动态Masking"><a href="#7-1-静态Masking-vs-动态Masking" class="headerlink" title="7.1 静态Masking vs 动态Masking"></a>7.1 静态Masking vs 动态Masking</h2><ul>
<li>静态Masking：Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。</li>
<li>动态Masking：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。</li>
</ul>
<h2 id="7-2-with-NSP-vs-without-NSP"><a href="#7-2-with-NSP-vs-without-NSP" class="headerlink" title="7.2  with NSP vs without NSP"></a>7.2  with NSP vs without NSP</h2><p>原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。</p>
<h3 id="7-3-更大的mini-batch"><a href="#7-3-更大的mini-batch" class="headerlink" title="7.3  更大的mini-batch"></a>7.3  更大的mini-batch</h3><p>原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。</p>
<h3 id="7-4-更多的数据，更长时间的训练"><a href="#7-4-更多的数据，更长时间的训练" class="headerlink" title="7.4  更多的数据，更长时间的训练"></a>7.4  更多的数据，更长时间的训练</h3><p>RoBERTa用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。</p>
<h1 id="8-Albert"><a href="#8-Albert" class="headerlink" title="8. Albert"></a>8. Albert</h1><h3 id="8-1-对Embedding因式分解"><a href="#8-1-对Embedding因式分解" class="headerlink" title="8.1 对Embedding因式分解"></a>8.1 对Embedding因式分解</h3><p>ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的 embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从$O(V×H)O(V×H)O(V×H)$降低到了$O(V×E+E×H)O(V×E+E×H)O(V×E+E×H) $，当 $E&lt;&lt;H$时参数量减少的很明显。</p>
<h3 id="8-2-跨层的参数共享（Cross-layer-parameter-sharing）"><a href="#8-2-跨层的参数共享（Cross-layer-parameter-sharing）" class="headerlink" title="8.2 跨层的参数共享（Cross-layer parameter sharing）"></a>8.2 跨层的参数共享（Cross-layer parameter sharing）</h3><p>全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。</p>
<h3 id="8-3-句间连贯（Inter-sentence-coherence-loss）"><a href="#8-3-句间连贯（Inter-sentence-coherence-loss）" class="headerlink" title="8.3 句间连贯（Inter-sentence coherence loss）"></a>8.3 句间连贯（Inter-sentence coherence loss）</h3><p>在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。</p>
<h3 id="8-4-移除dropout"><a href="#8-4-移除dropout" class="headerlink" title="8.4 移除dropout"></a>8.4 移除dropout</h3><p>ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</p>
<h1 id="9-DistillBert"><a href="#9-DistillBert" class="headerlink" title="9. DistillBert"></a>9. DistillBert</h1><p>Todo</p>
<h1 id="10-xlnet"><a href="#10-xlnet" class="headerlink" title="10. xlnet"></a>10. xlnet</h1><ul>
<li><p>自回归 vs 自编码，通过全排列及mask实现</p>
</li>
<li><p>Bert输入存在mask噪声，输出条件独立，缺乏生成能力</p>
</li>
<li>双流Attention，自己预测自己，自己又不能看见自己</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/eddf04ba8545" target="_blank" rel="noopener">改进版的RoBERTa到底改进了什么？</a></li>
<li><a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">一文揭开ALBERT的神秘面纱</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>glove</tag>
        <tag>elmo</tag>
        <tag>gpt</tag>
        <tag>gpt2</tag>
        <tag>bert</tag>
        <tag>roberta</tag>
        <tag>distillBert</tag>
        <tag>albert</tag>
        <tag>xlnet</tag>
      </tags>
  </entry>
  <entry>
    <title>focal loss</title>
    <url>/2020/03/31/focal-loss/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2020/03/29/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="速记"><a href="#速记" class="headerlink" title="速记"></a>速记</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">时间复杂度</th>
<th style="text-align:center">稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">选择排序</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">希尔排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">不稳定</td>
</tr>
<tr>
<td style="text-align:center">基数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">桶排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
<tr>
<td style="text-align:center">计数排序</td>
<td style="text-align:center"></td>
<td style="text-align:center">稳定</td>
</tr>
</tbody>
</table>
</div>
<p>稳定性记忆方法：高复杂度（ $O(n^2)$ ）选择排序是例外，低复杂度（ $O(nlogn)$ ）归并排序是例外。<br><br>选择排序不稳定举例：(7) 2 5 9 3 4 [7] 1…当我们利用直接选择排序算法进行排序时候,(7)和1调换,(7)就跑到了[7]的后面了,原来的次序改变了,这样就不稳定了.</p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""快速排序"""</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    mid = array[start]  </span><br><span class="line">    low = start  </span><br><span class="line">    high = end </span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        array[low] = array[high]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        array[high] = array[low]</span><br><span class="line"></span><br><span class="line">    array[low] = mid  </span><br><span class="line"></span><br><span class="line">    quick_sort(alist, start, low - <span class="number">1</span>)  </span><br><span class="line">    quick_sort(alist, low + <span class="number">1</span>, end)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">quick_sort(array, <span class="number">0</span>, len(array) - <span class="number">1</span>)          <span class="comment"># 闭区间 [0, len(array) -1]</span></span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(array) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    mid = len(array) // <span class="number">2</span></span><br><span class="line">    left_array = merge_sort(array[:mid])</span><br><span class="line">    right_array = merge_sort(array[mid:])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> merge(left_array, right_array)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left_array, right_array)</span>:</span></span><br><span class="line">    array = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array) <span class="keyword">and</span> j &lt; len(right_array):</span><br><span class="line">        <span class="keyword">if</span> left_array[i] &lt;= right_array[j]:</span><br><span class="line">            array.append(left_array[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            array.append(right_array[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left_array):</span><br><span class="line">        array.append(left_array[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; len(right_array):</span><br><span class="line">        array.append(right_array[j])</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = merge_sort(array)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="topK问题"><a href="#topK问题" class="headerlink" title="topK问题"></a>topK问题</h2><h3 id="暴力解法"><a href="#暴力解法" class="headerlink" title="暴力解法"></a>暴力解法</h3><p>最基本的思路，将N个数进行完全排序，从中选出排在前K的元素即为所求。有了这个思路，我们可以选择相应的排序算法进行处理，目前来看快速排序，堆排序和归并排序都能达到$O(NlogN)$的时间复杂度。</p>
<h3 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h3><p>可以采用数据池的思想，选择其中前K个数作为数据池，后面的N-K个数与这K个数进行比较，若小于其中的任何一个数，则进行替换。这种思路的算法复杂度是$O(N*K)$</p>
<h3 id="大根堆"><a href="#大根堆" class="headerlink" title="大根堆"></a>大根堆</h3><p>大根堆维护一个大小为K的数组，目前该大根堆中的元素是排名前K的数，其中根是最大的数。此后，每次从原数组中取一个元素与根进行比较，如小于根的元素，则将根元素替换并进行堆调整（下沉），即保证大根堆中的元素仍然是排名前K的数，且根元素仍然最大；否则不予处理，取下一个数组元素继续该过程。该算法的时间复杂度是O(N*logK)，一般来说企业中都采用该策略处理topK问题，因为该算法不需要一次将原数组中的内容全部加载到内存中，而这正是海量数据处理必然会面临的一个关卡。</p>
<h3 id="快速排序-1"><a href="#快速排序-1" class="headerlink" title="快速排序"></a>快速排序</h3><p>利用快速排序的分划函数找到分划位置K，则其前面的内容即为所求。该算法是一种非常有效的处理方式，时间复杂度是O(N)（证明可以参考算法导论书籍）。对于能一次加载到内存中的数组，该策略非常优秀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topK</span><span class="params">(array, K)</span>:</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = len(array) - <span class="number">1</span></span><br><span class="line">    index = quick_sort(array, left, right)</span><br><span class="line">    <span class="keyword">while</span> index+<span class="number">1</span> != K:</span><br><span class="line">        <span class="keyword">if</span> index+<span class="number">1</span> &lt; K:</span><br><span class="line">            index = quick_sort(array, index+<span class="number">1</span>, right)</span><br><span class="line">        <span class="keyword">elif</span> index+<span class="number">1</span> &gt; K:</span><br><span class="line">            index = quick_sort(array, left, index<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> array[:K]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array, left, right)</span>:</span></span><br><span class="line">    <span class="comment"># print(left, right)</span></span><br><span class="line">    mid = array[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[right] &lt; mid:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        array[left] = array[right]</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> array[left] &gt;= mid:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        array[right] = array[left]</span><br><span class="line">    array[left] = mid</span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test</span></span><br><span class="line">array = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">array = topK(array, K=<span class="number">8</span>)          </span><br><span class="line">print(array)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://www.jianshu.com/p/5b8f00d6a9d7" target="_blank" rel="noopener">topK算法问题</a></li>
<li><a href="https://blog.csdn.net/zyq522376829/article/details/47686867?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">海量数据处理 - 10亿个数中找出最大的10000个数（top K问题）</a></li>
</ol>
]]></content>
      <categories>
        <category>手撕代码</category>
      </categories>
      <tags>
        <tag>快排排序</tag>
        <tag>归并排序</tag>
        <tag>topK</tag>
      </tags>
  </entry>
  <entry>
    <title>蓄水池抽样算法</title>
    <url>/2020/03/17/%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择一个数据？<br></li>
<li>解法：我们以概率1选择第一个数据，以1/2的概率选择第二个数据，以此类推，以1/m的概率选择第m个对象（如果后面某一数据一旦选中，替换掉以前选中的数据）。当所有数据流过时，每个对象具有相同的被选中的概率1/n。<br></li>
<li>证明：<script type="math/tex">P_m = \frac{1}{m} \cdot \frac{m}{m+1} \cdot \frac{m+1}{m+2} \cdot *** \cdot \frac{n-2}{n-1} \cdot \frac{n-1}{n} = \frac{1}{n}</script></li>
<li><p>Python代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n)</span>:</span></span><br><span class="line">    res = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        num = random.randint(<span class="number">1</span>, i):</span><br><span class="line">        <span class="keyword">if</span> i == num:</span><br><span class="line">            res = i</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="蓄水池抽样"><a href="#蓄水池抽样" class="headerlink" title="蓄水池抽样"></a>蓄水池抽样</h2><ul>
<li>问题：流式数据（Streaming Data），数据长度为n但不知道，如何从中等概率随机选择k（k &lt; n）个数据？</li>
<li>解法：先选中前k个数据（当作一个蓄水池），然后以k/(k+1)的概率选择第k+1个数据，以k/(k+2)的概率选择第k+2个数据，以此类推以k/m的概率选择第m个数据，一旦后面有某个数据被选中，则随机替换蓄水池中的一个数据。最终每个数据被选中的概率均为k/n。</li>
<li>证明：<script type="math/tex; mode=display">
\begin{align}
P_m &= \frac{k}{m}  (\frac{m+1-k}{m+1} +\frac{k}{m+1} \frac{k-1}{k})  (\frac{m+2-k}{m+2} +\frac{k}{m+2}\frac{k-1}{k}) ...(\frac{n-k}{n} +\frac{k}{n}\frac{k-1}{k})  \\
 &= \frac{k}{n}
\end{align}</script></li>
<li>Python代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(n, k)</span>:</span></span><br><span class="line">    res = [<span class="number">0</span>] * k</span><br><span class="line">    <span class="keyword">for</span> i range(k):</span><br><span class="line">        res[i] = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i range(k, n):</span><br><span class="line">        num = random.randint(<span class="number">0</span>, i)</span><br><span class="line">        <span class="keyword">if</span> num &lt; k:</span><br><span class="line">            res[num]  = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/87bc058b762e" target="_blank" rel="noopener">蓄水池抽样</a></li>
<li><a href="https://www.jianshu.com/p/7a9ea6ece2af" target="_blank" rel="noopener">蓄水池抽样算法(Reservoir Sampling)</a></li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>蓄水池抽样</tag>
      </tags>
  </entry>
  <entry>
    <title>面试记录</title>
    <url>/2020/03/13/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="阿里小蜜部门1-简历评估面试-35min-已凉"><a href="#阿里小蜜部门1-简历评估面试-35min-已凉" class="headerlink" title="阿里小蜜部门1-简历评估面试 35min 已凉"></a>阿里小蜜部门1-简历评估面试 35min 已凉</h2><ol>
<li>自我介绍</li>
<li>介绍项目</li>
<li>Bert层数那么高为啥还能收敛？</li>
<li>文本匹配的方法可以分为哪几类？各自优缺点</li>
<li>介绍TextCNN</li>
<li>介绍LSTM</li>
<li>梯度消失的解决方法</li>
<li>最大熵</li>
</ol>
<h2 id="阿里小蜜部门2-1面-1h"><a href="#阿里小蜜部门2-1面-1h" class="headerlink" title="阿里小蜜部门2-1面 1h"></a>阿里小蜜部门2-1面 1h</h2><ol>
<li>自我介绍</li>
<li>算法题 leetcode: twoSum，数据量很大，hash表算法</li>
<li>介绍项目</li>
<li>如BERT模型的准确率到了瓶颈，怎么解决？</li>
</ol>
]]></content>
      <categories>
        <category>NLP面试</category>
      </categories>
      <tags>
        <tag>NLP面试</tag>
      </tags>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/2020/03/07/ResNet/</url>
    <content><![CDATA[<p>Todo</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Batch Normalization</title>
    <url>/2020/03/07/Batch-Normalization/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>由于下层的Layer的参数发生改变，导致上层的输入的分布发生改变，最后深度神经网络难以训练。（注意：底层为最下层）</p>
<ul>
<li>Internal Covariate Shift (内部协变量偏移): 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程</li>
<li>Internal Covariate Shift 带来的问题<ul>
<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>
<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</li>
</ul>
</li>
</ul>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><ul>
<li>算法流程图<br><img src="images\BN.png" alt="BN"><br><br>($\epsilon$是为了增加训练稳定性而加入的小的常量数据)</li>
<li><p>测试阶段如何使用Batch Normalization？</p>
<ul>
<li>BN在每一层计算的$\mu$与$\sigma^2$都是基于当前batch中的训练数据，但是这就带来了一个问题：在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时$\mu$与$\sigma^2$的计算一定是有偏估计，这个时候该如何进行计算？</li>
<li>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的$\mu<em>{batch}$与$\sigma^2</em>{batch}$。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：<script type="math/tex; mode=display">\mu_{test} = E(\mu_{batch})</script><script type="math/tex; mode=display">\sigma^2_{test} = \frac{m}{m-1}E(\sigma^2_{batch})</script></li>
<li>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：<script type="math/tex; mode=display">BN(x_{test}) = \gamma \frac{X_{test} - \mu_{test}}{\sqrt{\sigma^2_{test} + \epsilon}} + \beta</script></li>
</ul>
</li>
<li><p>batch_normalization做了normalization后为什么要变回来？即 scale and shift<br><br> 如果只做normalize在某些情况下会出现问题，比如对象是Sigmoid函数的output，而且output是分布在Sigmoid函数的两侧，normalize会强制把output分布在Sigmoid函数的中间的非饱和区域，这样会导致这层网络所学习到的特征分布被normalize破坏。而上面算法的最后一步，scale and shift可以令零均值单位方差的分布（normalize之后的分布）通过调节$\gamma$和$\beta$变成任意更好的分布（对于喂给下一层网络来说）。因为这个$\gamma$和$\beta$是在训练过程中可以学习得到参数。</p>
</li>
</ul>
<h2 id="BN的优势"><a href="#BN的优势" class="headerlink" title="BN的优势"></a>BN的优势</h2><ul>
<li>BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</li>
<li>BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</li>
<li>BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>
<li>BN具有一定的正则化效果<br><br>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</li>
</ul>
<h2 id="BN的缺陷"><a href="#BN的缺陷" class="headerlink" title="BN的缺陷"></a>BN的缺陷</h2><ul>
<li>不适用于mini-batch非常小的训练环境</li>
<li>不适用于RNN，因为它是一个动态的网络结构，同一个batch中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得BN并不能正确的使用。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">Batch Normalization原理与实战</a></li>
<li><a href="https://www.zhihu.com/question/38102762/answer/607815171" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a></li>
<li><a href="https://www.zhihu.com/question/55917730/answer/154269264" target="_blank" rel="noopener">请问batch_normalization做了normalization后为什么要变回来？</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失、梯度爆炸、解决方法</title>
    <url>/2020/03/06/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><h2 id="梯度消失解决方法"><a href="#梯度消失解决方法" class="headerlink" title="梯度消失解决方法"></a>梯度消失解决方法</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title>TextCNN&amp;TextGCN</title>
    <url>/2020/03/04/TextCNN/</url>
    <content><![CDATA[<h2 id="TextCNN原理"><a href="#TextCNN原理" class="headerlink" title="TextCNN原理"></a>TextCNN原理</h2><ul>
<li>框架图 <img src="images/TextCNN.png" alt="TextCNN"></li>
<li>$x_i: $ the k-dimensional word vector corresponding to the i-th word in the sentence.</li>
<li>向量拼接 <script type="math/tex; mode=display">x_{1:n} = concat(x_i, x_2, ...,x_n)</script></li>
<li>卷积操作 <script type="math/tex; mode=display">c_i = f (w \cdot x_{i:i+h-1} + b)</script></li>
<li>卷积结果 <script type="math/tex; mode=display">c = [c_1, c_2, c_3, ..., c_{n-h+1}]</script></li>
<li>max-over-time pooling <script type="math/tex; mode=display">\hat c = max(x)</script></li>
<li>这是一个卷积核的结果，实际网络中有256个卷积核和两个通道(static embedding 和 dynamic embedding)</li>
</ul>
<h2 id="TextGCN原理"><a href="#TextGCN原理" class="headerlink" title="TextGCN原理"></a>TextGCN原理</h2>]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TextCNN</tag>
        <tag>TextGCN</tag>
      </tags>
  </entry>
  <entry>
    <title>TF-IDF及TextRank</title>
    <url>/2020/03/04/TF-IDF%E5%8F%8ATextRank/</url>
    <content><![CDATA[<h2 id="1-TF-IDF"><a href="#1-TF-IDF" class="headerlink" title="1. TF-IDF"></a>1. TF-IDF</h2><ul>
<li>TF<ul>
<li>TF是词频(TF，Term Frequency): 词频（TF）表示词条（关键字）在文本中出现的频率。<script type="math/tex; mode=display">词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}</script></li>
</ul>
</li>
<li>IDF<ul>
<li>IDF是逆向文件频率 (IDF，Inverse Document Frequency): 某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。<script type="math/tex; mode=display">逆文档频率(IDF) = log \frac{语料库的总文档数}{包含该词的文档数+1}</script></li>
</ul>
</li>
<li>TF-IDF<ul>
<li>TF-IDF实际上是 $TF * IDF$ 。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。<script type="math/tex; mode=display">TFIDF = TF \cdot IDF</script></li>
</ul>
</li>
</ul>
<h2 id="2-TextRank"><a href="#2-TextRank" class="headerlink" title="2. TextRank"></a>2. TextRank</h2><ul>
<li>基于词语词之间的共现性构建无向图。<br><br>参考<a href="https://my.oschina.net/u/3800567/blog/2870640" target="_blank" rel="noopener">jieba源码分析之关键字提取(TF-IDF/TextRank)</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>TextRank</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2020/03/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-激活函数的理解"><a href="#1-激活函数的理解" class="headerlink" title="1.激活函数的理解"></a>1.激活函数的理解</h2><p>解决线形模型 y = wx + b 的弊端，线形分类模型的分界地方都是平面或者超平面，无法解决具有非线形模型特征的数据。如果没有激励函数，在这种情况下你每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，相当于没有隐藏层，网络的学习能力有限。<br><br><img src="images/非线型数据.png" alt="非线型数据"><br><br><a href="https://zhuanlan.zhihu.com/p/25279356" target="_blank" rel="noopener">形象的解释神经网络激活函数的作用是什么？</a><br><br><a href="https://blog.csdn.net/GreatXiang888/article/details/99296607" target="_blank" rel="noopener">常见激活函数，及其优缺点 - 面试篇</a></p>
<h2 id="2-Sigmoid"><a href="#2-Sigmoid" class="headerlink" title="2. Sigmoid"></a>2. Sigmoid</h2><ul>
<li>数学形式<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+ e^{-x}}</script><img src="images\sigmoid.png" alt="sigmoid"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\sigma ^{'}(x)  &=  (1+e^{-x}) ^ {-2}  \cdot e^{-x} \\
          &= \frac{e^{-x}}{(1+e^{-x})^2} \\
          &= \frac{1+e^{-x} -1}{(1+e^{-x})^2} \\
          &= \frac{1}{ 1+ e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
          &= \sigma(x) - \sigma ^ 2(x) \\
          &= \sigma(x) (1-\sigma(x))
\end{align}</script><img src="images\sigmoid导数.png" alt="sigmoid导数"></li>
<li>特点 <ul>
<li>Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。 </li>
</ul>
</li>
<li>缺点<ul>
<li>如果我们初始化神经网络的权值为 $[0,1]$ 之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 $(1,+∞)$ 区间内的值，则会出现梯度爆炸情况。梯度消失更容易产生。</li>
<li>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li>
<li>Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，这个特性会导致为在后面神经网络的高层处理中收到不是零中心的数据。这将导致梯度下降时的晃动，因为如果数据到了神经元永远时正数时，反向传播时权值w就会全为正数或者负数。</li>
</ul>
</li>
</ul>
<h2 id="3-tanh"><a href="#3-tanh" class="headerlink" title="3. tanh"></a>3. tanh</h2><ul>
<li>数学形式<script type="math/tex; mode=display">tanh(x) = \frac{e^{x} - e^{-x}} {e^{x} + e^{-x}}</script><img src="images\tanh.png" alt="tanh"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{align}
\tanh ^{'}(x)  &=  1 - tanh^2(x)
\end{align}</script><img src="images\tanh导数.png" alt="tanh导数"></li>
<li>特点 <ul>
<li>解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</li>
</ul>
</li>
</ul>
<h2 id="4-Relu"><a href="#4-Relu" class="headerlink" title="4. Relu"></a>4. Relu</h2><ul>
<li>数学形式<script type="math/tex; mode=display">Relu(x) = max(0, x)</script><img src="images\Relu.png" alt="Relu"></li>
<li>导数形式<script type="math/tex; mode=display">
\begin{equation}
Relu(x) = \left \{
\begin{aligned}
1 &  & {x>0} \\
0 &  & x<0 \\
None & & x=0 
\end{aligned}
\right.
\end{equation}</script></li>
<li>优点<ul>
<li>解决了梯度消失问题</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid</li>
</ul>
</li>
<li>缺点<ul>
<li>输出不是zero-centered</li>
<li>原点不可导</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见。例如w初始化全部为一些负数。 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li>
<li>leaky relu函数，$f(x)=max(αx,x)$ , 比如取α=0.01\alpha=0.01α=0.01，可以改善relu中x&lt;0部分的dead问题。</li>
</ul>
</li>
</ul>
<h2 id="5-如何选择合适的激活函数"><a href="#5-如何选择合适的激活函数" class="headerlink" title="5. 如何选择合适的激活函数"></a>5. 如何选择合适的激活函数</h2><ul>
<li>首选 ReLU，速度快，但是要注意学习速率的调整，</li>
<li>如果 ReLU 效果欠佳,尝试使用 Leaky ReLU 变种。</li>
<li>可以尝试使用 tanh。</li>
<li>Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。</li>
<li>在浅层神经网络中，选择使用哪种激励函数影响不大。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归（Logistic Regression）</title>
    <url>/2020/03/01/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic%20Regression%EF%BC%89/</url>
    <content><![CDATA[<h2 id="1-假说"><a href="#1-假说" class="headerlink" title="1. 假说"></a>1. 假说</h2><p>假设样本的标签为0和1， $h(x)$ 为取得预测为标签1的概率。</p>
<script type="math/tex; mode=display">h(x) = \delta(w^Tx)</script><p>其中， $ \delta(x)$ 为 $Sigmoid$ 函数</p>
<script type="math/tex; mode=display">\delta(x) = \frac{1}{1+e^{-x}}</script><p>即：</p>
<script type="math/tex; mode=display">h(x) = \frac{1}{1+e^{-w^Tx}}</script><p>线性变化 $f(x) = w^Tx $ 可以理解为一种回归，加入$Sigmoid$ 函数后值域为$[0,1]$，可以应用至分类分类问题，理解为逻辑回归。</p>
<h2 id="2-后验概率"><a href="#2-后验概率" class="headerlink" title="2. 后验概率"></a>2. 后验概率</h2><script type="math/tex; mode=display">p(y=1|x) = h(x) = \frac{1}{1+e^{-w^Tx}}</script><script type="math/tex; mode=display">p(y=0|x) = 1- h(x) = 1- \frac{1}{1+e^{-w^Tx}} = \frac{e^{-w^Tx}}{1+e^{-w^Tx}}</script><h2 id="3-似然函数"><a href="#3-似然函数" class="headerlink" title="3. 似然函数"></a>3. 似然函数</h2><p>N 为数据样本数</p>
<script type="math/tex; mode=display">L(w) = \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i}  = \prod_{i=1}^N h(x_i)^{y_i}(1-h(x_i))^{1-y_i}</script><h2 id="4-对数似然"><a href="#4-对数似然" class="headerlink" title="4. 对数似然"></a>4. 对数似然</h2><script type="math/tex; mode=display">
\begin{align}
    log(L(w)) &= log( \prod_{i=1}^N p(y=1|x_i)^{y_i}  p(y=0|x_i)^{1-y_i} ) \\
    &=  \sum_{i=1}^N  (y_i log(h(x_i))  + (1-y_i)log(1-h(x_i)) ) \\
    &= \sum_{i=1}^N \{ y_i\{log(h(x_i)) - log(1-h(x_i))\} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{ y_i log\frac{h(x_i)}{1-h(x_i)} + log(1-h(x_i))\} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) + log(\frac{e^{-w^Tx}}{1+e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(\frac{1+e^{-w^Tx}}{ e^{-w^Tx}}) \} \\
    &= \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}
\end{align}</script><h2 id="5-损失函数"><a href="#5-损失函数" class="headerlink" title="5. 损失函数"></a>5. 损失函数</h2><p>似然函数乘以 -1/N</p>
<script type="math/tex; mode=display">J(w) = -\frac{1}{N} log(L(w)) = -\frac{1}{N} \sum_{i=1}^N \{  y_i(w^Tx_i) - log(1+ e^{w^Tx}) \}</script><h2 id="6-梯度下降"><a href="#6-梯度下降" class="headerlink" title="6. 梯度下降"></a>6. 梯度下降</h2><p>Loss 如下：</p>
<script type="math/tex; mode=display">
\begin{align}
    \frac{ \partial J(w) }{\partial w} &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} \frac{\partial h(x_i)}{\partial w} - \frac{1-y_i}{1-h(x_i)} \frac{\partial h(x_i)}{\partial w}  \} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \}\frac{\partial h(x_i)}{\partial w}  \\
    &=  -\frac{1}{N}\sum_{i=1}^N \{ \frac{y_i}{h(x_i)} -  \frac{1-y_i}{1-h(x_i)} \} h(x_i)(1-h(x_i)) \frac{\partial w^Tx_i}{\partial w} \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i (1-h(x_i)) -(1-y_i)h(x_i) \} x_i \\
    &= -\frac{1}{N}\sum_{i=1}^N \{ y_i - h(x_i)\} x_i
\end{align}</script><p>推导过程<br><img src="../images/LR推导.jpg" alt="LR"></p>
<p>参数更新如下：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= w-\alpha \frac{\partial J(w)}{ \partial w}\\
&= w- \frac{1}{N}\sum_{i=1}^N \{  h(x_i) -y_i\} x_i
\end{align}</script><p>参数更新理解</p>
<h2 id="7-实践"><a href="#7-实践" class="headerlink" title="7. 实践"></a>7. 实践</h2><h2 id="8-一些问题"><a href="#8-一些问题" class="headerlink" title="8. 一些问题"></a>8. 一些问题</h2><ul>
<li><p>LR为什么是线性模型？<br><br>Logistic Regression从几率的概念构建线性回归模型。一个事件发生的几率（odds）为该事件发生的概率与不发生概率的比值，几率的取值范围为$[0,+\infty)$，其对数的取值范围为实数域，所以，可以将对数几率作为因变量构建线性回归模型: $log\frac{p}{1-p} = w^Tx$,  由此可得 $p = \frac{1}{1+e^{-w^Tx}}$。这便是Logistic Regression采用sigmoid函数的原因，sigmoid函数将自变量的线性组合映射到（0,1），用以表述分类的概率特性。从sigmoid函数看出，当$w^Tx &gt;0 $ 时，y=1，否则 y=0。$w^Tx=0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）, 所以说逻辑回归本质上是一个线性模型。</p>
</li>
<li><p>为什么逻辑回归比线性回归要好？<br><br>逻辑回归能够用于分类，不过其本质还是线性回归。它仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。</p>
</li>
<li><p>sigmoid函数</p>
<script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script><script type="math/tex; mode=display">\sigma^{'}(z) = \frac{e^{-z}}{(1+e^{-z})^2} = \sigma(z) (1- \sigma(z))</script><ul>
<li>优点：<ul>
<li>Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。</li>
<li>求导简单。</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于其软饱和性，容易产生梯度消失，导致训练出现问题。</li>
</ul>
</li>
</ul>
</li>
<li>LR 如何解决多分类问题？<br><br> 如果y不是在[0,1]中取值，而是在K个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当K个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果K个类别是互斥的，即 y=i 的时候意味着 y 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）, 模型通过 softmax 函数来对概率建模。</li>
</ul>
<ul>
<li><p>$probability$和$odds$的定义</p>
<ul>
<li>$probability$指的是<strong>发生的次数</strong>/<strong>总次数</strong>, 如果抛硬币:    </li>
</ul>
<script type="math/tex; mode=display">p = \frac{正面向上的次数}{总次数}$$  p的取值范围为$[0, 1]$

+ $odds$则是一种比率, 指的是**发生的次数**/**没有发生的次数**:  

$$odds = \frac{正面向上次数}{反面向上次数}$$  odds的取值范围为$[0, +\infty)$   

+ 回顾**伯努利分布**: 如果$X$是**伯努利分布**中的随机变量, $X$的取值为$\{0,1\}$, 非$0$即$1$, 如抛硬币的正反面:   

则:  $P(X=1)=p \quad P(X=0)=1-p$   

代入$odds$: 
$$odds=\frac{p}{1-p} \quad odds\in[0, +\infty)</script></li>
</ul>
<ul>
<li><p>$logit$函数和$sigmoid$函数及它们的特性</p>
<ul>
<li>我们对$odds$取$log$, 扩展$odds$的取值范围到实数空间$[-\infty, +\infty]$, 这就是$logit$函数:   </li>
</ul>
<script type="math/tex; mode=display">logit(p)=log_e(odds)=log_e(\frac{p}{1-p}) \quad p \in (0, 1) \quad logit(p) \in [-\infty, +\infty]</script><p>注意: 接下来我们会省略$log$的底$e$</p>
<ul>
<li>接下来, 我们用<strong>线性回归模型</strong>来表示$logit(p)$, 因为<strong>线性回归模型</strong>和$logit$函数的<strong>输出</strong>有着同样的取值范围:   </li>
</ul>
<p>例如: $logit(p) = \theta_1 x_1 + \theta_2 x_2 + bias$<br>下面我们来画一下$logit(p)$, 注意$p \in (0, 1)$, 当$p=0$或$p=1$的时候, $logit$函数属于未定义.</p>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/ltlitao717/article/details/75453480" target="_blank" rel="noopener">机器学习杂货铺-手推LR</a></li>
<li><a href="https://blog.csdn.net/weixin_44915167/article/details/89377022?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">机器学习面试题之LR</a></li>
<li><a href="https://github.com/aespresso/a_journey_into_math_of_ml" target="_blank" rel="noopener">a journey into math of ml</a></li>
<li><a href="https://blog.csdn.net/zhenghaitian/article/details/83618748" target="_blank" rel="noopener">【机器学习】LR（线性回归）—— python3 实现方案</a></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>Logistic Regression</tag>
        <tag>LR</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合及其解决方法</title>
    <url>/2020/03/01/%E8%BF%87%E6%8B%9F%E5%90%88%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="1-过拟合概念"><a href="#1-过拟合概念" class="headerlink" title="1. 过拟合概念"></a>1. 过拟合概念</h2><p>在深度学习或机器学习过程中，在训练集上表现过于优越，在验证集及测试集中表现不佳，模型泛化能力差。<br><a href="https://blog.csdn.net/jingbo18/article/details/80609006" target="_blank" rel="noopener">过拟合及常见处理办法整理</a></p>
<h2 id="2-常见原因"><a href="#2-常见原因" class="headerlink" title="2. 常见原因"></a>2. 常见原因</h2><p>原因主要是数据样本少及噪声多</p>
<ul>
<li>数据样本少</li>
<li>数据噪声多</li>
<li>模型复杂度高</li>
<li>迭代次数多</li>
</ul>
<h2 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h2><ul>
<li>获取更多的数据<ul>
<li>数据源获取更多的数据</li>
<li>数据增强</li>
</ul>
</li>
<li>更改模型结构<ul>
<li>换简单模型</li>
<li>L1 &amp; L2 范式</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
</li>
</ul>
<h2 id="4-数据增强"><a href="#4-数据增强" class="headerlink" title="4. 数据增强"></a>4. 数据增强</h2><p>自然语言处理技术中常用的数据增强方法</p>
<ul>
<li>同义词替换</li>
<li>随机插入</li>
<li>随机交换</li>
<li>随机删除</li>
</ul>
<h2 id="5-L1-amp-L2-范式"><a href="#5-L1-amp-L2-范式" class="headerlink" title="5. L1 &amp; L2 范式"></a>5. L1 &amp; L2 范式</h2><ul>
<li>范式定义<script type="math/tex; mode=display">\left (  \sum_{i}\left  | x_i \right | ^p   \right)^\frac{1}{p}</script></li>
<li>加入范式的目标函数<script type="math/tex; mode=display">J^2\left(w, b\right) = J\left(w, b\right) + \frac{\lambda}{2m}\Omega (w)</script></li>
<li>L1范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  = \sum_{i} \left | w_i  \right |</script></li>
<li>L2范式惩罚因子<script type="math/tex; mode=display">\Omega (w)  =  \sum_{i} \left | w_i  \right | ^2</script></li>
<li>结论<br><br>L1 正则化用作特征选择，L2 正则化用作防止过拟合<br><br>参考介绍—-<a href="https://www.jianshu.com/p/569efedf6985" target="_blank" rel="noopener">机器学习中的正则化</a></li>
</ul>
<h2 id="6-Dropout"><a href="#6-Dropout" class="headerlink" title="6. Dropout"></a>6. Dropout</h2><p>思想：以指数级别创建不同的网络结果，然后各个网络结果加权平均，解决过拟合<br><br>训练阶段，对每一个神经元的输出以keep_prob保留，1-keep_prob置为0；<br><br>预测阶段，对每一个神经元的输出乘以keep_prob。<br><br>缺点: 训练时间变长2-3倍。<br><br>参考介绍—-<a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
        <tag>L1范式</tag>
        <tag>L2范式</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN、LSTM、GRU对比</title>
    <url>/2020/02/29/RNN%E3%80%81LSTM%E3%80%81GRU%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="RNN框架图"><a href="#RNN框架图" class="headerlink" title="RNN框架图"></a>RNN框架图</h3><h3 id="梯度消失及爆炸"><a href="#梯度消失及爆炸" class="headerlink" title="梯度消失及爆炸"></a>梯度消失及爆炸</h3><p>Todo</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>从RNN网络出发开始介绍LSTM网络，记录其架构图及公式。</p>
<h3 id="LSTM的框架图"><a href="#LSTM的框架图" class="headerlink" title="LSTM的框架图"></a>LSTM的框架图</h3><p><img src="/images/LSTM.png" alt="LSTM"></p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><script type="math/tex; mode=display">f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)</script><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><script type="math/tex; mode=display">i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)</script><script type="math/tex; mode=display">\tilde{C_t}=tanh(W_c\cdot[h_{t-1},x_t]+b_c)</script><script type="math/tex; mode=display">C_t=f_t * C_{t-1} + i_t * \tilde{C_t}</script><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><script type="math/tex; mode=display">o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)</script><script type="math/tex; mode=display">h_t=o_t * tanh(C_t)</script><h3 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h3><ul>
<li>首先需要明确的是，RNN 中的梯度消失/梯度爆炸和普通的 MLP 或者深层 CNN 中梯度消失/梯度爆炸的含义不一样。MLP/CNN 中不同的层有不同的参数，各是各的梯度；而 RNN 中同样的权重在各个时间步共享，最终的梯度 g = 各个时间步的梯度 g_t 的和。</li>
<li>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</li>
<li>LSTM 中梯度的传播有很多条路径，cell 这条路径上只有逐元素相乘和相加的操作，梯度流最稳定；但是其他路径上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘。</li>
<li>但是在其他路径上，LSTM 的梯度流和普通 RNN 没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 = 各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度 + 消失梯度 = 正常梯度）。因此 LSTM 通过改善一条路径上的梯度问题拯救了总体的远距离梯度。</li>
<li>同样，因为总的远距离梯度 = 各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度 = 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><h3 id="GRU框架图"><a href="#GRU框架图" class="headerlink" title="GRU框架图"></a>GRU框架图</h3><p><img src="/images/LSTM.png" alt="GRU"></p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><script type="math/tex; mode=display">z_t = \sigma (W_z \cdot [h_{t-1}, x_t] + b_z)</script><script type="math/tex; mode=display">r_t = \sigma (W_r \cdot [h_{t-1}, x_t] + b_r)</script><script type="math/tex; mode=display">\tilde{h_t} = W \cdot [r_t * h_{t-1}, x_t] + b</script><script type="math/tex; mode=display">h_t = (1-z_t) * h_{t-1}  + z_t * \tilde{h_t}</script><h3 id="与LSTM区别"><a href="#与LSTM区别" class="headerlink" title="与LSTM区别"></a>与LSTM区别</h3><ul>
<li>GRU和LSTM的性能在很多任务上不分伯仲。</li>
<li>GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。</li>
<li>从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将 hidden state 传给下一个单元，而LSTM则用 memory cell 把 hidden state 包装起来。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="https://www.zhihu.com/question/34878706/answer/665429718" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li>
<li><a href="https://blog.csdn.net/u012223913/article/details/77724621" target="_blank" rel="noopener">LSTM 和GRU的区别</a></li>
</ul>
]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>自然语言处理</tag>
        <tag>RNN</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
</search>
